nohup: ignoring input
2022-06-23 12:52:18.822903: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-06-23 12:54:05.679237: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1
2022-06-23 12:54:05.695128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:84:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2022-06-23 12:54:05.696151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: 
pciBusID: 0000:85:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2022-06-23 12:54:05.696187: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-06-23 12:54:05.700346: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2022-06-23 12:54:05.700414: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
2022-06-23 12:54:05.701975: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10
2022-06-23 12:54:05.702288: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10
2022-06-23 12:54:05.706582: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11
2022-06-23 12:54:05.707465: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11
2022-06-23 12:54:05.707870: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2022-06-23 12:54:05.711717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1
2022-06-23 12:54:05.712429: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-06-23 12:54:05.904921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:84:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2022-06-23 12:54:05.906238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: 
pciBusID: 0000:85:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2022-06-23 12:54:05.910633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1
2022-06-23 12:54:05.910732: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-06-23 12:54:07.027933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-23 12:54:07.027994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1 
2022-06-23 12:54:07.028011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N Y 
2022-06-23 12:54:07.028020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   Y N 
2022-06-23 12:54:07.032923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8599 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:84:00.0, compute capability: 3.7)
2022-06-23 12:54:07.035275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10792 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id: 0000:85:00.0, compute capability: 3.7)
2022-06-23 12:54:07.416638: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
2022-06-23 12:54:07.417408: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2099935000 Hz
2022-06-23 12:54:10.064998: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2022-06-23 12:54:10.549240: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8201
2022-06-23 12:54:11.635272: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2022-06-23 12:54:12.202778: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
Initializing Experimenter
	Loading Data from ../data/data100k_raw_combined_atlas_cut.pkl
	Data Loaded
	Creating Splits
	Splits Created
Done initalizing
		LATENT DIM 128
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 48s - loss: 0.3945 - val_loss: 0.3574
Epoch 2/256
3063/3063 - 41s - loss: 0.3533 - val_loss: 0.3563
Epoch 3/256
3063/3063 - 42s - loss: 0.3423 - val_loss: 0.3362
Epoch 4/256
3063/3063 - 42s - loss: 0.3311 - val_loss: 0.3491
Epoch 5/256
3063/3063 - 42s - loss: 0.3209 - val_loss: 0.3147
Epoch 6/256
3063/3063 - 42s - loss: 0.3098 - val_loss: 0.3061
Epoch 7/256
3063/3063 - 41s - loss: 0.3011 - val_loss: 0.3004
Epoch 8/256
3063/3063 - 41s - loss: 0.2912 - val_loss: 0.2864
Epoch 9/256
3063/3063 - 42s - loss: 0.2873 - val_loss: 0.2944
Epoch 10/256
3063/3063 - 41s - loss: 0.2812 - val_loss: 0.2881
Epoch 11/256
3063/3063 - 41s - loss: 0.2769 - val_loss: 0.2815
Epoch 12/256
3063/3063 - 41s - loss: 0.2735 - val_loss: 0.2811
Epoch 13/256
3063/3063 - 41s - loss: 0.2701 - val_loss: 0.2678
Epoch 14/256
3063/3063 - 41s - loss: 0.2660 - val_loss: 0.2852
Epoch 15/256
3063/3063 - 42s - loss: 0.2622 - val_loss: 0.2852
Epoch 16/256
3063/3063 - 42s - loss: 0.2601 - val_loss: 0.2570
Epoch 17/256
3063/3063 - 42s - loss: 0.2574 - val_loss: 0.2596
Epoch 18/256
3063/3063 - 41s - loss: 0.2541 - val_loss: 0.2570
Epoch 19/256
3063/3063 - 41s - loss: 0.2518 - val_loss: 0.2620
Epoch 20/256
3063/3063 - 42s - loss: 0.2496 - val_loss: 0.2576
Epoch 21/256
3063/3063 - 42s - loss: 0.2472 - val_loss: 0.2717
Epoch 22/256
3063/3063 - 42s - loss: 0.2455 - val_loss: 0.2998
Epoch 23/256
3063/3063 - 42s - loss: 0.2430 - val_loss: 0.2555
Epoch 24/256
3063/3063 - 42s - loss: 0.2414 - val_loss: 0.2604
Epoch 25/256
3063/3063 - 42s - loss: 0.2396 - val_loss: 0.2552
Epoch 26/256
3063/3063 - 41s - loss: 0.2370 - val_loss: 0.2491
Epoch 27/256
3063/3063 - 42s - loss: 0.2355 - val_loss: 0.2536
Epoch 28/256
3063/3063 - 41s - loss: 0.2336 - val_loss: 0.2444
Epoch 29/256
3063/3063 - 42s - loss: 0.2320 - val_loss: 0.2479
Epoch 30/256
3063/3063 - 42s - loss: 0.2299 - val_loss: 0.2547
Epoch 31/256
3063/3063 - 42s - loss: 0.2298 - val_loss: 0.2521
Epoch 32/256
3063/3063 - 42s - loss: 0.2284 - val_loss: 0.2665
Epoch 33/256
3063/3063 - 43s - loss: 0.2264 - val_loss: 0.2471
Epoch 34/256
3063/3063 - 42s - loss: 0.2253 - val_loss: 0.2500
Epoch 35/256
3063/3063 - 41s - loss: 0.2234 - val_loss: 0.2460
Epoch 36/256
3063/3063 - 42s - loss: 0.2221 - val_loss: 0.2491
Epoch 37/256
3063/3063 - 42s - loss: 0.2205 - val_loss: 0.2537
Epoch 38/256
3063/3063 - 41s - loss: 0.2194 - val_loss: 0.2519
Epoch 39/256
3063/3063 - 41s - loss: 0.2186 - val_loss: 0.2503
Epoch 40/256
3063/3063 - 42s - loss: 0.2158 - val_loss: 0.2458
Epoch 41/256
3063/3063 - 41s - loss: 0.2160 - val_loss: 0.2509
Epoch 42/256
3063/3063 - 42s - loss: 0.2159 - val_loss: 0.2494
Epoch 43/256
3063/3063 - 41s - loss: 0.2135 - val_loss: 0.2433
Epoch 44/256
3063/3063 - 41s - loss: 0.2133 - val_loss: 0.2474
Epoch 45/256
3063/3063 - 41s - loss: 0.2111 - val_loss: 0.2462
Epoch 46/256
3063/3063 - 41s - loss: 0.2092 - val_loss: 0.2497
Epoch 47/256
3063/3063 - 41s - loss: 0.2085 - val_loss: 0.2657
Epoch 48/256
3063/3063 - 41s - loss: 0.2068 - val_loss: 0.2415
Epoch 49/256
3063/3063 - 41s - loss: 0.2067 - val_loss: 0.2531
Epoch 50/256
3063/3063 - 42s - loss: 0.2049 - val_loss: 0.2494
Epoch 51/256
3063/3063 - 41s - loss: 0.2037 - val_loss: 0.2749
Epoch 52/256
3063/3063 - 41s - loss: 0.2029 - val_loss: 0.2572
Epoch 53/256
3063/3063 - 41s - loss: 0.2009 - val_loss: 0.2603
Epoch 54/256
3063/3063 - 41s - loss: 0.2007 - val_loss: 0.2454
Epoch 55/256
3063/3063 - 41s - loss: 0.1996 - val_loss: 0.2386
Epoch 56/256
3063/3063 - 42s - loss: 0.1982 - val_loss: 0.2436
Epoch 57/256
3063/3063 - 41s - loss: 0.1974 - val_loss: 0.2494
Epoch 58/256
3063/3063 - 42s - loss: 0.1950 - val_loss: 0.2510
Epoch 59/256
3063/3063 - 41s - loss: 0.1942 - val_loss: 0.2510
Epoch 60/256
3063/3063 - 41s - loss: 0.1940 - val_loss: 0.2627
Epoch 61/256
3063/3063 - 41s - loss: 0.1929 - val_loss: 0.2828
Epoch 62/256
3063/3063 - 42s - loss: 0.1911 - val_loss: 0.2720
Epoch 63/256
3063/3063 - 41s - loss: 0.1909 - val_loss: 0.2757
Epoch 64/256
3063/3063 - 42s - loss: 0.1901 - val_loss: 0.2588
Epoch 65/256
3063/3063 - 41s - loss: 0.1886 - val_loss: 0.2685
Epoch 66/256
3063/3063 - 42s - loss: 0.1872 - val_loss: 0.2703
Epoch 67/256
3063/3063 - 41s - loss: 0.1844 - val_loss: 0.2523
Epoch 68/256
3063/3063 - 41s - loss: 0.1844 - val_loss: 0.2553
Epoch 69/256
3063/3063 - 42s - loss: 0.1833 - val_loss: 0.2553
Epoch 70/256
3063/3063 - 42s - loss: 0.1824 - val_loss: 0.2464
Epoch 71/256
3063/3063 - 42s - loss: 0.1806 - val_loss: 0.2604
Epoch 72/256
3063/3063 - 42s - loss: 0.1803 - val_loss: 0.2678
Epoch 73/256
3063/3063 - 41s - loss: 0.1784 - val_loss: 0.2547
Epoch 74/256
3063/3063 - 41s - loss: 0.1771 - val_loss: 0.2581
Epoch 75/256
3063/3063 - 41s - loss: 0.1768 - val_loss: 0.2552
Epoch 76/256
3063/3063 - 41s - loss: 0.1751 - val_loss: 0.2731
Epoch 77/256
3063/3063 - 42s - loss: 0.1736 - val_loss: 0.2665
Epoch 78/256
3063/3063 - 41s - loss: 0.1724 - val_loss: 0.2568
Epoch 79/256
3063/3063 - 42s - loss: 0.1713 - val_loss: 0.2677
Epoch 80/256
3063/3063 - 41s - loss: 0.1699 - val_loss: 0.2736
Epoch 81/256
3063/3063 - 41s - loss: 0.1692 - val_loss: 0.2586
Epoch 82/256
3063/3063 - 41s - loss: 0.1682 - val_loss: 0.3245
Epoch 83/256
3063/3063 - 41s - loss: 0.1687 - val_loss: 0.2743
Epoch 84/256
3063/3063 - 42s - loss: 0.1653 - val_loss: 0.2638
Epoch 85/256
3063/3063 - 41s - loss: 0.1648 - val_loss: 0.2603
Epoch 86/256
3063/3063 - 41s - loss: 0.1639 - val_loss: 0.2718
Epoch 87/256
3063/3063 - 41s - loss: 0.1610 - val_loss: 0.2891
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 128)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 45s - loss: 0.3916 - val_loss: 0.3859
Epoch 2/256
3063/3063 - 42s - loss: 0.3558 - val_loss: 0.3439
Epoch 3/256
3063/3063 - 42s - loss: 0.3473 - val_loss: 0.3771
Epoch 4/256
3063/3063 - 42s - loss: 0.3410 - val_loss: 0.3552
Epoch 5/256
3063/3063 - 41s - loss: 0.3334 - val_loss: 0.3350
Epoch 6/256
3063/3063 - 42s - loss: 0.3240 - val_loss: 0.3154
Epoch 7/256
3063/3063 - 41s - loss: 0.3113 - val_loss: 0.3021
Epoch 8/256
3063/3063 - 42s - loss: 0.3012 - val_loss: 0.3143
Epoch 9/256
3063/3063 - 41s - loss: 0.2927 - val_loss: 0.2922
Epoch 10/256
3063/3063 - 42s - loss: 0.2848 - val_loss: 0.2847
Epoch 11/256
3063/3063 - 42s - loss: 0.2786 - val_loss: 0.2711
Epoch 12/256
3063/3063 - 42s - loss: 0.2727 - val_loss: 0.2665
Epoch 13/256
3063/3063 - 42s - loss: 0.2679 - val_loss: 0.2727
Epoch 14/256
3063/3063 - 41s - loss: 0.2643 - val_loss: 0.2716
Epoch 15/256
3063/3063 - 42s - loss: 0.2602 - val_loss: 0.2663
Epoch 16/256
3063/3063 - 41s - loss: 0.2568 - val_loss: 0.2629
Epoch 17/256
3063/3063 - 42s - loss: 0.2546 - val_loss: 0.2629
Epoch 18/256
3063/3063 - 42s - loss: 0.2518 - val_loss: 0.2680
Epoch 19/256
3063/3063 - 42s - loss: 0.2496 - val_loss: 0.2576
Epoch 20/256
3063/3063 - 42s - loss: 0.2464 - val_loss: 0.2557
Epoch 21/256
3063/3063 - 42s - loss: 0.2450 - val_loss: 0.2661
Epoch 22/256
3063/3063 - 42s - loss: 0.2429 - val_loss: 0.2510
Epoch 23/256
3063/3063 - 42s - loss: 0.2397 - val_loss: 0.2458
Epoch 24/256
3063/3063 - 42s - loss: 0.2378 - val_loss: 0.2490
Epoch 25/256
3063/3063 - 42s - loss: 0.2369 - val_loss: 0.2448
Epoch 26/256
3063/3063 - 42s - loss: 0.2338 - val_loss: 0.2488
Epoch 27/256
3063/3063 - 43s - loss: 0.2328 - val_loss: 0.2508
Epoch 28/256
3063/3063 - 42s - loss: 0.2316 - val_loss: 0.2535
Epoch 29/256
3063/3063 - 42s - loss: 0.2291 - val_loss: 0.2410
Epoch 30/256
3063/3063 - 41s - loss: 0.2265 - val_loss: 0.2396
Epoch 31/256
3063/3063 - 41s - loss: 0.2252 - val_loss: 0.2534
Epoch 32/256
3063/3063 - 42s - loss: 0.2249 - val_loss: 0.2398
Epoch 33/256
3063/3063 - 42s - loss: 0.2227 - val_loss: 0.2414
Epoch 34/256
3063/3063 - 42s - loss: 0.2208 - val_loss: 0.2498
Epoch 35/256
3063/3063 - 42s - loss: 0.2197 - val_loss: 0.2498
Epoch 36/256
3063/3063 - 42s - loss: 0.2187 - val_loss: 0.2423
Epoch 37/256
3063/3063 - 41s - loss: 0.2170 - val_loss: 0.2566
Epoch 38/256
3063/3063 - 41s - loss: 0.2165 - val_loss: 0.2383
Epoch 39/256
3063/3063 - 41s - loss: 0.2155 - val_loss: 0.2495
Epoch 40/256
3063/3063 - 41s - loss: 0.2129 - val_loss: 0.2377
Epoch 41/256
3063/3063 - 41s - loss: 0.2123 - val_loss: 0.2442
Epoch 42/256
3063/3063 - 40s - loss: 0.2117 - val_loss: 0.2391
Epoch 43/256
3063/3063 - 41s - loss: 0.2090 - val_loss: 0.2509
Epoch 44/256
3063/3063 - 41s - loss: 0.2095 - val_loss: 0.2449
Epoch 45/256
3063/3063 - 42s - loss: 0.2084 - val_loss: 0.2440
Epoch 46/256
3063/3063 - 41s - loss: 0.2067 - val_loss: 0.2463
Epoch 47/256
3063/3063 - 42s - loss: 0.2055 - val_loss: 0.2415
Epoch 48/256
3063/3063 - 42s - loss: 0.2045 - val_loss: 0.2385
Epoch 49/256
3063/3063 - 42s - loss: 0.2035 - val_loss: 0.2447
Epoch 50/256
3063/3063 - 42s - loss: 0.2007 - val_loss: 0.2377
Epoch 51/256
3063/3063 - 43s - loss: 0.2003 - val_loss: 0.2616
Epoch 52/256
3063/3063 - 42s - loss: 0.1995 - val_loss: 0.2565
Epoch 53/256
3063/3063 - 41s - loss: 0.1982 - val_loss: 0.2428
Epoch 54/256
3063/3063 - 41s - loss: 0.1973 - val_loss: 0.2418
Epoch 55/256
3063/3063 - 41s - loss: 0.1965 - val_loss: 0.2440
Epoch 56/256
3063/3063 - 41s - loss: 0.1947 - val_loss: 0.2725
Epoch 57/256
3063/3063 - 42s - loss: 0.1936 - val_loss: 0.2448
Epoch 58/256
3063/3063 - 41s - loss: 0.1918 - val_loss: 0.2470
Epoch 59/256
3063/3063 - 42s - loss: 0.1911 - val_loss: 0.2446
Epoch 60/256
3063/3063 - 42s - loss: 0.1905 - val_loss: 0.2593
Epoch 61/256
3063/3063 - 42s - loss: 0.1897 - val_loss: 0.2388
Epoch 62/256
3063/3063 - 42s - loss: 0.1875 - val_loss: 0.2465
Epoch 63/256
3063/3063 - 41s - loss: 0.1881 - val_loss: 0.2491
Epoch 64/256
3063/3063 - 42s - loss: 0.1854 - val_loss: 0.2409
Epoch 65/256
3063/3063 - 42s - loss: 0.1838 - val_loss: 0.2488
Epoch 66/256
3063/3063 - 42s - loss: 0.1823 - val_loss: 0.2495
Epoch 67/256
3063/3063 - 42s - loss: 0.1819 - val_loss: 0.2668
Epoch 68/256
3063/3063 - 42s - loss: 0.1811 - val_loss: 0.2478
Epoch 69/256
3063/3063 - 42s - loss: 0.1799 - val_loss: 0.2583
Epoch 70/256
3063/3063 - 42s - loss: 0.1777 - val_loss: 0.2547
Epoch 71/256
3063/3063 - 42s - loss: 0.1760 - val_loss: 0.2575
Epoch 72/256
3063/3063 - 42s - loss: 0.1784 - val_loss: 0.2720
Epoch 73/256
3063/3063 - 42s - loss: 0.1765 - val_loss: 0.2572
Epoch 74/256
3063/3063 - 41s - loss: 0.1740 - val_loss: 0.2512
Epoch 75/256
3063/3063 - 42s - loss: 0.1733 - val_loss: 0.2488
Epoch 76/256
3063/3063 - 41s - loss: 0.1720 - val_loss: 0.2592
Epoch 77/256
3063/3063 - 42s - loss: 0.1718 - val_loss: 0.2801
Epoch 78/256
3063/3063 - 41s - loss: 0.1693 - val_loss: 0.2615
Epoch 79/256
3063/3063 - 43s - loss: 0.1687 - val_loss: 0.2535
Epoch 80/256
3063/3063 - 42s - loss: 0.1676 - val_loss: 0.2658
Epoch 81/256
3063/3063 - 42s - loss: 0.1667 - val_loss: 0.2552
Epoch 82/256
3063/3063 - 42s - loss: 0.1658 - val_loss: 0.2672
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 128)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 44s - loss: 0.3914 - val_loss: 0.3637
Epoch 2/256
3063/3063 - 42s - loss: 0.3565 - val_loss: 0.3599
Epoch 3/256
3063/3063 - 41s - loss: 0.3468 - val_loss: 0.3332
Epoch 4/256
3063/3063 - 41s - loss: 0.3388 - val_loss: 0.3311
Epoch 5/256
3063/3063 - 41s - loss: 0.3295 - val_loss: 0.3276
Epoch 6/256
3063/3063 - 41s - loss: 0.3208 - val_loss: 0.3185
Epoch 7/256
3063/3063 - 42s - loss: 0.3114 - val_loss: 0.2969
Epoch 8/256
3063/3063 - 41s - loss: 0.3015 - val_loss: 0.2928
Epoch 9/256
3063/3063 - 41s - loss: 0.2930 - val_loss: 0.2931
Epoch 10/256
3063/3063 - 42s - loss: 0.2853 - val_loss: 0.2879
Epoch 11/256
3063/3063 - 42s - loss: 0.2807 - val_loss: 0.2913
Epoch 12/256
3063/3063 - 42s - loss: 0.2761 - val_loss: 0.2873
Epoch 13/256
3063/3063 - 42s - loss: 0.2712 - val_loss: 0.2689
Epoch 14/256
3063/3063 - 41s - loss: 0.2683 - val_loss: 0.2757
Epoch 15/256
3063/3063 - 42s - loss: 0.2640 - val_loss: 0.2641
Epoch 16/256
3063/3063 - 42s - loss: 0.2610 - val_loss: 0.2721
Epoch 17/256
3063/3063 - 42s - loss: 0.2573 - val_loss: 0.2661
Epoch 18/256
3063/3063 - 42s - loss: 0.2550 - val_loss: 0.2744
Epoch 19/256
3063/3063 - 42s - loss: 0.2535 - val_loss: 0.2652
Epoch 20/256
3063/3063 - 41s - loss: 0.2503 - val_loss: 0.2741
Epoch 21/256
3063/3063 - 42s - loss: 0.2483 - val_loss: 0.2551
Epoch 22/256
3063/3063 - 41s - loss: 0.2466 - val_loss: 0.2792
Epoch 23/256
3063/3063 - 42s - loss: 0.2439 - val_loss: 0.2559
Epoch 24/256
3063/3063 - 42s - loss: 0.2420 - val_loss: 0.2597
Epoch 25/256
3063/3063 - 42s - loss: 0.2401 - val_loss: 0.2485
Epoch 26/256
3063/3063 - 42s - loss: 0.2391 - val_loss: 0.2489
Epoch 27/256
3063/3063 - 42s - loss: 0.2366 - val_loss: 0.2584
Epoch 28/256
3063/3063 - 42s - loss: 0.2349 - val_loss: 0.2456
Epoch 29/256
3063/3063 - 42s - loss: 0.2341 - val_loss: 0.2587
Epoch 30/256
3063/3063 - 42s - loss: 0.2330 - val_loss: 0.2455
Epoch 31/256
3063/3063 - 42s - loss: 0.2307 - val_loss: 0.2512
Epoch 32/256
3063/3063 - 43s - loss: 0.2285 - val_loss: 0.2796
Epoch 33/256
3063/3063 - 42s - loss: 0.2280 - val_loss: 0.2487
Epoch 34/256
3063/3063 - 43s - loss: 0.2262 - val_loss: 0.2446
Epoch 35/256
3063/3063 - 42s - loss: 0.2254 - val_loss: 0.2413
Epoch 36/256
3063/3063 - 42s - loss: 0.2242 - val_loss: 0.2578
Epoch 37/256
3063/3063 - 42s - loss: 0.2221 - val_loss: 0.2570
Epoch 38/256
3063/3063 - 42s - loss: 0.2202 - val_loss: 0.2434
Epoch 39/256
3063/3063 - 42s - loss: 0.2193 - val_loss: 0.2490
Epoch 40/256
3063/3063 - 42s - loss: 0.2187 - val_loss: 0.2437
Epoch 41/256
3063/3063 - 41s - loss: 0.2174 - val_loss: 0.2558
Epoch 42/256
3063/3063 - 42s - loss: 0.2149 - val_loss: 0.2862
Epoch 43/256
3063/3063 - 42s - loss: 0.2148 - val_loss: 0.2533
Epoch 44/256
3063/3063 - 41s - loss: 0.2135 - val_loss: 0.2418
Epoch 45/256
3063/3063 - 41s - loss: 0.2123 - val_loss: 0.2426
Epoch 46/256
3063/3063 - 41s - loss: 0.2115 - val_loss: 0.2428
Epoch 47/256
3063/3063 - 42s - loss: 0.2096 - val_loss: 0.2449
Epoch 48/256
3063/3063 - 41s - loss: 0.2074 - val_loss: 0.2417
Epoch 49/256
3063/3063 - 42s - loss: 0.2081 - val_loss: 0.2502
Epoch 50/256
3063/3063 - 41s - loss: 0.2061 - val_loss: 0.2528
Epoch 51/256
3063/3063 - 42s - loss: 0.2050 - val_loss: 0.2479
Epoch 52/256
3063/3063 - 41s - loss: 0.2041 - val_loss: 0.2473
Epoch 53/256
3063/3063 - 41s - loss: 0.2035 - val_loss: 0.2418
Epoch 54/256
3063/3063 - 42s - loss: 0.2009 - val_loss: 0.2608
Epoch 55/256
3063/3063 - 42s - loss: 0.2006 - val_loss: 0.2397
Epoch 56/256
3063/3063 - 42s - loss: 0.1992 - val_loss: 0.2530
Epoch 57/256
3063/3063 - 41s - loss: 0.1980 - val_loss: 0.2579
Epoch 58/256
3063/3063 - 42s - loss: 0.1973 - val_loss: 0.2458
Epoch 59/256
3063/3063 - 41s - loss: 0.1960 - val_loss: 0.2425
Epoch 60/256
3063/3063 - 42s - loss: 0.1945 - val_loss: 0.2444
Epoch 61/256
3063/3063 - 41s - loss: 0.1938 - val_loss: 0.2474
Epoch 62/256
3063/3063 - 42s - loss: 0.1929 - val_loss: 0.2484
Epoch 63/256
3063/3063 - 41s - loss: 0.1922 - val_loss: 0.2435
Epoch 64/256
3063/3063 - 42s - loss: 0.1897 - val_loss: 0.2760
Epoch 65/256
3063/3063 - 42s - loss: 0.1893 - val_loss: 0.2538
Epoch 66/256
3063/3063 - 42s - loss: 0.1880 - val_loss: 0.2684
Epoch 67/256
3063/3063 - 42s - loss: 0.1865 - val_loss: 0.2672
Epoch 68/256
3063/3063 - 42s - loss: 0.1851 - val_loss: 0.2719
Epoch 69/256
3063/3063 - 41s - loss: 0.1858 - val_loss: 0.2568
Epoch 70/256
3063/3063 - 42s - loss: 0.1833 - val_loss: 0.2574
Epoch 71/256
3063/3063 - 41s - loss: 0.1835 - val_loss: 0.2547
Epoch 72/256
3063/3063 - 41s - loss: 0.1814 - val_loss: 0.2437
Epoch 73/256
3063/3063 - 41s - loss: 0.1802 - val_loss: 0.2490
Epoch 74/256
3063/3063 - 43s - loss: 0.1791 - val_loss: 0.2546
Epoch 75/256
3063/3063 - 42s - loss: 0.1792 - val_loss: 0.2516
Epoch 76/256
3063/3063 - 43s - loss: 0.1770 - val_loss: 0.2600
Epoch 77/256
3063/3063 - 42s - loss: 0.1758 - val_loss: 0.2693
Epoch 78/256
3063/3063 - 42s - loss: 0.1740 - val_loss: 0.2657
Epoch 79/256
3063/3063 - 42s - loss: 0.1738 - val_loss: 0.2525
Epoch 80/256
3063/3063 - 42s - loss: 0.1736 - val_loss: 0.2681
Epoch 81/256
3063/3063 - 42s - loss: 0.1716 - val_loss: 0.2772
Epoch 82/256
3063/3063 - 41s - loss: 0.1699 - val_loss: 0.2690
Epoch 83/256
3063/3063 - 42s - loss: 0.1682 - val_loss: 0.2626
Epoch 84/256
3063/3063 - 42s - loss: 0.1686 - val_loss: 0.2747
Epoch 85/256
3063/3063 - 41s - loss: 0.1661 - val_loss: 0.2656
Epoch 86/256
3063/3063 - 42s - loss: 0.1655 - val_loss: 0.2585
Epoch 87/256
3063/3063 - 41s - loss: 0.1636 - val_loss: 0.2664
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 128)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 44s - loss: 0.3914 - val_loss: 0.3585
Epoch 2/256
3063/3063 - 42s - loss: 0.3565 - val_loss: 0.3412
Epoch 3/256
3063/3063 - 42s - loss: 0.3458 - val_loss: 0.3508
Epoch 4/256
3063/3063 - 42s - loss: 0.3362 - val_loss: 0.3409
Epoch 5/256
3063/3063 - 42s - loss: 0.3291 - val_loss: 0.3201
Epoch 6/256
3063/3063 - 43s - loss: 0.3186 - val_loss: 0.3086
Epoch 7/256
3063/3063 - 42s - loss: 0.3084 - val_loss: 0.3227
Epoch 8/256
3063/3063 - 43s - loss: 0.2999 - val_loss: 0.3086
Epoch 9/256
3063/3063 - 42s - loss: 0.2937 - val_loss: 0.2960
Epoch 10/256
3063/3063 - 42s - loss: 0.2863 - val_loss: 0.2814
Epoch 11/256
3063/3063 - 42s - loss: 0.2816 - val_loss: 0.3049
Epoch 12/256
3063/3063 - 42s - loss: 0.2744 - val_loss: 0.2737
Epoch 13/256
3063/3063 - 43s - loss: 0.2699 - val_loss: 0.3015
Epoch 14/256
3063/3063 - 42s - loss: 0.2671 - val_loss: 0.2632
Epoch 15/256
3063/3063 - 42s - loss: 0.2636 - val_loss: 0.2801
Epoch 16/256
3063/3063 - 42s - loss: 0.2604 - val_loss: 0.2786
Epoch 17/256
3063/3063 - 43s - loss: 0.2582 - val_loss: 0.2724
Epoch 18/256
3063/3063 - 43s - loss: 0.2547 - val_loss: 0.2567
Epoch 19/256
3063/3063 - 43s - loss: 0.2515 - val_loss: 0.2545
Epoch 20/256
3063/3063 - 42s - loss: 0.2488 - val_loss: 0.2587
Epoch 21/256
3063/3063 - 43s - loss: 0.2484 - val_loss: 0.2502
Epoch 22/256
3063/3063 - 42s - loss: 0.2443 - val_loss: 0.2540
Epoch 23/256
3063/3063 - 42s - loss: 0.2435 - val_loss: 0.2645
Epoch 24/256
3063/3063 - 42s - loss: 0.2409 - val_loss: 0.2609
Epoch 25/256
3063/3063 - 42s - loss: 0.2387 - val_loss: 0.2623
Epoch 26/256
3063/3063 - 42s - loss: 0.2368 - val_loss: 0.2495
Epoch 27/256
3063/3063 - 42s - loss: 0.2351 - val_loss: 0.2840
Epoch 28/256
3063/3063 - 42s - loss: 0.2335 - val_loss: 0.2432
Epoch 29/256
3063/3063 - 43s - loss: 0.2313 - val_loss: 0.2471
Epoch 30/256
3063/3063 - 42s - loss: 0.2292 - val_loss: 0.2435
Epoch 31/256
3063/3063 - 42s - loss: 0.2275 - val_loss: 0.2456
Epoch 32/256
3063/3063 - 44s - loss: 0.2262 - val_loss: 0.2400
Epoch 33/256
3063/3063 - 42s - loss: 0.2255 - val_loss: 0.2601
Epoch 34/256
3063/3063 - 42s - loss: 0.2237 - val_loss: 0.2476
Epoch 35/256
3063/3063 - 42s - loss: 0.2232 - val_loss: 0.2428
Epoch 36/256
3063/3063 - 41s - loss: 0.2201 - val_loss: 0.2585
Epoch 37/256
3063/3063 - 42s - loss: 0.2194 - val_loss: 0.2384
Epoch 38/256
3063/3063 - 42s - loss: 0.2179 - val_loss: 0.2668
Epoch 39/256
3063/3063 - 42s - loss: 0.2174 - val_loss: 0.2380
Epoch 40/256
3063/3063 - 42s - loss: 0.2150 - val_loss: 0.2389
Epoch 41/256
3063/3063 - 42s - loss: 0.2143 - val_loss: 0.2589
Epoch 42/256
3063/3063 - 43s - loss: 0.2130 - val_loss: 0.2435
Epoch 43/256
3063/3063 - 43s - loss: 0.2114 - val_loss: 0.2359
Epoch 44/256
3063/3063 - 42s - loss: 0.2109 - val_loss: 0.2457
Epoch 45/256
3063/3063 - 42s - loss: 0.2092 - val_loss: 0.2435
Epoch 46/256
3063/3063 - 42s - loss: 0.2082 - val_loss: 0.2392
Epoch 47/256
3063/3063 - 42s - loss: 0.2073 - val_loss: 0.2457
Epoch 48/256
3063/3063 - 42s - loss: 0.2060 - val_loss: 0.2448
Epoch 49/256
3063/3063 - 42s - loss: 0.2044 - val_loss: 0.2412
Epoch 50/256
3063/3063 - 42s - loss: 0.2031 - val_loss: 0.2539
Epoch 51/256
3063/3063 - 42s - loss: 0.2023 - val_loss: 0.2496
Epoch 52/256
3063/3063 - 42s - loss: 0.2016 - val_loss: 0.2394
Epoch 53/256
3063/3063 - 42s - loss: 0.2005 - val_loss: 0.2520
Epoch 54/256
3063/3063 - 42s - loss: 0.1983 - val_loss: 0.2421
Epoch 55/256
3063/3063 - 42s - loss: 0.1979 - val_loss: 0.2414
Epoch 56/256
3063/3063 - 42s - loss: 0.1978 - val_loss: 0.2394
Epoch 57/256
3063/3063 - 44s - loss: 0.1956 - val_loss: 0.2450
Epoch 58/256
3063/3063 - 43s - loss: 0.1947 - val_loss: 0.2512
Epoch 59/256
3063/3063 - 42s - loss: 0.1947 - val_loss: 0.2543
Epoch 60/256
3063/3063 - 41s - loss: 0.1925 - val_loss: 0.2551
Epoch 61/256
3063/3063 - 42s - loss: 0.1904 - val_loss: 0.2766
Epoch 62/256
3063/3063 - 42s - loss: 0.1898 - val_loss: 0.2515
Epoch 63/256
3063/3063 - 42s - loss: 0.1890 - val_loss: 0.2526
Epoch 64/256
3063/3063 - 43s - loss: 0.1892 - val_loss: 0.2692
Epoch 65/256
3063/3063 - 42s - loss: 0.1875 - val_loss: 0.2545
Epoch 66/256
3063/3063 - 41s - loss: 0.1853 - val_loss: 0.2588
Epoch 67/256
3063/3063 - 41s - loss: 0.1841 - val_loss: 0.2438
Epoch 68/256
3063/3063 - 42s - loss: 0.1829 - val_loss: 0.2838
Epoch 69/256
3063/3063 - 42s - loss: 0.1830 - val_loss: 0.2622
Epoch 70/256
3063/3063 - 42s - loss: 0.1818 - val_loss: 0.2610
Epoch 71/256
3063/3063 - 42s - loss: 0.1808 - val_loss: 0.2692
Epoch 72/256
3063/3063 - 42s - loss: 0.1793 - val_loss: 0.2478
Epoch 73/256
3063/3063 - 42s - loss: 0.1779 - val_loss: 0.2867
Epoch 74/256
3063/3063 - 42s - loss: 0.1772 - val_loss: 0.2653
Epoch 75/256
3063/3063 - 42s - loss: 0.1759 - val_loss: 0.2558
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 128)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 46s - loss: 0.3911 - val_loss: 0.3555
Epoch 2/256
3063/3063 - 42s - loss: 0.3558 - val_loss: 0.3501
Epoch 3/256
3063/3063 - 42s - loss: 0.3465 - val_loss: 0.3367
Epoch 4/256
3063/3063 - 42s - loss: 0.3387 - val_loss: 0.3345
Epoch 5/256
3063/3063 - 42s - loss: 0.3327 - val_loss: 0.3310
Epoch 6/256
3063/3063 - 42s - loss: 0.3243 - val_loss: 0.3316
Epoch 7/256
3063/3063 - 42s - loss: 0.3181 - val_loss: 0.3177
Epoch 8/256
3063/3063 - 41s - loss: 0.3105 - val_loss: 0.3129
Epoch 9/256
3063/3063 - 43s - loss: 0.3024 - val_loss: 0.3128
Epoch 10/256
3063/3063 - 42s - loss: 0.2946 - val_loss: 0.3061
Epoch 11/256
3063/3063 - 42s - loss: 0.2869 - val_loss: 0.2837
Epoch 12/256
3063/3063 - 42s - loss: 0.2801 - val_loss: 0.2786
Epoch 13/256
3063/3063 - 42s - loss: 0.2748 - val_loss: 0.2781
Epoch 14/256
3063/3063 - 42s - loss: 0.2701 - val_loss: 0.2867
Epoch 15/256
3063/3063 - 42s - loss: 0.2661 - val_loss: 0.2689
Epoch 16/256
3063/3063 - 42s - loss: 0.2628 - val_loss: 0.2698
Epoch 17/256
3063/3063 - 42s - loss: 0.2592 - val_loss: 0.2602
Epoch 18/256
3063/3063 - 43s - loss: 0.2553 - val_loss: 0.2634
Epoch 19/256
3063/3063 - 42s - loss: 0.2531 - val_loss: 0.2857
Epoch 20/256
3063/3063 - 43s - loss: 0.2515 - val_loss: 0.2562
Epoch 21/256
3063/3063 - 42s - loss: 0.2483 - val_loss: 0.2529
Epoch 22/256
3063/3063 - 42s - loss: 0.2448 - val_loss: 0.2554
Epoch 23/256
3063/3063 - 43s - loss: 0.2428 - val_loss: 0.2518
Epoch 24/256
3063/3063 - 42s - loss: 0.2412 - val_loss: 0.2731
Epoch 25/256
3063/3063 - 43s - loss: 0.2391 - val_loss: 0.2507
Epoch 26/256
3063/3063 - 42s - loss: 0.2375 - val_loss: 0.2482
Epoch 27/256
3063/3063 - 43s - loss: 0.2360 - val_loss: 0.2514
Epoch 28/256
3063/3063 - 42s - loss: 0.2323 - val_loss: 0.2552
Epoch 29/256
3063/3063 - 42s - loss: 0.2311 - val_loss: 0.2426
Epoch 30/256
3063/3063 - 43s - loss: 0.2294 - val_loss: 0.2475
Epoch 31/256
3063/3063 - 42s - loss: 0.2284 - val_loss: 0.2434
Epoch 32/256
3063/3063 - 43s - loss: 0.2262 - val_loss: 0.2428
Epoch 33/256
3063/3063 - 43s - loss: 0.2261 - val_loss: 0.2495
Epoch 34/256
3063/3063 - 42s - loss: 0.2238 - val_loss: 0.2495
Epoch 35/256
3063/3063 - 42s - loss: 0.2225 - val_loss: 0.2419
Epoch 36/256
3063/3063 - 42s - loss: 0.2216 - val_loss: 0.2425
Epoch 37/256
3063/3063 - 42s - loss: 0.2212 - val_loss: 0.2458
Epoch 38/256
3063/3063 - 42s - loss: 0.2189 - val_loss: 0.2601
Epoch 39/256
3063/3063 - 42s - loss: 0.2181 - val_loss: 0.2545
Epoch 40/256
3063/3063 - 42s - loss: 0.2166 - val_loss: 0.2521
Epoch 41/256
3063/3063 - 44s - loss: 0.2151 - val_loss: 0.2380
Epoch 42/256
3063/3063 - 42s - loss: 0.2138 - val_loss: 0.2428
Epoch 43/256
3063/3063 - 42s - loss: 0.2125 - val_loss: 0.2423
Epoch 44/256
3063/3063 - 42s - loss: 0.2120 - val_loss: 0.2515
Epoch 45/256
3063/3063 - 42s - loss: 0.2114 - val_loss: 0.2633
Epoch 46/256
3063/3063 - 42s - loss: 0.2097 - val_loss: 0.2467
Epoch 47/256
3063/3063 - 42s - loss: 0.2093 - val_loss: 0.2435
Epoch 48/256
3063/3063 - 42s - loss: 0.2073 - val_loss: 0.2476
Epoch 49/256
3063/3063 - 41s - loss: 0.2048 - val_loss: 0.2508
Epoch 50/256
3063/3063 - 41s - loss: 0.2051 - val_loss: 0.2818
Epoch 51/256
3063/3063 - 41s - loss: 0.2043 - val_loss: 0.2388
Epoch 52/256
3063/3063 - 42s - loss: 0.2024 - val_loss: 0.2429
Epoch 53/256
3063/3063 - 42s - loss: 0.2016 - val_loss: 0.2537
Epoch 54/256
3063/3063 - 42s - loss: 0.1994 - val_loss: 0.2458
Epoch 55/256
3063/3063 - 41s - loss: 0.1991 - val_loss: 0.2415
Epoch 56/256
3063/3063 - 41s - loss: 0.1983 - val_loss: 0.2409
Epoch 57/256
3063/3063 - 41s - loss: 0.1975 - val_loss: 0.2689
Epoch 58/256
3063/3063 - 42s - loss: 0.1957 - val_loss: 0.2510
Epoch 59/256
3063/3063 - 41s - loss: 0.1948 - val_loss: 0.2522
Epoch 60/256
3063/3063 - 42s - loss: 0.1937 - val_loss: 0.2584
Epoch 61/256
3063/3063 - 41s - loss: 0.1926 - val_loss: 0.2500
Epoch 62/256
3063/3063 - 42s - loss: 0.1911 - val_loss: 0.2375
Epoch 63/256
3063/3063 - 41s - loss: 0.1896 - val_loss: 0.2513
Epoch 64/256
3063/3063 - 41s - loss: 0.1890 - val_loss: 0.2730
Epoch 65/256
3063/3063 - 41s - loss: 0.1877 - val_loss: 0.2565
Epoch 66/256
3063/3063 - 42s - loss: 0.1870 - val_loss: 0.2605
Epoch 67/256
3063/3063 - 42s - loss: 0.1865 - val_loss: 0.2553
Epoch 68/256
3063/3063 - 42s - loss: 0.1838 - val_loss: 0.2637
Epoch 69/256
3063/3063 - 42s - loss: 0.1826 - val_loss: 0.2506
Epoch 70/256
3063/3063 - 41s - loss: 0.1812 - val_loss: 0.2552
Epoch 71/256
3063/3063 - 42s - loss: 0.1801 - val_loss: 0.2864
Epoch 72/256
3063/3063 - 42s - loss: 0.1786 - val_loss: 0.2577
Epoch 73/256
3063/3063 - 42s - loss: 0.1780 - val_loss: 0.2551
Epoch 74/256
3063/3063 - 43s - loss: 0.1778 - val_loss: 0.2732
Epoch 75/256
3063/3063 - 42s - loss: 0.1751 - val_loss: 0.2510
Epoch 76/256
3063/3063 - 43s - loss: 0.1752 - val_loss: 0.2567
Epoch 77/256
3063/3063 - 42s - loss: 0.1731 - val_loss: 0.2797
Epoch 78/256
3063/3063 - 42s - loss: 0.1732 - val_loss: 0.2621
Epoch 79/256
3063/3063 - 42s - loss: 0.1729 - val_loss: 0.2647
Epoch 80/256
3063/3063 - 42s - loss: 0.1706 - val_loss: 0.2708
Epoch 81/256
3063/3063 - 41s - loss: 0.1696 - val_loss: 0.2577
Epoch 82/256
3063/3063 - 41s - loss: 0.1674 - val_loss: 0.2675
Epoch 83/256
3063/3063 - 41s - loss: 0.1653 - val_loss: 0.2623
Epoch 84/256
3063/3063 - 42s - loss: 0.1650 - val_loss: 0.2583
Epoch 85/256
3063/3063 - 42s - loss: 0.1640 - val_loss: 0.2767
Epoch 86/256
3063/3063 - 42s - loss: 0.1636 - val_loss: 0.2668
Epoch 87/256
3063/3063 - 42s - loss: 0.1608 - val_loss: 0.2704
Epoch 88/256
3063/3063 - 42s - loss: 0.1598 - val_loss: 0.2688
Epoch 89/256
3063/3063 - 42s - loss: 0.1603 - val_loss: 0.2838
Epoch 90/256
3063/3063 - 42s - loss: 0.1590 - val_loss: 0.2610
Epoch 91/256
3063/3063 - 42s - loss: 0.1564 - val_loss: 0.2694
Epoch 92/256
3063/3063 - 42s - loss: 0.1552 - val_loss: 0.2798
Epoch 93/256
3063/3063 - 41s - loss: 0.1527 - val_loss: 0.2766
Epoch 94/256
3063/3063 - 42s - loss: 0.1537 - val_loss: 0.2759
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 128)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 43s - loss: 0.3934 - val_loss: 0.3510
Epoch 2/256
3063/3063 - 41s - loss: 0.3566 - val_loss: 0.3564
Epoch 3/256
3063/3063 - 41s - loss: 0.3458 - val_loss: 0.3422
Epoch 4/256
3063/3063 - 42s - loss: 0.3389 - val_loss: 0.3394
Epoch 5/256
3063/3063 - 42s - loss: 0.3315 - val_loss: 0.3246
Epoch 6/256
3063/3063 - 42s - loss: 0.3211 - val_loss: 0.3101
Epoch 7/256
3063/3063 - 42s - loss: 0.3075 - val_loss: 0.3093
Epoch 8/256
3063/3063 - 42s - loss: 0.2969 - val_loss: 0.2935
Epoch 9/256
3063/3063 - 44s - loss: 0.2893 - val_loss: 0.2928
Epoch 10/256
3063/3063 - 41s - loss: 0.2826 - val_loss: 0.2881
Epoch 11/256
3063/3063 - 41s - loss: 0.2775 - val_loss: 0.2855
Epoch 12/256
3063/3063 - 41s - loss: 0.2720 - val_loss: 0.2710
Epoch 13/256
3063/3063 - 42s - loss: 0.2674 - val_loss: 0.2677
Epoch 14/256
3063/3063 - 41s - loss: 0.2628 - val_loss: 0.2707
Epoch 15/256
3063/3063 - 41s - loss: 0.2601 - val_loss: 0.2723
Epoch 16/256
3063/3063 - 41s - loss: 0.2561 - val_loss: 0.2694
Epoch 17/256
3063/3063 - 41s - loss: 0.2530 - val_loss: 0.2626
Epoch 18/256
3063/3063 - 41s - loss: 0.2510 - val_loss: 0.2525
Epoch 19/256
3063/3063 - 41s - loss: 0.2472 - val_loss: 0.2518
Epoch 20/256
3063/3063 - 41s - loss: 0.2452 - val_loss: 0.2534
Epoch 21/256
3063/3063 - 41s - loss: 0.2429 - val_loss: 0.2561
Epoch 22/256
3063/3063 - 42s - loss: 0.2395 - val_loss: 0.2658
Epoch 23/256
3063/3063 - 41s - loss: 0.2380 - val_loss: 0.2462
Epoch 24/256
3063/3063 - 41s - loss: 0.2369 - val_loss: 0.2517
Epoch 25/256
3063/3063 - 41s - loss: 0.2346 - val_loss: 0.2482
Epoch 26/256
3063/3063 - 41s - loss: 0.2329 - val_loss: 0.2473
Epoch 27/256
3063/3063 - 41s - loss: 0.2317 - val_loss: 0.2414
Epoch 28/256
3063/3063 - 41s - loss: 0.2299 - val_loss: 0.2417
Epoch 29/256
3063/3063 - 41s - loss: 0.2271 - val_loss: 0.2406
Epoch 30/256
3063/3063 - 41s - loss: 0.2265 - val_loss: 0.2427
Epoch 31/256
3063/3063 - 41s - loss: 0.2241 - val_loss: 0.2548
Epoch 32/256
3063/3063 - 42s - loss: 0.2236 - val_loss: 0.2496
Epoch 33/256
3063/3063 - 41s - loss: 0.2219 - val_loss: 0.2438
Epoch 34/256
3063/3063 - 41s - loss: 0.2217 - val_loss: 0.2423
Epoch 35/256
3063/3063 - 41s - loss: 0.2193 - val_loss: 0.2667
Epoch 36/256
3063/3063 - 41s - loss: 0.2191 - val_loss: 0.2359
Epoch 37/256
3063/3063 - 41s - loss: 0.2165 - val_loss: 0.2531
Epoch 38/256
3063/3063 - 41s - loss: 0.2151 - val_loss: 0.2376
Epoch 39/256
3063/3063 - 41s - loss: 0.2152 - val_loss: 0.2477
Epoch 40/256
3063/3063 - 42s - loss: 0.2132 - val_loss: 0.2440
Epoch 41/256
3063/3063 - 41s - loss: 0.2124 - val_loss: 0.2567
Epoch 42/256
3063/3063 - 41s - loss: 0.2114 - val_loss: 0.2371
Epoch 43/256
3063/3063 - 41s - loss: 0.2095 - val_loss: 0.2368
Epoch 44/256
3063/3063 - 42s - loss: 0.2086 - val_loss: 0.2528
Epoch 45/256
3063/3063 - 42s - loss: 0.2076 - val_loss: 0.2402
Epoch 46/256
3063/3063 - 42s - loss: 0.2066 - val_loss: 0.2468
Epoch 47/256
3063/3063 - 42s - loss: 0.2061 - val_loss: 0.2417
Epoch 48/256
3063/3063 - 42s - loss: 0.2028 - val_loss: 0.2574
Epoch 49/256
3063/3063 - 42s - loss: 0.2041 - val_loss: 0.2490
Epoch 50/256
3063/3063 - 41s - loss: 0.2028 - val_loss: 0.2498
Epoch 51/256
3063/3063 - 41s - loss: 0.2011 - val_loss: 0.2528
Epoch 52/256
3063/3063 - 41s - loss: 0.2005 - val_loss: 0.2509
Epoch 53/256
3063/3063 - 41s - loss: 0.1994 - val_loss: 0.2467
Epoch 54/256
3063/3063 - 41s - loss: 0.1975 - val_loss: 0.2398
Epoch 55/256
3063/3063 - 41s - loss: 0.1965 - val_loss: 0.2395
Epoch 56/256
3063/3063 - 41s - loss: 0.1951 - val_loss: 0.2504
Epoch 57/256
3063/3063 - 41s - loss: 0.1942 - val_loss: 0.2424
Epoch 58/256
3063/3063 - 42s - loss: 0.1932 - val_loss: 0.2514
Epoch 59/256
3063/3063 - 41s - loss: 0.1924 - val_loss: 0.2482
Epoch 60/256
3063/3063 - 42s - loss: 0.1909 - val_loss: 0.2414
Epoch 61/256
3063/3063 - 41s - loss: 0.1905 - val_loss: 0.2489
Epoch 62/256
3063/3063 - 41s - loss: 0.1878 - val_loss: 0.2396
Epoch 63/256
3063/3063 - 41s - loss: 0.1881 - val_loss: 0.2519
Epoch 64/256
3063/3063 - 41s - loss: 0.1859 - val_loss: 0.2580
Epoch 65/256
3063/3063 - 41s - loss: 0.1854 - val_loss: 0.2607
Epoch 66/256
3063/3063 - 41s - loss: 0.1851 - val_loss: 0.2494
Epoch 67/256
3063/3063 - 41s - loss: 0.1832 - val_loss: 0.2563
Epoch 68/256
3063/3063 - 41s - loss: 0.1817 - val_loss: 0.2496
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 128)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 43s - loss: 0.3922 - val_loss: 0.3507
Epoch 2/256
3063/3063 - 42s - loss: 0.3563 - val_loss: 0.3459
Epoch 3/256
3063/3063 - 41s - loss: 0.3476 - val_loss: 0.3411
Epoch 4/256
3063/3063 - 41s - loss: 0.3413 - val_loss: 0.3339
Epoch 5/256
3063/3063 - 41s - loss: 0.3317 - val_loss: 0.3276
Epoch 6/256
3063/3063 - 42s - loss: 0.3183 - val_loss: 0.3214
Epoch 7/256
3063/3063 - 41s - loss: 0.3108 - val_loss: 0.3020
Epoch 8/256
3063/3063 - 42s - loss: 0.3008 - val_loss: 0.2918
Epoch 9/256
3063/3063 - 42s - loss: 0.2921 - val_loss: 0.3000
Epoch 10/256
3063/3063 - 42s - loss: 0.2840 - val_loss: 0.2843
Epoch 11/256
3063/3063 - 41s - loss: 0.2785 - val_loss: 0.2821
Epoch 12/256
3063/3063 - 42s - loss: 0.2734 - val_loss: 0.2656
Epoch 13/256
3063/3063 - 42s - loss: 0.2690 - val_loss: 0.2777
Epoch 14/256
3063/3063 - 42s - loss: 0.2646 - val_loss: 0.2685
Epoch 15/256
3063/3063 - 41s - loss: 0.2606 - val_loss: 0.2653
Epoch 16/256
3063/3063 - 41s - loss: 0.2571 - val_loss: 0.2960
Epoch 17/256
3063/3063 - 41s - loss: 0.2541 - val_loss: 0.2634
Epoch 18/256
3063/3063 - 41s - loss: 0.2515 - val_loss: 0.2746
Epoch 19/256
3063/3063 - 42s - loss: 0.2496 - val_loss: 0.2541
Epoch 20/256
3063/3063 - 42s - loss: 0.2469 - val_loss: 0.2843
Epoch 21/256
3063/3063 - 42s - loss: 0.2450 - val_loss: 0.2691
Epoch 22/256
3063/3063 - 41s - loss: 0.2436 - val_loss: 0.2506
Epoch 23/256
3063/3063 - 42s - loss: 0.2411 - val_loss: 0.2664
Epoch 24/256
3063/3063 - 42s - loss: 0.2388 - val_loss: 0.2501
Epoch 25/256
3063/3063 - 41s - loss: 0.2376 - val_loss: 0.2612
Epoch 26/256
3063/3063 - 42s - loss: 0.2357 - val_loss: 0.2512
Epoch 27/256
3063/3063 - 41s - loss: 0.2338 - val_loss: 0.2490
Epoch 28/256
3063/3063 - 42s - loss: 0.2333 - val_loss: 0.2475
Epoch 29/256
3063/3063 - 41s - loss: 0.2315 - val_loss: 0.2489
Epoch 30/256
3063/3063 - 41s - loss: 0.2294 - val_loss: 0.2525
Epoch 31/256
3063/3063 - 41s - loss: 0.2281 - val_loss: 0.2520
Epoch 32/256
3063/3063 - 41s - loss: 0.2275 - val_loss: 0.2556
Epoch 33/256
3063/3063 - 41s - loss: 0.2264 - val_loss: 0.2415
Epoch 34/256
3063/3063 - 42s - loss: 0.2243 - val_loss: 0.2445
Epoch 35/256
3063/3063 - 41s - loss: 0.2231 - val_loss: 0.2526
Epoch 36/256
3063/3063 - 41s - loss: 0.2213 - val_loss: 0.2453
Epoch 37/256
3063/3063 - 41s - loss: 0.2185 - val_loss: 0.2427
Epoch 38/256
3063/3063 - 41s - loss: 0.2184 - val_loss: 0.2435
Epoch 39/256
3063/3063 - 41s - loss: 0.2177 - val_loss: 0.2408
Epoch 40/256
3063/3063 - 42s - loss: 0.2172 - val_loss: 0.2399
Epoch 41/256
3063/3063 - 41s - loss: 0.2158 - val_loss: 0.2462
Epoch 42/256
3063/3063 - 42s - loss: 0.2131 - val_loss: 0.2434
Epoch 43/256
3063/3063 - 41s - loss: 0.2127 - val_loss: 0.2549
Epoch 44/256
3063/3063 - 41s - loss: 0.2118 - val_loss: 0.2364
Epoch 45/256
3063/3063 - 41s - loss: 0.2105 - val_loss: 0.2491
Epoch 46/256
3063/3063 - 41s - loss: 0.2098 - val_loss: 0.2494
Epoch 47/256
3063/3063 - 41s - loss: 0.2082 - val_loss: 0.2447
Epoch 48/256
3063/3063 - 42s - loss: 0.2072 - val_loss: 0.2556
Epoch 49/256
3063/3063 - 41s - loss: 0.2049 - val_loss: 0.2524
Epoch 50/256
3063/3063 - 42s - loss: 0.2036 - val_loss: 0.2406
Epoch 51/256
3063/3063 - 41s - loss: 0.2034 - val_loss: 0.2486
Epoch 52/256
3063/3063 - 41s - loss: 0.2025 - val_loss: 0.2530
Epoch 53/256
3063/3063 - 41s - loss: 0.2008 - val_loss: 0.2454
Epoch 54/256
3063/3063 - 41s - loss: 0.1994 - val_loss: 0.2526
Epoch 55/256
3063/3063 - 41s - loss: 0.1982 - val_loss: 0.2355
Epoch 56/256
3063/3063 - 42s - loss: 0.1981 - val_loss: 0.2447
Epoch 57/256
3063/3063 - 41s - loss: 0.1960 - val_loss: 0.2574
Epoch 58/256
3063/3063 - 42s - loss: 0.1956 - val_loss: 0.2480
Epoch 59/256
3063/3063 - 41s - loss: 0.1947 - val_loss: 0.2551
Epoch 60/256
3063/3063 - 42s - loss: 0.1942 - val_loss: 0.2610
Epoch 61/256
3063/3063 - 41s - loss: 0.1938 - val_loss: 0.2380
Epoch 62/256
3063/3063 - 42s - loss: 0.1909 - val_loss: 0.2439
Epoch 63/256
3063/3063 - 42s - loss: 0.1906 - val_loss: 0.2513
Epoch 64/256
3063/3063 - 42s - loss: 0.1901 - val_loss: 0.2569
Epoch 65/256
3063/3063 - 41s - loss: 0.1881 - val_loss: 0.2476
Epoch 66/256
3063/3063 - 42s - loss: 0.1869 - val_loss: 0.2574
Epoch 67/256
3063/3063 - 41s - loss: 0.1868 - val_loss: 0.2490
Epoch 68/256
3063/3063 - 41s - loss: 0.1848 - val_loss: 0.2796
Epoch 69/256
3063/3063 - 41s - loss: 0.1841 - val_loss: 0.2723
Epoch 70/256
3063/3063 - 41s - loss: 0.1826 - val_loss: 0.2574
Epoch 71/256
3063/3063 - 41s - loss: 0.1821 - val_loss: 0.2652
Epoch 72/256
3063/3063 - 41s - loss: 0.1803 - val_loss: 0.2615
Epoch 73/256
3063/3063 - 41s - loss: 0.1790 - val_loss: 0.2748
Epoch 74/256
3063/3063 - 41s - loss: 0.1789 - val_loss: 0.2521
Epoch 75/256
3063/3063 - 41s - loss: 0.1771 - val_loss: 0.2664
Epoch 76/256
3063/3063 - 41s - loss: 0.1755 - val_loss: 0.2457
Epoch 77/256
3063/3063 - 41s - loss: 0.1753 - val_loss: 0.2702
Epoch 78/256
3063/3063 - 41s - loss: 0.1747 - val_loss: 0.2476
Epoch 79/256
3063/3063 - 41s - loss: 0.1743 - val_loss: 0.2560
Epoch 80/256
3063/3063 - 41s - loss: 0.1707 - val_loss: 0.2573
Epoch 81/256
3063/3063 - 41s - loss: 0.1705 - val_loss: 0.2689
Epoch 82/256
3063/3063 - 41s - loss: 0.1696 - val_loss: 0.2649
Epoch 83/256
3063/3063 - 42s - loss: 0.1686 - val_loss: 0.2656
Epoch 84/256
3063/3063 - 42s - loss: 0.1662 - val_loss: 0.2834
Epoch 85/256
3063/3063 - 42s - loss: 0.1657 - val_loss: 0.2710
Epoch 86/256
3063/3063 - 41s - loss: 0.1652 - val_loss: 0.2826
Epoch 87/256
3063/3063 - 41s - loss: 0.1646 - val_loss: 0.2649
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 128)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 44s - loss: 0.3917 - val_loss: 0.3543
Epoch 2/256
3063/3063 - 41s - loss: 0.3588 - val_loss: 0.3433
Epoch 3/256
3063/3063 - 43s - loss: 0.3469 - val_loss: 0.3388
Epoch 4/256
3063/3063 - 41s - loss: 0.3370 - val_loss: 0.3466
Epoch 5/256
3063/3063 - 42s - loss: 0.3287 - val_loss: 0.3154
Epoch 6/256
3063/3063 - 41s - loss: 0.3174 - val_loss: 0.3134
Epoch 7/256
3063/3063 - 42s - loss: 0.3080 - val_loss: 0.3022
Epoch 8/256
3063/3063 - 42s - loss: 0.2966 - val_loss: 0.2960
Epoch 9/256
3063/3063 - 41s - loss: 0.2892 - val_loss: 0.2922
Epoch 10/256
3063/3063 - 42s - loss: 0.2834 - val_loss: 0.2889
Epoch 11/256
3063/3063 - 41s - loss: 0.2776 - val_loss: 0.2866
Epoch 12/256
3063/3063 - 41s - loss: 0.2730 - val_loss: 0.2789
Epoch 13/256
3063/3063 - 42s - loss: 0.2677 - val_loss: 0.2633
Epoch 14/256
3063/3063 - 41s - loss: 0.2650 - val_loss: 0.2682
Epoch 15/256
3063/3063 - 41s - loss: 0.2607 - val_loss: 0.2587
Epoch 16/256
3063/3063 - 42s - loss: 0.2572 - val_loss: 0.2664
Epoch 17/256
3063/3063 - 41s - loss: 0.2542 - val_loss: 0.2655
Epoch 18/256
3063/3063 - 42s - loss: 0.2526 - val_loss: 0.2524
Epoch 19/256
3063/3063 - 42s - loss: 0.2495 - val_loss: 0.2844
Epoch 20/256
3063/3063 - 42s - loss: 0.2471 - val_loss: 0.2607
Epoch 21/256
3063/3063 - 42s - loss: 0.2442 - val_loss: 0.2524
Epoch 22/256
3063/3063 - 42s - loss: 0.2427 - val_loss: 0.2506
Epoch 23/256
3063/3063 - 42s - loss: 0.2401 - val_loss: 0.2550
Epoch 24/256
3063/3063 - 42s - loss: 0.2390 - val_loss: 0.2678
Epoch 25/256
3063/3063 - 42s - loss: 0.2367 - val_loss: 0.2440
Epoch 26/256
3063/3063 - 42s - loss: 0.2364 - val_loss: 0.2456
Epoch 27/256
3063/3063 - 42s - loss: 0.2335 - val_loss: 0.2961
Epoch 28/256
3063/3063 - 42s - loss: 0.2319 - val_loss: 0.2553
Epoch 29/256
3063/3063 - 43s - loss: 0.2307 - val_loss: 0.2532
Epoch 30/256
3063/3063 - 42s - loss: 0.2296 - val_loss: 0.2561
Epoch 31/256
3063/3063 - 42s - loss: 0.2263 - val_loss: 0.2547
Epoch 32/256
3063/3063 - 42s - loss: 0.2266 - val_loss: 0.2426
Epoch 33/256
3063/3063 - 42s - loss: 0.2248 - val_loss: 0.2466
Epoch 34/256
3063/3063 - 41s - loss: 0.2230 - val_loss: 0.2510
Epoch 35/256
3063/3063 - 42s - loss: 0.2206 - val_loss: 0.2549
Epoch 36/256
3063/3063 - 42s - loss: 0.2210 - val_loss: 0.2487
Epoch 37/256
3063/3063 - 42s - loss: 0.2188 - val_loss: 0.2530
Epoch 38/256
3063/3063 - 42s - loss: 0.2185 - val_loss: 0.2419
Epoch 39/256
3063/3063 - 43s - loss: 0.2168 - val_loss: 0.2467
Epoch 40/256
3063/3063 - 42s - loss: 0.2154 - val_loss: 0.2462
Epoch 41/256
3063/3063 - 42s - loss: 0.2142 - val_loss: 0.2776
Epoch 42/256
3063/3063 - 42s - loss: 0.2133 - val_loss: 0.2531
Epoch 43/256
3063/3063 - 42s - loss: 0.2118 - val_loss: 0.2642
Epoch 44/256
3063/3063 - 42s - loss: 0.2095 - val_loss: 0.2490
Epoch 45/256
3063/3063 - 42s - loss: 0.2093 - val_loss: 0.2453
Epoch 46/256
3063/3063 - 42s - loss: 0.2083 - val_loss: 0.2436
Epoch 47/256
3063/3063 - 41s - loss: 0.2070 - val_loss: 0.2539
Epoch 48/256
3063/3063 - 42s - loss: 0.2062 - val_loss: 0.2467
Epoch 49/256
3063/3063 - 42s - loss: 0.2047 - val_loss: 0.2469
Epoch 50/256
3063/3063 - 41s - loss: 0.2036 - val_loss: 0.2510
Epoch 51/256
3063/3063 - 42s - loss: 0.2027 - val_loss: 0.2691
Epoch 52/256
3063/3063 - 41s - loss: 0.2022 - val_loss: 0.2450
Epoch 53/256
3063/3063 - 42s - loss: 0.2002 - val_loss: 0.2627
Epoch 54/256
3063/3063 - 42s - loss: 0.1993 - val_loss: 0.2516
Epoch 55/256
3063/3063 - 42s - loss: 0.1977 - val_loss: 0.2508
Epoch 56/256
3063/3063 - 43s - loss: 0.1970 - val_loss: 0.2505
Epoch 57/256
3063/3063 - 41s - loss: 0.1957 - val_loss: 0.2382
Epoch 58/256
3063/3063 - 41s - loss: 0.1947 - val_loss: 0.2584
Epoch 59/256
3063/3063 - 42s - loss: 0.1933 - val_loss: 0.2450
Epoch 60/256
3063/3063 - 41s - loss: 0.1923 - val_loss: 0.2433
Epoch 61/256
3063/3063 - 42s - loss: 0.1900 - val_loss: 0.2491
Epoch 62/256
3063/3063 - 42s - loss: 0.1895 - val_loss: 0.2678
Epoch 63/256
3063/3063 - 42s - loss: 0.1886 - val_loss: 0.2450
Epoch 64/256
3063/3063 - 42s - loss: 0.1871 - val_loss: 0.2511
Epoch 65/256
3063/3063 - 42s - loss: 0.1860 - val_loss: 0.2551
Epoch 66/256
3063/3063 - 41s - loss: 0.1846 - val_loss: 0.2644
Epoch 67/256
3063/3063 - 41s - loss: 0.1844 - val_loss: 0.2561
Epoch 68/256
3063/3063 - 42s - loss: 0.1832 - val_loss: 0.2555
Epoch 69/256
3063/3063 - 42s - loss: 0.1815 - val_loss: 0.2565
Epoch 70/256
3063/3063 - 42s - loss: 0.1805 - val_loss: 0.2582
Epoch 71/256
3063/3063 - 41s - loss: 0.1807 - val_loss: 0.2579
Epoch 72/256
3063/3063 - 42s - loss: 0.1774 - val_loss: 0.2628
Epoch 73/256
3063/3063 - 42s - loss: 0.1768 - val_loss: 0.2530
Epoch 74/256
3063/3063 - 41s - loss: 0.1756 - val_loss: 0.2529
Epoch 75/256
3063/3063 - 42s - loss: 0.1749 - val_loss: 0.2592
Epoch 76/256
3063/3063 - 42s - loss: 0.1738 - val_loss: 0.2556
Epoch 77/256
3063/3063 - 42s - loss: 0.1715 - val_loss: 0.2557
Epoch 78/256
3063/3063 - 42s - loss: 0.1710 - val_loss: 0.2597
Epoch 79/256
3063/3063 - 41s - loss: 0.1706 - val_loss: 0.2698
Epoch 80/256
3063/3063 - 42s - loss: 0.1695 - val_loss: 0.2670
Epoch 81/256
3063/3063 - 41s - loss: 0.1680 - val_loss: 0.2803
Epoch 82/256
3063/3063 - 42s - loss: 0.1671 - val_loss: 0.2579
Epoch 83/256
3063/3063 - 42s - loss: 0.1652 - val_loss: 0.2597
Epoch 84/256
3063/3063 - 42s - loss: 0.1642 - val_loss: 0.2843
Epoch 85/256
3063/3063 - 41s - loss: 0.1625 - val_loss: 0.2835
Epoch 86/256
3063/3063 - 42s - loss: 0.1616 - val_loss: 0.2858
Epoch 87/256
3063/3063 - 42s - loss: 0.1610 - val_loss: 0.2705
Epoch 88/256
3063/3063 - 42s - loss: 0.1588 - val_loss: 0.2864
Epoch 89/256
3063/3063 - 42s - loss: 0.1593 - val_loss: 0.2739
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 128)_64
[[0.9652737617523548, 0.965921060617619, 0.965152395055413, 0.9663206799313722, 0.9651593128982494, 0.9656965396172348, 0.9665684722617214, 0.9661979632217341]]
[[453.15151515 498.46666667 433.44927536 564.30188679 460.12307692
  498.46666667 564.30188679 388.41558442]]
[[135.89124261 149.41410785 129.98292772 168.75424296 137.90542439
  149.51349639 168.79174807 116.01353425]]
[[41.53888889 45.52207002 42.12394366 43.85337243 41.6545961  41.94670407
  45.11010558 43.03309353]]
[[29.07114848 31.85425456 29.46940256 30.69677778 29.15212661 29.35098368
  31.55548725 30.11830329]]
$2^7$ & $0.9658 \pm 0.0005$ & 117K & $482.6\pm 57.6$ & $144.5\pm 17.2$ & $43.1\pm 1.5$ & $30.2\pm 1.0$\\

