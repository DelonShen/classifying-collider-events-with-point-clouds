{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Models\n",
    "This first block of code trains the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Experimenter\n",
      "\tLoading Data from ../data/data100k_raw_combined_atlas_cut.pkl\n",
      "\tData Loaded\n",
      "\tCreating Splits\n",
      "\tSplits Created\n",
      "Done initalizing\n",
      "RIGHT NOW: pairwise\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nm \u001b[38;5;129;01min\u001b[39;00m to_train:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRIGHT NOW: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39mnm)\n\u001b[0;32m---> 53\u001b[0m     \u001b[43mPI\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_multijet_to_inv_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weight_invariant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maux_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdR_keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultijet_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     PI\u001b[38;5;241m.\u001b[39mtrain_classifier(nm, model_params_dict[nm], epochs\u001b[38;5;241m=\u001b[39mEPOCHS)\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m###\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/code/delon/pc_ec/paper_code/experiment.py:144\u001b[0m, in \u001b[0;36mExperimenter.data_loader\u001b[0;34m(self, data_key, data_generator, weight_generator, y_tensorizer, aux_params)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata_loader\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_key, data_generator, weight_generator, y_tensorizer, aux_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()):\n\u001b[1;32m    143\u001b[0m     X_train, y_train \u001b[38;5;241m=\u001b[39m data_generator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevents_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevents_oup_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevents_tag_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39maux_params)\n\u001b[0;32m--> 144\u001b[0m     X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mdata_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevents_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevents_oup_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevents_tag_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43maux_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets[data_key\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_weight\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;241m=\u001b[39m weight_generator(y_train)\n\u001b[1;32m    148\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(X_train)\n",
      "File \u001b[0;32m/code/delon/pc_ec/paper_code/utils.py:213\u001b[0m, in \u001b[0;36mgen_multijet_to_inv_dataset\u001b[0;34m(inp, oup, events_tag, multijet_n, dR_keep, pad, pad_n, systematics_test, MET_jet)\u001b[0m\n\u001b[1;32m    211\u001b[0m temp_jets_pt \u001b[38;5;241m=\u001b[39m [jet[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mPerp() \u001b[38;5;28;01mfor\u001b[39;00m jet \u001b[38;5;129;01min\u001b[39;00m inp[event_idx]]\n\u001b[1;32m    212\u001b[0m sort_idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(temp_jets_pt)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 213\u001b[0m temp_jets \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_jets\u001b[49m\u001b[43m)\u001b[49m[sort_idxs]\n\u001b[1;32m    215\u001b[0m n_jets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(temp_jets)\n\u001b[1;32m    217\u001b[0m jetP4_tot \u001b[38;5;241m=\u001b[39m TLorentzVector()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#to get latex to work \n",
    "os.environ['PATH'] = \"%s:/usr/local/cuda-11.2/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/anaconda/bin:/home/delon/texlive/bin/x86_64-linux:/home/delon/.local/bin:/home/delon/bin\"%os.environ['PATH']\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "\n",
    "import experiment \n",
    "from utils import *\n",
    "from Architectures import *\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "suppress_warnings()\n",
    "EPOCHS = 512\n",
    "\n",
    "os.environ['PYTHONHASHSEED']=str(0)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "filename = '../data/data100k_raw_combined_atlas_cut.pkl'\n",
    "num_round = None\n",
    "\n",
    "#TESTING\n",
    "EPOCHS = 1\n",
    "filename = '../data/data50k_raw_combined_atlas_cut_small.pkl'\n",
    "num_round = 1\n",
    "######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SUFFIX = ''\n",
    "PI = experiment.Experimenter(filename)\n",
    "\n",
    "to_train = ['pairwise',\n",
    "            'particlewise',\n",
    "#             'tripletwise',\n",
    "#             'pairwise_nl',\n",
    "#             'pairwise_nl_iter',\n",
    "#             'nested_concat',\n",
    "            'naivednn']\n",
    "\n",
    "for nm in to_train:\n",
    "    print('RIGHT NOW: %s'%nm)\n",
    "    PI.data_loader(nm, gen_multijet_to_inv_dataset, class_weight_invariant, tf.constant, aux_params=dict(dR_keep=False, multijet_n=1))\n",
    "    PI.train_classifier(nm, model_params_dict[nm], epochs=EPOCHS)\n",
    "    print('###')\n",
    "\n",
    "print('DNN Classifier')\n",
    "PI.data_loader('dnn', gen_dataset_high_level, class_weight_invariant, tf.constant)\n",
    "PI.train_classifier('dnn', model_params_dict[nm] , use_weights_during_fit = True, epochs=EPOCHS)\n",
    "print('###')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.41904303, 0.58095694]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = classifiers['pairwise'](**model_params_dict['pairwise'])\n",
    "test_model(tf.constant([[[random.random() for feature in range(7)] for partilce in range(15)]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we're gonna generate some graphs and get the performance of the archictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "matplotlib.rcParams['text.latex.preamble'] = [\n",
    "    r'\\usepackage{amsmath}',\n",
    "    r'\\usepackage{amssymb}',\n",
    "    r'\\usepackage{hyperref}',\n",
    "    ] \n",
    "\n",
    "suppress_warnings()\n",
    "EPOCHS = 64\n",
    "\n",
    "filename = 'data/data100k_raw_combined_atlas_cut.pkl'\n",
    "\n",
    "def human_format(num):\n",
    "    magnitude = 0\n",
    "    while abs(num) >= 1000:\n",
    "        magnitude += 1\n",
    "        num /= 1000.0\n",
    "    # add more suffixes if you need them\n",
    "    return '%.0f%s' % (num, ['', 'K', 'M', 'G', 'T', 'P'][magnitude])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "X_train, y_train = gen_dataset_high_level(PI.events_train, PI.events_oup_train, PI.events_tag_train) \n",
    "X_test, y_test = gen_dataset_high_level(PI.events_test, PI.events_oup_test, PI.events_tag_test) \n",
    "\n",
    "X_train = np.array(X_train)\n",
    "yo_train = np.array([np.argmax(y) for y in y_train])\n",
    "yo_test = np.array([np.argmax(y) for y in y_test])\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "xg_train = xgb.DMatrix(X_train, label=yo_train)\n",
    "xg_test = xgb.DMatrix(X_test, label=yo_test)\n",
    "\n",
    "\n",
    "# setup parameters for xgboost\n",
    "param = {'colsample_bytree': 0.7729268575934765, 'eta': 0.25, 'gamma': 1.002343020792451, 'max_depth': 10, 'min_child_weight': 9, 'n_estimators': 530, 'reg_alpha': 41.0, 'reg_lambda': 0.8554269844258477} #THESE SELECTED as optimal BY HYPEROPT \n",
    "\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softprob'\n",
    "param['num_class'] = 2\n",
    "\n",
    "if(num_round == None):\n",
    "    num_round = param['n_estimators']\n",
    "\n",
    "\n",
    "bst_filename = 'models/'+filename.split('.')[0].split('/')[-1]+'BDT.json'\n",
    "bst = None\n",
    "import pickle\n",
    "bst = xgb.train(param, xg_train, num_round)\n",
    "\n",
    "\n",
    "yhat_test = bst.predict(xg_test).reshape(yo_test.shape[0], 2)\n",
    "yhat_test = np.array([true for (true, false) in yhat_test])\n",
    "yop_test  = np.array([true for (true, false) in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(yop_test, yhat_test)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "import bisect \n",
    "\n",
    "location = bisect.bisect_left(list(reversed(thresholds)), 0.5)\n",
    "print('At 0.5 threshold we have BDT signal efficiency %.3f'%list(reversed(tpr))[location-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.size'] = 20\n",
    "plt.rcParams['figure.autolayout'] = False\n",
    "plt.rcParams['text.usetex'] = True\n",
    "\n",
    "models_to_plot = [PI.model_name_to_model(model_name) for model_name in list(PI.models.keys())]\n",
    "model_params_to_plot = [model_params_dict[PI.model_name_to_model(model_name)] for model_name in list(PI.models.keys())]\n",
    "model_params_to_plot\n",
    "\n",
    "\n",
    "fig, ax = PI.plot_multiple_sorted_by_AUC(models_to_plot, model_params_to_plot)\n",
    "colormap = sns.cubehelix_palette(start=26/10, light=.97, as_cmap=True)\n",
    "ax.plot(tpr, 1/fpr, label=r'%s'%('BDT + ATLAS Features'), color=colormap(0.33))                                         \n",
    "\n",
    "\n",
    "ax.get_legend().remove()\n",
    "ax.legend(loc='upper right', frameon=False, labelspacing=2.0)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "labels, handles = list(zip(*reversed(sorted(zip(labels, handles)))))\n",
    "ax.legend(handles, labels, loc='upper right', frameon=False)\n",
    "\n",
    "for txt in ax.texts:\n",
    "    txt.set_visible(False)\n",
    "\n",
    "annotation_string = r'\\textbf{ROC Curve for Event Classification}'\n",
    "annotation_string += '\\n'\n",
    "annotation_string += r'$t\\overline{t}(H\\rightarrow\\tau\\tau)$ and $t\\overline{t}(t\\rightarrow \\tau\\nu b)$'\n",
    "annotation_string += '\\n'\n",
    "annotation_string += r'\\textsc{MadGraph 5}+\\textsc{Pythia} 8+\\textsc{Delphes}'\n",
    "annotation_string += '\\n'\n",
    "annotation_string += r'Anti-Kt with $R=0.4$, $\\sqrt{s} = 14$'\n",
    "ax.text(.05,1, annotation_string)\n",
    "plt.gcf().set_size_inches(10, 10)\n",
    "\n",
    "fig.savefig('figures/roc_curves.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate table of data for performance at signal efficiency\n",
    "import bisect \n",
    "location = bisect.bisect_left(list(reversed(thresholds)), 0.5)\n",
    "\n",
    "table_file = open('figures/performance_table.tex', 'w+')\n",
    "table_data = [(PI.get_ROC(classifier_key, param)) for (classifier_key, param) in zip(models_to_plot, model_params_to_plot)]\n",
    "\n",
    "c_names = classifiers_name\n",
    "for (fpr, tpr, thresholds, auc), key, params in zip(table_data, models_to_plot, model_params_to_plot):\n",
    "    location_0 = bisect.bisect_left(tpr, 0.7)\n",
    "    location = bisect.bisect_left(tpr, 0.3)\n",
    "    c_model = PI.models['%s_%s'%(key, PI.get_tail_string(params))]\n",
    "    NPARAMS = human_format(c_model.count_params())\n",
    "    table_file.write('%s & %.3f & %s &%.1f & %.1f & %.1f & %.1f\\\\\\\\\\n'%(c_names[key], auc, NPARAMS,\n",
    "                                                           1/fpr[location-1], (tpr)[location-1]/fpr[location-1],\n",
    "                                                           1/fpr[location_0-1], (tpr)[location_0-1]/fpr[location_0-1]))\n",
    "    \n",
    "yhat_test = bst.predict(xg_test).reshape(yo_test.shape[0], 2)\n",
    "yhat_test = np.array([true for (true, false) in yhat_test])\n",
    "yop_test  = np.array([true for (true, false) in y_test])\n",
    "from sklearn import metrics\n",
    "fpr, tpr, thresholds = metrics.roc_curve(yop_test, yhat_test)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "location_0 = bisect.bisect_left(tpr, 0.7)\n",
    "location = bisect.bisect_left(tpr, 0.3)\n",
    "table_file.write('%s & %.3f & %s &%.1f & %.1f & %.1f & %.1f\\\\\\\\\\n'%('BDT + ATLAS Features', auc, NPARAMS,\n",
    "                                                       1/fpr[location-1], (tpr)[location-1]/fpr[location-1],\n",
    "                                                       1/fpr[location_0-1], (tpr)[location_0-1]/fpr[location_0-1]))\n",
    "\n",
    "table_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####GENERATING TSNE PLOT\n",
    "from tqdm import trange\n",
    "\n",
    "from scipy.stats import gaussian_kde \n",
    "from scipy.stats import kendalltau\n",
    "import seaborn as sns\n",
    "\n",
    "tail_string = PI.get_tail_string(model_params_dict['pairwise'])\n",
    "pairwise_model = PI.models['%s_%s'%('pairwise', tail_string)]\n",
    "latent_getter = LatentGetter(pairwise_model.layers[0:3], condensed=True)\n",
    "\n",
    "X_test, y_test = PI.get_test_dataset('pairwise')\n",
    "\n",
    "n_cut = int(len(X_test)*0.5)\n",
    "indices = np.random.permutation(len(X_test))\n",
    "cut = np.s_[indices[:n_cut]]\n",
    "latent_reps = latent_getter.predict(X_test.numpy()[cut])\n",
    "\n",
    "latent_label = y_test.numpy()[cut]\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "distance_matrix = pairwise_distances(latent_reps, latent_reps, metric='cosine', n_jobs=-1)\n",
    "latent_reps_embedded_tsne = TSNE(metric=\"precomputed\", n_components=2, learning_rate='auto', \n",
    "                                  verbose=2, perplexity=50, \n",
    "                                 n_iter=2000, n_jobs=-1)\n",
    "latent_reps_embedded = latent_reps_embedded_tsne.fit_transform(distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcec",
   "language": "python",
   "name": "pcec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
