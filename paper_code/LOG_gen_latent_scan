nohup: ignoring input
2022-05-19 16:55:29.561330: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-05-19 16:56:37.091121: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1
2022-05-19 16:56:37.106256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:84:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2022-05-19 16:56:37.107268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: 
pciBusID: 0000:85:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2022-05-19 16:56:37.107302: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-05-19 16:56:37.111387: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2022-05-19 16:56:37.111451: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
2022-05-19 16:56:37.113010: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10
2022-05-19 16:56:37.113318: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10
2022-05-19 16:56:37.117769: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11
2022-05-19 16:56:37.118631: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11
2022-05-19 16:56:37.118868: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2022-05-19 16:56:37.122660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1
2022-05-19 16:56:37.123388: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-19 16:56:37.246667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:84:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2022-05-19 16:56:37.247673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: 
pciBusID: 0000:85:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2022-05-19 16:56:37.251262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1
2022-05-19 16:56:37.251329: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-05-19 16:56:38.187794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-05-19 16:56:38.187854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1 
2022-05-19 16:56:38.187869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N Y 
2022-05-19 16:56:38.187877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   Y N 
2022-05-19 16:56:38.192778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8599 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:84:00.0, compute capability: 3.7)
2022-05-19 16:56:38.194671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10792 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id: 0000:85:00.0, compute capability: 3.7)
2022-05-19 16:56:38.636621: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
2022-05-19 16:56:38.637381: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2099935000 Hz
2022-05-19 16:56:39.618456: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2022-05-19 16:56:39.898981: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
Initializing Experimenter
	Loading Data from ../data/data100k_raw_combined_atlas_cut.pkl
	Data Loaded
	Creating Splits
	Splits Created
Done initalizing
DNN Classifier
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 10s - loss: 0.3947 - val_loss: 0.3558
Epoch 2/256
3063/3063 - 9s - loss: 0.3828 - val_loss: 0.3591
Epoch 3/256
3063/3063 - 9s - loss: 0.3818 - val_loss: 0.3667
Epoch 4/256
3063/3063 - 9s - loss: 0.3805 - val_loss: 0.3541
Epoch 5/256
3063/3063 - 9s - loss: 0.3782 - val_loss: 0.3497
Epoch 6/256
3063/3063 - 9s - loss: 0.3793 - val_loss: 0.3494
Epoch 7/256
3063/3063 - 9s - loss: 0.3763 - val_loss: 0.3536
Epoch 8/256
3063/3063 - 10s - loss: 0.3777 - val_loss: 0.3531
Epoch 9/256
3063/3063 - 9s - loss: 0.3765 - val_loss: 0.3516
Epoch 10/256
3063/3063 - 9s - loss: 0.3762 - val_loss: 0.3483
Epoch 11/256
3063/3063 - 9s - loss: 0.3755 - val_loss: 0.3495
Epoch 12/256
3063/3063 - 9s - loss: 0.3751 - val_loss: 0.3471
Epoch 13/256
3063/3063 - 9s - loss: 0.3757 - val_loss: 0.3519
Epoch 14/256
3063/3063 - 9s - loss: 0.3762 - val_loss: 0.3498
Epoch 15/256
3063/3063 - 8s - loss: 0.3747 - val_loss: 0.3510
Epoch 16/256
3063/3063 - 9s - loss: 0.3753 - val_loss: 0.3488
Epoch 17/256
3063/3063 - 9s - loss: 0.3753 - val_loss: 0.3465
Epoch 18/256
3063/3063 - 9s - loss: 0.3739 - val_loss: 0.3496
Epoch 19/256
3063/3063 - 9s - loss: 0.3743 - val_loss: 0.3504
Epoch 20/256
3063/3063 - 9s - loss: 0.3743 - val_loss: 0.3471
Epoch 21/256
3063/3063 - 9s - loss: 0.3734 - val_loss: 0.3503
Epoch 22/256
3063/3063 - 9s - loss: 0.3743 - val_loss: 0.3504
Epoch 23/256
3063/3063 - 9s - loss: 0.3727 - val_loss: 0.3480
Epoch 24/256
3063/3063 - 9s - loss: 0.3732 - val_loss: 0.3471
Epoch 25/256
3063/3063 - 9s - loss: 0.3728 - val_loss: 0.3495
Epoch 26/256
3063/3063 - 9s - loss: 0.3738 - val_loss: 0.3510
Epoch 27/256
3063/3063 - 9s - loss: 0.3745 - val_loss: 0.3477
Epoch 28/256
3063/3063 - 9s - loss: 0.3738 - val_loss: 0.3466
Epoch 29/256
3063/3063 - 9s - loss: 0.3734 - val_loss: 0.3490
Epoch 30/256
3063/3063 - 9s - loss: 0.3728 - val_loss: 0.3453
Epoch 31/256
3063/3063 - 9s - loss: 0.3736 - val_loss: 0.3465
Epoch 32/256
3063/3063 - 9s - loss: 0.3721 - val_loss: 0.3481
Epoch 33/256
3063/3063 - 9s - loss: 0.3737 - val_loss: 0.3476
Epoch 34/256
3063/3063 - 10s - loss: 0.3732 - val_loss: 0.3530
Epoch 35/256
3063/3063 - 9s - loss: 0.3722 - val_loss: 0.3466
Epoch 36/256
3063/3063 - 9s - loss: 0.3734 - val_loss: 0.3480
Epoch 37/256
3063/3063 - 9s - loss: 0.3714 - val_loss: 0.3492
Epoch 38/256
3063/3063 - 9s - loss: 0.3724 - val_loss: 0.3483
Epoch 39/256
3063/3063 - 9s - loss: 0.3735 - val_loss: 0.3523
Epoch 40/256
3063/3063 - 9s - loss: 0.3726 - val_loss: 0.3508
Epoch 41/256
3063/3063 - 9s - loss: 0.3712 - val_loss: 0.3467
Epoch 42/256
3063/3063 - 9s - loss: 0.3726 - val_loss: 0.3498
Epoch 43/256
3063/3063 - 9s - loss: 0.3725 - val_loss: 0.3477
Epoch 44/256
3063/3063 - 9s - loss: 0.3738 - val_loss: 0.3456
Epoch 45/256
3063/3063 - 9s - loss: 0.3724 - val_loss: 0.3507
Epoch 46/256
3063/3063 - 9s - loss: 0.3727 - val_loss: 0.3477
Epoch 47/256
3063/3063 - 9s - loss: 0.3723 - val_loss: 0.3478
Epoch 48/256
3063/3063 - 9s - loss: 0.3725 - val_loss: 0.3489
Epoch 49/256
3063/3063 - 9s - loss: 0.3725 - val_loss: 0.3504
Epoch 50/256
3063/3063 - 9s - loss: 0.3718 - val_loss: 0.3454
Epoch 51/256
3063/3063 - 9s - loss: 0.3730 - val_loss: 0.3491
Epoch 52/256
3063/3063 - 9s - loss: 0.3728 - val_loss: 0.3457
Epoch 53/256
3063/3063 - 9s - loss: 0.3711 - val_loss: 0.3486
Epoch 54/256
3063/3063 - 9s - loss: 0.3731 - val_loss: 0.3462
Epoch 55/256
3063/3063 - 9s - loss: 0.3738 - val_loss: 0.3472
Epoch 56/256
3063/3063 - 9s - loss: 0.3717 - val_loss: 0.3512
Epoch 57/256
3063/3063 - 9s - loss: 0.3707 - val_loss: 0.3463
Epoch 58/256
3063/3063 - 9s - loss: 0.3719 - val_loss: 0.3463
Epoch 59/256
3063/3063 - 9s - loss: 0.3725 - val_loss: 0.3463
Epoch 60/256
3063/3063 - 9s - loss: 0.3719 - val_loss: 0.3493
Epoch 61/256
3063/3063 - 9s - loss: 0.3710 - val_loss: 0.3468
Epoch 62/256
3063/3063 - 9s - loss: 0.3717 - val_loss: 0.3519
2022-05-19 17:07:14.525401: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2022-05-19 17:07:14.865504: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8201
###
		LATENT DIM 1
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 33s - loss: 0.4204 - val_loss: 0.3732
Epoch 2/256
3063/3063 - 30s - loss: 0.3728 - val_loss: 0.3879
Epoch 3/256
3063/3063 - 30s - loss: 0.3653 - val_loss: 0.3599
Epoch 4/256
3063/3063 - 30s - loss: 0.3609 - val_loss: 0.3605
Epoch 5/256
3063/3063 - 29s - loss: 0.3579 - val_loss: 0.3507
Epoch 6/256
3063/3063 - 29s - loss: 0.3549 - val_loss: 0.3485
Epoch 7/256
3063/3063 - 30s - loss: 0.3517 - val_loss: 0.3534
Epoch 8/256
3063/3063 - 29s - loss: 0.3482 - val_loss: 0.3493
Epoch 9/256
3063/3063 - 29s - loss: 0.3469 - val_loss: 0.3423
Epoch 10/256
3063/3063 - 29s - loss: 0.3446 - val_loss: 0.3435
Epoch 11/256
3063/3063 - 29s - loss: 0.3426 - val_loss: 0.3455
Epoch 12/256
3063/3063 - 29s - loss: 0.3409 - val_loss: 0.3493
Epoch 13/256
3063/3063 - 30s - loss: 0.3391 - val_loss: 0.3389
Epoch 14/256
3063/3063 - 29s - loss: 0.3364 - val_loss: 0.3379
Epoch 15/256
3063/3063 - 29s - loss: 0.3342 - val_loss: 0.3476
Epoch 16/256
3063/3063 - 30s - loss: 0.3325 - val_loss: 0.3378
Epoch 17/256
3063/3063 - 30s - loss: 0.3291 - val_loss: 0.3317
Epoch 18/256
3063/3063 - 30s - loss: 0.3271 - val_loss: 0.3427
Epoch 19/256
3063/3063 - 30s - loss: 0.3252 - val_loss: 0.3298
Epoch 20/256
3063/3063 - 30s - loss: 0.3228 - val_loss: 0.3228
Epoch 21/256
3063/3063 - 30s - loss: 0.3209 - val_loss: 0.3203
Epoch 22/256
3063/3063 - 30s - loss: 0.3190 - val_loss: 0.3757
Epoch 23/256
3063/3063 - 30s - loss: 0.3167 - val_loss: 0.3259
Epoch 24/256
3063/3063 - 30s - loss: 0.3150 - val_loss: 0.3272
Epoch 25/256
3063/3063 - 30s - loss: 0.3132 - val_loss: 0.3136
Epoch 26/256
3063/3063 - 30s - loss: 0.3116 - val_loss: 0.3154
Epoch 27/256
3063/3063 - 30s - loss: 0.3098 - val_loss: 0.3290
Epoch 28/256
3063/3063 - 30s - loss: 0.3092 - val_loss: 0.3135
Epoch 29/256
3063/3063 - 30s - loss: 0.3074 - val_loss: 0.3079
Epoch 30/256
3063/3063 - 30s - loss: 0.3060 - val_loss: 0.3089
Epoch 31/256
3063/3063 - 30s - loss: 0.3052 - val_loss: 0.3125
Epoch 32/256
3063/3063 - 30s - loss: 0.3042 - val_loss: 0.3086
Epoch 33/256
3063/3063 - 30s - loss: 0.3040 - val_loss: 0.3178
Epoch 34/256
3063/3063 - 30s - loss: 0.3020 - val_loss: 0.3203
Epoch 35/256
3063/3063 - 30s - loss: 0.3012 - val_loss: 0.3099
Epoch 36/256
3063/3063 - 30s - loss: 0.3001 - val_loss: 0.3073
Epoch 37/256
3063/3063 - 30s - loss: 0.2991 - val_loss: 0.3011
Epoch 38/256
3063/3063 - 30s - loss: 0.2978 - val_loss: 0.3183
Epoch 39/256
3063/3063 - 30s - loss: 0.2970 - val_loss: 0.3045
Epoch 40/256
3063/3063 - 30s - loss: 0.2961 - val_loss: 0.3080
Epoch 41/256
3063/3063 - 30s - loss: 0.2956 - val_loss: 0.2988
Epoch 42/256
3063/3063 - 30s - loss: 0.2951 - val_loss: 0.3016
Epoch 43/256
3063/3063 - 30s - loss: 0.2941 - val_loss: 0.2986
Epoch 44/256
3063/3063 - 30s - loss: 0.2936 - val_loss: 0.3013
Epoch 45/256
3063/3063 - 30s - loss: 0.2928 - val_loss: 0.2996
Epoch 46/256
3063/3063 - 30s - loss: 0.2914 - val_loss: 0.2983
Epoch 47/256
3063/3063 - 30s - loss: 0.2904 - val_loss: 0.2935
Epoch 48/256
3063/3063 - 30s - loss: 0.2894 - val_loss: 0.2977
Epoch 49/256
3063/3063 - 30s - loss: 0.2893 - val_loss: 0.3077
Epoch 50/256
3063/3063 - 30s - loss: 0.2880 - val_loss: 0.2977
Epoch 51/256
3063/3063 - 30s - loss: 0.2876 - val_loss: 0.3081
Epoch 52/256
3063/3063 - 30s - loss: 0.2873 - val_loss: 0.2973
Epoch 53/256
3063/3063 - 30s - loss: 0.2855 - val_loss: 0.2993
Epoch 54/256
3063/3063 - 30s - loss: 0.2853 - val_loss: 0.2935
Epoch 55/256
3063/3063 - 30s - loss: 0.2856 - val_loss: 0.2972
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 1)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 32s - loss: 0.4272 - val_loss: 0.3856
Epoch 2/256
3063/3063 - 30s - loss: 0.3783 - val_loss: 0.3668
Epoch 3/256
3063/3063 - 30s - loss: 0.3673 - val_loss: 0.3704
Epoch 4/256
3063/3063 - 30s - loss: 0.3617 - val_loss: 0.3874
Epoch 5/256
3063/3063 - 30s - loss: 0.3582 - val_loss: 0.3589
Epoch 6/256
3063/3063 - 30s - loss: 0.3541 - val_loss: 0.3585
Epoch 7/256
3063/3063 - 30s - loss: 0.3506 - val_loss: 0.3494
Epoch 8/256
3063/3063 - 30s - loss: 0.3494 - val_loss: 0.3461
Epoch 9/256
3063/3063 - 30s - loss: 0.3480 - val_loss: 0.3421
Epoch 10/256
3063/3063 - 30s - loss: 0.3447 - val_loss: 0.3407
Epoch 11/256
3063/3063 - 30s - loss: 0.3435 - val_loss: 0.3375
Epoch 12/256
3063/3063 - 30s - loss: 0.3410 - val_loss: 0.3352
Epoch 13/256
3063/3063 - 30s - loss: 0.3381 - val_loss: 0.3364
Epoch 14/256
3063/3063 - 30s - loss: 0.3374 - val_loss: 0.3454
Epoch 15/256
3063/3063 - 30s - loss: 0.3351 - val_loss: 0.3342
Epoch 16/256
3063/3063 - 30s - loss: 0.3329 - val_loss: 0.3314
Epoch 17/256
3063/3063 - 30s - loss: 0.3308 - val_loss: 0.3289
Epoch 18/256
3063/3063 - 30s - loss: 0.3287 - val_loss: 0.3338
Epoch 19/256
3063/3063 - 30s - loss: 0.3262 - val_loss: 0.3417
Epoch 20/256
3063/3063 - 30s - loss: 0.3233 - val_loss: 0.3226
Epoch 21/256
3063/3063 - 30s - loss: 0.3220 - val_loss: 0.3478
Epoch 22/256
3063/3063 - 30s - loss: 0.3198 - val_loss: 0.3255
Epoch 23/256
3063/3063 - 30s - loss: 0.3185 - val_loss: 0.3193
Epoch 24/256
3063/3063 - 30s - loss: 0.3163 - val_loss: 0.3226
Epoch 25/256
3063/3063 - 30s - loss: 0.3153 - val_loss: 0.3154
Epoch 26/256
3063/3063 - 30s - loss: 0.3146 - val_loss: 0.3247
Epoch 27/256
3063/3063 - 30s - loss: 0.3138 - val_loss: 0.3134
Epoch 28/256
3063/3063 - 30s - loss: 0.3125 - val_loss: 0.3291
Epoch 29/256
3063/3063 - 30s - loss: 0.3109 - val_loss: 0.3193
Epoch 30/256
3063/3063 - 30s - loss: 0.3095 - val_loss: 0.3140
Epoch 31/256
3063/3063 - 30s - loss: 0.3082 - val_loss: 0.3138
Epoch 32/256
3063/3063 - 30s - loss: 0.3078 - val_loss: 0.3083
Epoch 33/256
3063/3063 - 30s - loss: 0.3065 - val_loss: 0.3190
Epoch 34/256
3063/3063 - 30s - loss: 0.3060 - val_loss: 0.3069
Epoch 35/256
3063/3063 - 30s - loss: 0.3049 - val_loss: 0.3089
Epoch 36/256
3063/3063 - 29s - loss: 0.3040 - val_loss: 0.3064
Epoch 37/256
3063/3063 - 30s - loss: 0.3029 - val_loss: 0.3095
Epoch 38/256
3063/3063 - 29s - loss: 0.3026 - val_loss: 0.3042
Epoch 39/256
3063/3063 - 30s - loss: 0.3029 - val_loss: 0.3056
Epoch 40/256
3063/3063 - 30s - loss: 0.3011 - val_loss: 0.3058
Epoch 41/256
3063/3063 - 30s - loss: 0.3004 - val_loss: 0.3166
Epoch 42/256
3063/3063 - 30s - loss: 0.2999 - val_loss: 0.3024
Epoch 43/256
3063/3063 - 30s - loss: 0.2989 - val_loss: 0.3116
Epoch 44/256
3063/3063 - 30s - loss: 0.2988 - val_loss: 0.3023
Epoch 45/256
3063/3063 - 30s - loss: 0.2971 - val_loss: 0.3088
Epoch 46/256
3063/3063 - 30s - loss: 0.2966 - val_loss: 0.3048
Epoch 47/256
3063/3063 - 30s - loss: 0.2961 - val_loss: 0.3016
Epoch 48/256
3063/3063 - 30s - loss: 0.2952 - val_loss: 0.3004
Epoch 49/256
3063/3063 - 30s - loss: 0.2947 - val_loss: 0.3067
Epoch 50/256
3063/3063 - 30s - loss: 0.2939 - val_loss: 0.2996
Epoch 51/256
3063/3063 - 30s - loss: 0.2928 - val_loss: 0.3006
Epoch 52/256
3063/3063 - 30s - loss: 0.2920 - val_loss: 0.3353
Epoch 53/256
3063/3063 - 30s - loss: 0.2914 - val_loss: 0.2979
Epoch 54/256
3063/3063 - 30s - loss: 0.2908 - val_loss: 0.3002
Epoch 55/256
3063/3063 - 30s - loss: 0.2902 - val_loss: 0.3096
Epoch 56/256
3063/3063 - 30s - loss: 0.2897 - val_loss: 0.2987
Epoch 57/256
3063/3063 - 30s - loss: 0.2894 - val_loss: 0.2972
Epoch 58/256
3063/3063 - 30s - loss: 0.2883 - val_loss: 0.2970
Epoch 59/256
3063/3063 - 30s - loss: 0.2885 - val_loss: 0.2970
Epoch 60/256
3063/3063 - 30s - loss: 0.2874 - val_loss: 0.2935
Epoch 61/256
3063/3063 - 30s - loss: 0.2875 - val_loss: 0.2946
Epoch 62/256
3063/3063 - 30s - loss: 0.2857 - val_loss: 0.2943
Epoch 63/256
3063/3063 - 30s - loss: 0.2856 - val_loss: 0.2977
Epoch 64/256
3063/3063 - 30s - loss: 0.2847 - val_loss: 0.2936
Epoch 65/256
3063/3063 - 30s - loss: 0.2834 - val_loss: 0.3068
Epoch 66/256
3063/3063 - 30s - loss: 0.2831 - val_loss: 0.2912
Epoch 67/256
3063/3063 - 30s - loss: 0.2829 - val_loss: 0.2920
Epoch 68/256
3063/3063 - 30s - loss: 0.2827 - val_loss: 0.2932
Epoch 69/256
3063/3063 - 30s - loss: 0.2815 - val_loss: 0.2898
Epoch 70/256
3063/3063 - 30s - loss: 0.2798 - val_loss: 0.3045
Epoch 71/256
3063/3063 - 30s - loss: 0.2802 - val_loss: 0.2890
Epoch 72/256
3063/3063 - 30s - loss: 0.2799 - val_loss: 0.2891
Epoch 73/256
3063/3063 - 30s - loss: 0.2808 - val_loss: 0.2867
Epoch 74/256
3063/3063 - 30s - loss: 0.2787 - val_loss: 0.2942
Epoch 75/256
3063/3063 - 30s - loss: 0.2783 - val_loss: 0.2865
Epoch 76/256
3063/3063 - 30s - loss: 0.2775 - val_loss: 0.3303
Epoch 77/256
3063/3063 - 30s - loss: 0.2773 - val_loss: 0.3083
Epoch 78/256
3063/3063 - 30s - loss: 0.2758 - val_loss: 0.2932
Epoch 79/256
3063/3063 - 30s - loss: 0.2751 - val_loss: 0.2910
Epoch 80/256
3063/3063 - 30s - loss: 0.2752 - val_loss: 0.2890
Epoch 81/256
3063/3063 - 30s - loss: 0.2745 - val_loss: 0.2880
Epoch 82/256
3063/3063 - 30s - loss: 0.2736 - val_loss: 0.2845
Epoch 83/256
3063/3063 - 30s - loss: 0.2730 - val_loss: 0.2817
Epoch 84/256
3063/3063 - 30s - loss: 0.2726 - val_loss: 0.2853
Epoch 85/256
3063/3063 - 30s - loss: 0.2726 - val_loss: 0.2822
Epoch 86/256
3063/3063 - 30s - loss: 0.2713 - val_loss: 0.2899
Epoch 87/256
3063/3063 - 30s - loss: 0.2711 - val_loss: 0.2834
Epoch 88/256
3063/3063 - 30s - loss: 0.2706 - val_loss: 0.2853
Epoch 89/256
3063/3063 - 30s - loss: 0.2700 - val_loss: 0.2840
Epoch 90/256
3063/3063 - 30s - loss: 0.2695 - val_loss: 0.2830
Epoch 91/256
3063/3063 - 30s - loss: 0.2697 - val_loss: 0.3165
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 1)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 32s - loss: 0.4141 - val_loss: 0.3767
Epoch 2/256
3063/3063 - 30s - loss: 0.3745 - val_loss: 0.3640
Epoch 3/256
3063/3063 - 30s - loss: 0.3655 - val_loss: 0.3628
Epoch 4/256
3063/3063 - 30s - loss: 0.3608 - val_loss: 0.3502
Epoch 5/256
3063/3063 - 30s - loss: 0.3564 - val_loss: 0.3476
Epoch 6/256
3063/3063 - 30s - loss: 0.3535 - val_loss: 0.3548
Epoch 7/256
3063/3063 - 30s - loss: 0.3513 - val_loss: 0.3510
Epoch 8/256
3063/3063 - 30s - loss: 0.3481 - val_loss: 0.3502
Epoch 9/256
3063/3063 - 30s - loss: 0.3467 - val_loss: 0.3503
Epoch 10/256
3063/3063 - 30s - loss: 0.3448 - val_loss: 0.3395
Epoch 11/256
3063/3063 - 30s - loss: 0.3425 - val_loss: 0.3574
Epoch 12/256
3063/3063 - 30s - loss: 0.3406 - val_loss: 0.3469
Epoch 13/256
3063/3063 - 30s - loss: 0.3377 - val_loss: 0.3354
Epoch 14/256
3063/3063 - 30s - loss: 0.3351 - val_loss: 0.3341
Epoch 15/256
3063/3063 - 30s - loss: 0.3345 - val_loss: 0.3338
Epoch 16/256
3063/3063 - 30s - loss: 0.3319 - val_loss: 0.3288
Epoch 17/256
3063/3063 - 30s - loss: 0.3301 - val_loss: 0.3301
Epoch 18/256
3063/3063 - 30s - loss: 0.3286 - val_loss: 0.3426
Epoch 19/256
3063/3063 - 30s - loss: 0.3270 - val_loss: 0.3428
Epoch 20/256
3063/3063 - 30s - loss: 0.3236 - val_loss: 0.3267
Epoch 21/256
3063/3063 - 30s - loss: 0.3232 - val_loss: 0.3299
Epoch 22/256
3063/3063 - 30s - loss: 0.3195 - val_loss: 0.3766
Epoch 23/256
3063/3063 - 30s - loss: 0.3176 - val_loss: 0.3188
Epoch 24/256
3063/3063 - 30s - loss: 0.3154 - val_loss: 0.3195
Epoch 25/256
3063/3063 - 30s - loss: 0.3141 - val_loss: 0.3172
Epoch 26/256
3063/3063 - 30s - loss: 0.3117 - val_loss: 0.3266
Epoch 27/256
3063/3063 - 30s - loss: 0.3103 - val_loss: 0.3126
Epoch 28/256
3063/3063 - 30s - loss: 0.3096 - val_loss: 0.3194
Epoch 29/256
3063/3063 - 30s - loss: 0.3078 - val_loss: 0.3111
Epoch 30/256
3063/3063 - 30s - loss: 0.3066 - val_loss: 0.3158
Epoch 31/256
3063/3063 - 30s - loss: 0.3054 - val_loss: 0.3068
Epoch 32/256
3063/3063 - 30s - loss: 0.3044 - val_loss: 0.3180
Epoch 33/256
3063/3063 - 30s - loss: 0.3031 - val_loss: 0.3149
Epoch 34/256
3063/3063 - 30s - loss: 0.3019 - val_loss: 0.3135
Epoch 35/256
3063/3063 - 30s - loss: 0.3002 - val_loss: 0.3053
Epoch 36/256
3063/3063 - 30s - loss: 0.3005 - val_loss: 0.3071
Epoch 37/256
3063/3063 - 30s - loss: 0.2985 - val_loss: 0.3257
Epoch 38/256
3063/3063 - 30s - loss: 0.2984 - val_loss: 0.3014
Epoch 39/256
3063/3063 - 30s - loss: 0.2978 - val_loss: 0.3039
Epoch 40/256
3063/3063 - 30s - loss: 0.2964 - val_loss: 0.3054
Epoch 41/256
3063/3063 - 30s - loss: 0.2964 - val_loss: 0.3089
Epoch 42/256
3063/3063 - 30s - loss: 0.2944 - val_loss: 0.3027
Epoch 43/256
3063/3063 - 30s - loss: 0.2942 - val_loss: 0.3011
Epoch 44/256
3063/3063 - 30s - loss: 0.2933 - val_loss: 0.3004
Epoch 45/256
3063/3063 - 30s - loss: 0.2924 - val_loss: 0.2968
Epoch 46/256
3063/3063 - 30s - loss: 0.2922 - val_loss: 0.3066
Epoch 47/256
3063/3063 - 30s - loss: 0.2909 - val_loss: 0.2941
Epoch 48/256
3063/3063 - 30s - loss: 0.2904 - val_loss: 0.2942
Epoch 49/256
3063/3063 - 30s - loss: 0.2897 - val_loss: 0.2982
Epoch 50/256
3063/3063 - 30s - loss: 0.2886 - val_loss: 0.2988
Epoch 51/256
3063/3063 - 30s - loss: 0.2886 - val_loss: 0.3011
Epoch 52/256
3063/3063 - 30s - loss: 0.2875 - val_loss: 0.2976
Epoch 53/256
3063/3063 - 30s - loss: 0.2879 - val_loss: 0.2934
Epoch 54/256
3063/3063 - 30s - loss: 0.2863 - val_loss: 0.2971
Epoch 55/256
3063/3063 - 30s - loss: 0.2865 - val_loss: 0.2975
Epoch 56/256
3063/3063 - 30s - loss: 0.2855 - val_loss: 0.2959
Epoch 57/256
3063/3063 - 30s - loss: 0.2848 - val_loss: 0.2966
Epoch 58/256
3063/3063 - 30s - loss: 0.2839 - val_loss: 0.3117
Epoch 59/256
3063/3063 - 30s - loss: 0.2834 - val_loss: 0.2973
Epoch 60/256
3063/3063 - 30s - loss: 0.2826 - val_loss: 0.3014
Epoch 61/256
3063/3063 - 30s - loss: 0.2826 - val_loss: 0.2925
Epoch 62/256
3063/3063 - 30s - loss: 0.2820 - val_loss: 0.2936
Epoch 63/256
3063/3063 - 30s - loss: 0.2820 - val_loss: 0.2909
Epoch 64/256
3063/3063 - 30s - loss: 0.2809 - val_loss: 0.2998
Epoch 65/256
3063/3063 - 30s - loss: 0.2797 - val_loss: 0.2936
Epoch 66/256
3063/3063 - 30s - loss: 0.2800 - val_loss: 0.3333
Epoch 67/256
3063/3063 - 30s - loss: 0.2797 - val_loss: 0.2913
Epoch 68/256
3063/3063 - 30s - loss: 0.2785 - val_loss: 0.2865
Epoch 69/256
3063/3063 - 30s - loss: 0.2786 - val_loss: 0.2910
Epoch 70/256
3063/3063 - 30s - loss: 0.2779 - val_loss: 0.2985
Epoch 71/256
3063/3063 - 30s - loss: 0.2766 - val_loss: 0.2915
Epoch 72/256
3063/3063 - 30s - loss: 0.2760 - val_loss: 0.2883
Epoch 73/256
3063/3063 - 30s - loss: 0.2761 - val_loss: 0.2911
Epoch 74/256
3063/3063 - 30s - loss: 0.2757 - val_loss: 0.2891
Epoch 75/256
3063/3063 - 30s - loss: 0.2751 - val_loss: 0.2912
Epoch 76/256
3063/3063 - 30s - loss: 0.2748 - val_loss: 0.2989
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 1)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 32s - loss: 0.3954 - val_loss: 0.3655
Epoch 2/256
3063/3063 - 30s - loss: 0.3655 - val_loss: 0.3515
Epoch 3/256
3063/3063 - 30s - loss: 0.3555 - val_loss: 0.3577
Epoch 4/256
3063/3063 - 30s - loss: 0.3481 - val_loss: 0.3540
Epoch 5/256
3063/3063 - 30s - loss: 0.3462 - val_loss: 0.3408
Epoch 6/256
3063/3063 - 30s - loss: 0.3414 - val_loss: 0.3358
Epoch 7/256
3063/3063 - 30s - loss: 0.3390 - val_loss: 0.3366
Epoch 8/256
3063/3063 - 30s - loss: 0.3361 - val_loss: 0.3330
Epoch 9/256
3063/3063 - 30s - loss: 0.3318 - val_loss: 0.3371
Epoch 10/256
3063/3063 - 30s - loss: 0.3286 - val_loss: 0.3391
Epoch 11/256
3063/3063 - 30s - loss: 0.3251 - val_loss: 0.3212
Epoch 12/256
3063/3063 - 30s - loss: 0.3203 - val_loss: 0.3198
Epoch 13/256
3063/3063 - 30s - loss: 0.3176 - val_loss: 0.3277
Epoch 14/256
3063/3063 - 30s - loss: 0.3154 - val_loss: 0.3112
Epoch 15/256
3063/3063 - 30s - loss: 0.3125 - val_loss: 0.3113
Epoch 16/256
3063/3063 - 30s - loss: 0.3097 - val_loss: 0.3204
Epoch 17/256
3063/3063 - 30s - loss: 0.3082 - val_loss: 0.3089
Epoch 18/256
3063/3063 - 30s - loss: 0.3060 - val_loss: 0.3030
Epoch 19/256
3063/3063 - 30s - loss: 0.3045 - val_loss: 0.3063
Epoch 20/256
3063/3063 - 30s - loss: 0.3024 - val_loss: 0.3193
Epoch 21/256
3063/3063 - 30s - loss: 0.3028 - val_loss: 0.3024
Epoch 22/256
3063/3063 - 30s - loss: 0.3011 - val_loss: 0.3019
Epoch 23/256
3063/3063 - 30s - loss: 0.2999 - val_loss: 0.3074
Epoch 24/256
3063/3063 - 30s - loss: 0.2988 - val_loss: 0.3128
Epoch 25/256
3063/3063 - 30s - loss: 0.2975 - val_loss: 0.2992
Epoch 26/256
3063/3063 - 30s - loss: 0.2963 - val_loss: 0.3006
Epoch 27/256
3063/3063 - 30s - loss: 0.2960 - val_loss: 0.3364
Epoch 28/256
3063/3063 - 30s - loss: 0.2951 - val_loss: 0.3069
Epoch 29/256
3063/3063 - 30s - loss: 0.2931 - val_loss: 0.2965
Epoch 30/256
3063/3063 - 30s - loss: 0.2929 - val_loss: 0.2964
Epoch 31/256
3063/3063 - 30s - loss: 0.2919 - val_loss: 0.3008
Epoch 32/256
3063/3063 - 30s - loss: 0.2916 - val_loss: 0.2967
Epoch 33/256
3063/3063 - 30s - loss: 0.2914 - val_loss: 0.3079
Epoch 34/256
3063/3063 - 30s - loss: 0.2897 - val_loss: 0.2976
Epoch 35/256
3063/3063 - 30s - loss: 0.2893 - val_loss: 0.2944
Epoch 36/256
3063/3063 - 30s - loss: 0.2878 - val_loss: 0.2940
Epoch 37/256
3063/3063 - 30s - loss: 0.2880 - val_loss: 0.2961
Epoch 38/256
3063/3063 - 30s - loss: 0.2876 - val_loss: 0.2933
Epoch 39/256
3063/3063 - 30s - loss: 0.2859 - val_loss: 0.2966
Epoch 40/256
3063/3063 - 30s - loss: 0.2858 - val_loss: 0.2931
Epoch 41/256
3063/3063 - 30s - loss: 0.2857 - val_loss: 0.3107
Epoch 42/256
3063/3063 - 30s - loss: 0.2842 - val_loss: 0.2882
Epoch 43/256
3063/3063 - 30s - loss: 0.2842 - val_loss: 0.2925
Epoch 44/256
3063/3063 - 30s - loss: 0.2838 - val_loss: 0.2919
Epoch 45/256
3063/3063 - 30s - loss: 0.2837 - val_loss: 0.2956
Epoch 46/256
3063/3063 - 30s - loss: 0.2818 - val_loss: 0.2903
Epoch 47/256
3063/3063 - 30s - loss: 0.2818 - val_loss: 0.3257
Epoch 48/256
3063/3063 - 30s - loss: 0.2816 - val_loss: 0.2961
Epoch 49/256
3063/3063 - 30s - loss: 0.2800 - val_loss: 0.2877
Epoch 50/256
3063/3063 - 30s - loss: 0.2799 - val_loss: 0.3020
Epoch 51/256
3063/3063 - 30s - loss: 0.2793 - val_loss: 0.2911
Epoch 52/256
3063/3063 - 30s - loss: 0.2789 - val_loss: 0.2875
Epoch 53/256
3063/3063 - 30s - loss: 0.2780 - val_loss: 0.2953
Epoch 54/256
3063/3063 - 30s - loss: 0.2767 - val_loss: 0.2867
Epoch 55/256
3063/3063 - 30s - loss: 0.2769 - val_loss: 0.2877
Epoch 56/256
3063/3063 - 30s - loss: 0.2774 - val_loss: 0.2866
Epoch 57/256
3063/3063 - 30s - loss: 0.2760 - val_loss: 0.2965
Epoch 58/256
3063/3063 - 29s - loss: 0.2757 - val_loss: 0.2919
Epoch 59/256
3063/3063 - 30s - loss: 0.2753 - val_loss: 0.2988
Epoch 60/256
3063/3063 - 30s - loss: 0.2743 - val_loss: 0.2868
Epoch 61/256
3063/3063 - 29s - loss: 0.2736 - val_loss: 0.2864
Epoch 62/256
3063/3063 - 29s - loss: 0.2732 - val_loss: 0.2930
Epoch 63/256
3063/3063 - 30s - loss: 0.2727 - val_loss: 0.2842
Epoch 64/256
3063/3063 - 30s - loss: 0.2725 - val_loss: 0.2985
Epoch 65/256
3063/3063 - 30s - loss: 0.2727 - val_loss: 0.2922
Epoch 66/256
3063/3063 - 30s - loss: 0.2704 - val_loss: 0.2923
Epoch 67/256
3063/3063 - 30s - loss: 0.2705 - val_loss: 0.3035
Epoch 68/256
3063/3063 - 30s - loss: 0.2708 - val_loss: 0.2979
Epoch 69/256
3063/3063 - 30s - loss: 0.2696 - val_loss: 0.2875
Epoch 70/256
3063/3063 - 30s - loss: 0.2694 - val_loss: 0.2879
Epoch 71/256
3063/3063 - 30s - loss: 0.2689 - val_loss: 0.2926
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 1)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.4242 - val_loss: 0.3813
Epoch 2/256
3063/3063 - 29s - loss: 0.3763 - val_loss: 0.3648
Epoch 3/256
3063/3063 - 29s - loss: 0.3656 - val_loss: 0.3556
Epoch 4/256
3063/3063 - 30s - loss: 0.3597 - val_loss: 0.3578
Epoch 5/256
3063/3063 - 30s - loss: 0.3571 - val_loss: 0.3569
Epoch 6/256
3063/3063 - 30s - loss: 0.3542 - val_loss: 0.3715
Epoch 7/256
3063/3063 - 30s - loss: 0.3535 - val_loss: 0.3568
Epoch 8/256
3063/3063 - 30s - loss: 0.3501 - val_loss: 0.3470
Epoch 9/256
3063/3063 - 30s - loss: 0.3477 - val_loss: 0.3652
Epoch 10/256
3063/3063 - 30s - loss: 0.3448 - val_loss: 0.3519
Epoch 11/256
3063/3063 - 30s - loss: 0.3425 - val_loss: 0.3395
Epoch 12/256
3063/3063 - 30s - loss: 0.3410 - val_loss: 0.3364
Epoch 13/256
3063/3063 - 30s - loss: 0.3393 - val_loss: 0.3550
Epoch 14/256
3063/3063 - 30s - loss: 0.3376 - val_loss: 0.3377
Epoch 15/256
3063/3063 - 30s - loss: 0.3364 - val_loss: 0.3363
Epoch 16/256
3063/3063 - 30s - loss: 0.3346 - val_loss: 0.3440
Epoch 17/256
3063/3063 - 30s - loss: 0.3330 - val_loss: 0.3318
Epoch 18/256
3063/3063 - 30s - loss: 0.3316 - val_loss: 0.3337
Epoch 19/256
3063/3063 - 30s - loss: 0.3304 - val_loss: 0.3496
Epoch 20/256
3063/3063 - 30s - loss: 0.3281 - val_loss: 0.3493
Epoch 21/256
3063/3063 - 30s - loss: 0.3262 - val_loss: 0.3328
Epoch 22/256
3063/3063 - 30s - loss: 0.3236 - val_loss: 0.3216
Epoch 23/256
3063/3063 - 30s - loss: 0.3209 - val_loss: 0.3243
Epoch 24/256
3063/3063 - 30s - loss: 0.3186 - val_loss: 0.3290
Epoch 25/256
3063/3063 - 30s - loss: 0.3166 - val_loss: 0.3201
Epoch 26/256
3063/3063 - 30s - loss: 0.3141 - val_loss: 0.3298
Epoch 27/256
3063/3063 - 30s - loss: 0.3134 - val_loss: 0.3164
Epoch 28/256
3063/3063 - 30s - loss: 0.3103 - val_loss: 0.3121
Epoch 29/256
3063/3063 - 30s - loss: 0.3093 - val_loss: 0.3157
Epoch 30/256
3063/3063 - 30s - loss: 0.3083 - val_loss: 0.3158
Epoch 31/256
3063/3063 - 30s - loss: 0.3061 - val_loss: 0.3080
Epoch 32/256
3063/3063 - 30s - loss: 0.3055 - val_loss: 0.3063
Epoch 33/256
3063/3063 - 30s - loss: 0.3040 - val_loss: 0.3062
Epoch 34/256
3063/3063 - 30s - loss: 0.3037 - val_loss: 0.3067
Epoch 35/256
3063/3063 - 30s - loss: 0.3027 - val_loss: 0.3049
Epoch 36/256
3063/3063 - 30s - loss: 0.3015 - val_loss: 0.3038
Epoch 37/256
3063/3063 - 30s - loss: 0.3004 - val_loss: 0.3222
Epoch 38/256
3063/3063 - 30s - loss: 0.3000 - val_loss: 0.3053
Epoch 39/256
3063/3063 - 30s - loss: 0.2990 - val_loss: 0.3218
Epoch 40/256
3063/3063 - 30s - loss: 0.2983 - val_loss: 0.3030
Epoch 41/256
3063/3063 - 30s - loss: 0.2965 - val_loss: 0.3048
Epoch 42/256
3063/3063 - 30s - loss: 0.2964 - val_loss: 0.3033
Epoch 43/256
3063/3063 - 30s - loss: 0.2955 - val_loss: 0.3043
Epoch 44/256
3063/3063 - 30s - loss: 0.2952 - val_loss: 0.3084
Epoch 45/256
3063/3063 - 30s - loss: 0.2947 - val_loss: 0.3250
Epoch 46/256
3063/3063 - 30s - loss: 0.2941 - val_loss: 0.3324
Epoch 47/256
3063/3063 - 30s - loss: 0.2940 - val_loss: 0.3099
Epoch 48/256
3063/3063 - 30s - loss: 0.2920 - val_loss: 0.2985
Epoch 49/256
3063/3063 - 30s - loss: 0.2911 - val_loss: 0.2954
Epoch 50/256
3063/3063 - 30s - loss: 0.2909 - val_loss: 0.3017
Epoch 51/256
3063/3063 - 30s - loss: 0.2904 - val_loss: 0.2964
Epoch 52/256
3063/3063 - 30s - loss: 0.2895 - val_loss: 0.3012
Epoch 53/256
3063/3063 - 30s - loss: 0.2894 - val_loss: 0.3201
Epoch 54/256
3063/3063 - 30s - loss: 0.2881 - val_loss: 0.3006
Epoch 55/256
3063/3063 - 30s - loss: 0.2879 - val_loss: 0.2946
Epoch 56/256
3063/3063 - 30s - loss: 0.2873 - val_loss: 0.3070
Epoch 57/256
3063/3063 - 30s - loss: 0.2868 - val_loss: 0.3033
Epoch 58/256
3063/3063 - 30s - loss: 0.2860 - val_loss: 0.2951
Epoch 59/256
3063/3063 - 30s - loss: 0.2858 - val_loss: 0.3149
Epoch 60/256
3063/3063 - 30s - loss: 0.2850 - val_loss: 0.2949
Epoch 61/256
3063/3063 - 30s - loss: 0.2840 - val_loss: 0.3009
Epoch 62/256
3063/3063 - 30s - loss: 0.2842 - val_loss: 0.2922
Epoch 63/256
3063/3063 - 30s - loss: 0.2830 - val_loss: 0.2964
Epoch 64/256
3063/3063 - 30s - loss: 0.2833 - val_loss: 0.3012
Epoch 65/256
3063/3063 - 30s - loss: 0.2819 - val_loss: 0.3014
Epoch 66/256
3063/3063 - 30s - loss: 0.2821 - val_loss: 0.2962
Epoch 67/256
3063/3063 - 30s - loss: 0.2817 - val_loss: 0.2951
Epoch 68/256
3063/3063 - 30s - loss: 0.2805 - val_loss: 0.2972
Epoch 69/256
3063/3063 - 30s - loss: 0.2804 - val_loss: 0.3005
Epoch 70/256
3063/3063 - 30s - loss: 0.2800 - val_loss: 0.2944
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 1)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.4277 - val_loss: 0.3902
Epoch 2/256
3063/3063 - 30s - loss: 0.3777 - val_loss: 0.3687
Epoch 3/256
3063/3063 - 30s - loss: 0.3655 - val_loss: 0.3773
Epoch 4/256
3063/3063 - 30s - loss: 0.3595 - val_loss: 0.3553
Epoch 5/256
3063/3063 - 30s - loss: 0.3554 - val_loss: 0.3506
Epoch 6/256
3063/3063 - 30s - loss: 0.3536 - val_loss: 0.3554
Epoch 7/256
3063/3063 - 30s - loss: 0.3509 - val_loss: 0.3577
Epoch 8/256
3063/3063 - 30s - loss: 0.3491 - val_loss: 0.3559
Epoch 9/256
3063/3063 - 30s - loss: 0.3482 - val_loss: 0.3451
Epoch 10/256
3063/3063 - 30s - loss: 0.3466 - val_loss: 0.3484
Epoch 11/256
3063/3063 - 30s - loss: 0.3457 - val_loss: 0.3447
Epoch 12/256
3063/3063 - 30s - loss: 0.3452 - val_loss: 0.3412
Epoch 13/256
3063/3063 - 30s - loss: 0.3435 - val_loss: 0.3427
Epoch 14/256
3063/3063 - 30s - loss: 0.3424 - val_loss: 0.3387
Epoch 15/256
3063/3063 - 30s - loss: 0.3399 - val_loss: 0.3683
Epoch 16/256
3063/3063 - 30s - loss: 0.3389 - val_loss: 0.3539
Epoch 17/256
3063/3063 - 30s - loss: 0.3373 - val_loss: 0.3373
Epoch 18/256
3063/3063 - 30s - loss: 0.3354 - val_loss: 0.3316
Epoch 19/256
3063/3063 - 30s - loss: 0.3330 - val_loss: 0.3291
Epoch 20/256
3063/3063 - 30s - loss: 0.3292 - val_loss: 0.3285
Epoch 21/256
3063/3063 - 30s - loss: 0.3258 - val_loss: 0.3425
Epoch 22/256
3063/3063 - 30s - loss: 0.3224 - val_loss: 0.3421
Epoch 23/256
3063/3063 - 30s - loss: 0.3200 - val_loss: 0.3170
Epoch 24/256
3063/3063 - 30s - loss: 0.3179 - val_loss: 0.3224
Epoch 25/256
3063/3063 - 30s - loss: 0.3158 - val_loss: 0.3267
Epoch 26/256
3063/3063 - 30s - loss: 0.3142 - val_loss: 0.3220
Epoch 27/256
3063/3063 - 30s - loss: 0.3128 - val_loss: 0.3122
Epoch 28/256
3063/3063 - 30s - loss: 0.3110 - val_loss: 0.3183
Epoch 29/256
3063/3063 - 30s - loss: 0.3088 - val_loss: 0.3100
Epoch 30/256
3063/3063 - 30s - loss: 0.3078 - val_loss: 0.3143
Epoch 31/256
3063/3063 - 30s - loss: 0.3067 - val_loss: 0.3207
Epoch 32/256
3063/3063 - 30s - loss: 0.3050 - val_loss: 0.3106
Epoch 33/256
3063/3063 - 30s - loss: 0.3043 - val_loss: 0.3065
Epoch 34/256
3063/3063 - 30s - loss: 0.3034 - val_loss: 0.3019
Epoch 35/256
3063/3063 - 30s - loss: 0.3012 - val_loss: 0.3046
Epoch 36/256
3063/3063 - 30s - loss: 0.3017 - val_loss: 0.3045
Epoch 37/256
3063/3063 - 30s - loss: 0.3001 - val_loss: 0.3060
Epoch 38/256
3063/3063 - 30s - loss: 0.2992 - val_loss: 0.3025
Epoch 39/256
3063/3063 - 30s - loss: 0.2988 - val_loss: 0.3166
Epoch 40/256
3063/3063 - 30s - loss: 0.2978 - val_loss: 0.3055
Epoch 41/256
3063/3063 - 29s - loss: 0.2967 - val_loss: 0.3023
Epoch 42/256
3063/3063 - 29s - loss: 0.2956 - val_loss: 0.3029
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 1)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.4276 - val_loss: 0.3778
Epoch 2/256
3063/3063 - 30s - loss: 0.3807 - val_loss: 0.3686
Epoch 3/256
3063/3063 - 30s - loss: 0.3660 - val_loss: 0.3597
Epoch 4/256
3063/3063 - 30s - loss: 0.3605 - val_loss: 0.3654
Epoch 5/256
3063/3063 - 30s - loss: 0.3556 - val_loss: 0.3548
Epoch 6/256
3063/3063 - 30s - loss: 0.3516 - val_loss: 0.3466
Epoch 7/256
3063/3063 - 30s - loss: 0.3492 - val_loss: 0.3427
Epoch 8/256
3063/3063 - 30s - loss: 0.3460 - val_loss: 0.3422
Epoch 9/256
3063/3063 - 30s - loss: 0.3433 - val_loss: 0.3531
Epoch 10/256
3063/3063 - 30s - loss: 0.3415 - val_loss: 0.3388
Epoch 11/256
3063/3063 - 30s - loss: 0.3396 - val_loss: 0.3348
Epoch 12/256
3063/3063 - 30s - loss: 0.3373 - val_loss: 0.3355
Epoch 13/256
3063/3063 - 30s - loss: 0.3357 - val_loss: 0.3355
Epoch 14/256
3063/3063 - 30s - loss: 0.3346 - val_loss: 0.3349
Epoch 15/256
3063/3063 - 30s - loss: 0.3320 - val_loss: 0.3331
Epoch 16/256
3063/3063 - 30s - loss: 0.3310 - val_loss: 0.3336
Epoch 17/256
3063/3063 - 30s - loss: 0.3288 - val_loss: 0.3345
Epoch 18/256
3063/3063 - 30s - loss: 0.3278 - val_loss: 0.3562
Epoch 19/256
3063/3063 - 30s - loss: 0.3261 - val_loss: 0.3406
Epoch 20/256
3063/3063 - 30s - loss: 0.3248 - val_loss: 0.3370
Epoch 21/256
3063/3063 - 30s - loss: 0.3228 - val_loss: 0.3334
Epoch 22/256
3063/3063 - 30s - loss: 0.3221 - val_loss: 0.3234
Epoch 23/256
3063/3063 - 30s - loss: 0.3211 - val_loss: 0.3259
Epoch 24/256
3063/3063 - 30s - loss: 0.3186 - val_loss: 0.3185
Epoch 25/256
3063/3063 - 30s - loss: 0.3170 - val_loss: 0.3308
Epoch 26/256
3063/3063 - 30s - loss: 0.3150 - val_loss: 0.3218
Epoch 27/256
3063/3063 - 30s - loss: 0.3129 - val_loss: 0.3178
Epoch 28/256
3063/3063 - 30s - loss: 0.3126 - val_loss: 0.3143
Epoch 29/256
3063/3063 - 30s - loss: 0.3106 - val_loss: 0.3253
Epoch 30/256
3063/3063 - 30s - loss: 0.3097 - val_loss: 0.3185
Epoch 31/256
3063/3063 - 30s - loss: 0.3081 - val_loss: 0.3172
Epoch 32/256
3063/3063 - 30s - loss: 0.3061 - val_loss: 0.3142
Epoch 33/256
3063/3063 - 30s - loss: 0.3052 - val_loss: 0.3066
Epoch 34/256
3063/3063 - 30s - loss: 0.3048 - val_loss: 0.3101
Epoch 35/256
3063/3063 - 30s - loss: 0.3035 - val_loss: 0.3126
Epoch 36/256
3063/3063 - 30s - loss: 0.3022 - val_loss: 0.3059
Epoch 37/256
3063/3063 - 30s - loss: 0.3009 - val_loss: 0.3122
Epoch 38/256
3063/3063 - 30s - loss: 0.2999 - val_loss: 0.3141
Epoch 39/256
3063/3063 - 30s - loss: 0.2993 - val_loss: 0.3080
Epoch 40/256
3063/3063 - 30s - loss: 0.2995 - val_loss: 0.3013
Epoch 41/256
3063/3063 - 30s - loss: 0.2974 - val_loss: 0.3074
Epoch 42/256
3063/3063 - 30s - loss: 0.2966 - val_loss: 0.3010
Epoch 43/256
3063/3063 - 30s - loss: 0.2957 - val_loss: 0.3086
Epoch 44/256
3063/3063 - 30s - loss: 0.2950 - val_loss: 0.3023
Epoch 45/256
3063/3063 - 30s - loss: 0.2943 - val_loss: 0.2988
Epoch 46/256
3063/3063 - 30s - loss: 0.2936 - val_loss: 0.3042
Epoch 47/256
3063/3063 - 30s - loss: 0.2931 - val_loss: 0.3031
Epoch 48/256
3063/3063 - 30s - loss: 0.2916 - val_loss: 0.2984
Epoch 49/256
3063/3063 - 30s - loss: 0.2916 - val_loss: 0.3031
Epoch 50/256
3063/3063 - 30s - loss: 0.2907 - val_loss: 0.2981
Epoch 51/256
3063/3063 - 30s - loss: 0.2901 - val_loss: 0.3033
Epoch 52/256
3063/3063 - 30s - loss: 0.2898 - val_loss: 0.3013
Epoch 53/256
3063/3063 - 30s - loss: 0.2893 - val_loss: 0.2987
Epoch 54/256
3063/3063 - 30s - loss: 0.2891 - val_loss: 0.2962
Epoch 55/256
3063/3063 - 30s - loss: 0.2881 - val_loss: 0.2948
Epoch 56/256
3063/3063 - 30s - loss: 0.2872 - val_loss: 0.2964
Epoch 57/256
3063/3063 - 30s - loss: 0.2867 - val_loss: 0.2975
Epoch 58/256
3063/3063 - 30s - loss: 0.2862 - val_loss: 0.2990
Epoch 59/256
3063/3063 - 30s - loss: 0.2857 - val_loss: 0.2997
Epoch 60/256
3063/3063 - 30s - loss: 0.2849 - val_loss: 0.3065
Epoch 61/256
3063/3063 - 30s - loss: 0.2850 - val_loss: 0.2967
Epoch 62/256
3063/3063 - 30s - loss: 0.2837 - val_loss: 0.2995
Epoch 63/256
3063/3063 - 30s - loss: 0.2843 - val_loss: 0.2942
Epoch 64/256
3063/3063 - 30s - loss: 0.2837 - val_loss: 0.2959
Epoch 65/256
3063/3063 - 30s - loss: 0.2823 - val_loss: 0.2989
Epoch 66/256
3063/3063 - 30s - loss: 0.2822 - val_loss: 0.3001
Epoch 67/256
3063/3063 - 30s - loss: 0.2829 - val_loss: 0.3026
Epoch 68/256
3063/3063 - 30s - loss: 0.2815 - val_loss: 0.2976
Epoch 69/256
3063/3063 - 30s - loss: 0.2807 - val_loss: 0.2925
Epoch 70/256
3063/3063 - 30s - loss: 0.2800 - val_loss: 0.2987
Epoch 71/256
3063/3063 - 30s - loss: 0.2803 - val_loss: 0.2978
Epoch 72/256
3063/3063 - 30s - loss: 0.2791 - val_loss: 0.2993
Epoch 73/256
3063/3063 - 30s - loss: 0.2792 - val_loss: 0.2936
Epoch 74/256
3063/3063 - 30s - loss: 0.2785 - val_loss: 0.2990
Epoch 75/256
3063/3063 - 30s - loss: 0.2771 - val_loss: 0.2924
Epoch 76/256
3063/3063 - 30s - loss: 0.2777 - val_loss: 0.2951
Epoch 77/256
3063/3063 - 30s - loss: 0.2771 - val_loss: 0.2927
Epoch 78/256
3063/3063 - 30s - loss: 0.2762 - val_loss: 0.2944
Epoch 79/256
3063/3063 - 30s - loss: 0.2755 - val_loss: 0.3025
Epoch 80/256
3063/3063 - 30s - loss: 0.2752 - val_loss: 0.2899
Epoch 81/256
3063/3063 - 30s - loss: 0.2757 - val_loss: 0.2985
Epoch 82/256
3063/3063 - 30s - loss: 0.2744 - val_loss: 0.2948
Epoch 83/256
3063/3063 - 30s - loss: 0.2746 - val_loss: 0.2907
Epoch 84/256
3063/3063 - 30s - loss: 0.2745 - val_loss: 0.2888
Epoch 85/256
3063/3063 - 30s - loss: 0.2729 - val_loss: 0.3119
Epoch 86/256
3063/3063 - 30s - loss: 0.2725 - val_loss: 0.3134
Epoch 87/256
3063/3063 - 30s - loss: 0.2728 - val_loss: 0.3100
Epoch 88/256
3063/3063 - 30s - loss: 0.2719 - val_loss: 0.2969
Epoch 89/256
3063/3063 - 30s - loss: 0.2719 - val_loss: 0.2877
Epoch 90/256
3063/3063 - 30s - loss: 0.2705 - val_loss: 0.3012
Epoch 91/256
3063/3063 - 30s - loss: 0.2702 - val_loss: 0.2967
Epoch 92/256
3063/3063 - 30s - loss: 0.2710 - val_loss: 0.2896
Epoch 93/256
3063/3063 - 30s - loss: 0.2701 - val_loss: 0.2862
Epoch 94/256
3063/3063 - 30s - loss: 0.2691 - val_loss: 0.2887
Epoch 95/256
3063/3063 - 30s - loss: 0.2691 - val_loss: 0.2876
Epoch 96/256
3063/3063 - 30s - loss: 0.2679 - val_loss: 0.2905
Epoch 97/256
3063/3063 - 30s - loss: 0.2678 - val_loss: 0.3223
Epoch 98/256
3063/3063 - 30s - loss: 0.2678 - val_loss: 0.2888
Epoch 99/256
3063/3063 - 30s - loss: 0.2661 - val_loss: 0.2901
Epoch 100/256
3063/3063 - 30s - loss: 0.2666 - val_loss: 0.2874
Epoch 101/256
3063/3063 - 30s - loss: 0.2658 - val_loss: 0.2988
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 1)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.3948 - val_loss: 0.3577
Epoch 2/256
3063/3063 - 30s - loss: 0.3651 - val_loss: 0.3470
Epoch 3/256
3063/3063 - 30s - loss: 0.3533 - val_loss: 0.3460
Epoch 4/256
3063/3063 - 30s - loss: 0.3474 - val_loss: 0.3386
Epoch 5/256
3063/3063 - 30s - loss: 0.3435 - val_loss: 0.3369
Epoch 6/256
3063/3063 - 30s - loss: 0.3393 - val_loss: 0.3338
Epoch 7/256
3063/3063 - 30s - loss: 0.3350 - val_loss: 0.3426
Epoch 8/256
3063/3063 - 30s - loss: 0.3300 - val_loss: 0.3258
Epoch 9/256
3063/3063 - 30s - loss: 0.3266 - val_loss: 0.3219
Epoch 10/256
3063/3063 - 30s - loss: 0.3245 - val_loss: 0.3170
Epoch 11/256
3063/3063 - 30s - loss: 0.3203 - val_loss: 0.3149
Epoch 12/256
3063/3063 - 30s - loss: 0.3176 - val_loss: 0.3104
Epoch 13/256
3063/3063 - 30s - loss: 0.3143 - val_loss: 0.3117
Epoch 14/256
3063/3063 - 30s - loss: 0.3138 - val_loss: 0.3105
Epoch 15/256
3063/3063 - 30s - loss: 0.3109 - val_loss: 0.3187
Epoch 16/256
3063/3063 - 30s - loss: 0.3084 - val_loss: 0.3266
Epoch 17/256
3063/3063 - 30s - loss: 0.3067 - val_loss: 0.3156
Epoch 18/256
3063/3063 - 30s - loss: 0.3050 - val_loss: 0.3026
Epoch 19/256
3063/3063 - 30s - loss: 0.3038 - val_loss: 0.3177
Epoch 20/256
3063/3063 - 30s - loss: 0.3025 - val_loss: 0.3258
Epoch 21/256
3063/3063 - 30s - loss: 0.3007 - val_loss: 0.3044
Epoch 22/256
3063/3063 - 30s - loss: 0.2998 - val_loss: 0.3129
Epoch 23/256
3063/3063 - 30s - loss: 0.2994 - val_loss: 0.3046
Epoch 24/256
3063/3063 - 30s - loss: 0.2981 - val_loss: 0.3015
Epoch 25/256
3063/3063 - 30s - loss: 0.2970 - val_loss: 0.2993
Epoch 26/256
3063/3063 - 30s - loss: 0.2967 - val_loss: 0.2965
Epoch 27/256
3063/3063 - 30s - loss: 0.2956 - val_loss: 0.3154
Epoch 28/256
3063/3063 - 30s - loss: 0.2952 - val_loss: 0.2999
Epoch 29/256
3063/3063 - 30s - loss: 0.2947 - val_loss: 0.3124
Epoch 30/256
3063/3063 - 30s - loss: 0.2937 - val_loss: 0.3029
Epoch 31/256
3063/3063 - 30s - loss: 0.2928 - val_loss: 0.2988
Epoch 32/256
3063/3063 - 30s - loss: 0.2919 - val_loss: 0.2954
Epoch 33/256
3063/3063 - 30s - loss: 0.2918 - val_loss: 0.3039
Epoch 34/256
3063/3063 - 30s - loss: 0.2905 - val_loss: 0.2952
Epoch 35/256
3063/3063 - 30s - loss: 0.2893 - val_loss: 0.3028
Epoch 36/256
3063/3063 - 30s - loss: 0.2889 - val_loss: 0.3018
Epoch 37/256
3063/3063 - 30s - loss: 0.2888 - val_loss: 0.2973
Epoch 38/256
3063/3063 - 30s - loss: 0.2881 - val_loss: 0.2986
Epoch 39/256
3063/3063 - 30s - loss: 0.2874 - val_loss: 0.2958
Epoch 40/256
3063/3063 - 30s - loss: 0.2861 - val_loss: 0.2929
Epoch 41/256
3063/3063 - 30s - loss: 0.2860 - val_loss: 0.3441
Epoch 42/256
3063/3063 - 30s - loss: 0.2860 - val_loss: 0.2947
Epoch 43/256
3063/3063 - 30s - loss: 0.2859 - val_loss: 0.2959
Epoch 44/256
3063/3063 - 30s - loss: 0.2847 - val_loss: 0.2988
Epoch 45/256
3063/3063 - 30s - loss: 0.2836 - val_loss: 0.3071
Epoch 46/256
3063/3063 - 30s - loss: 0.2836 - val_loss: 0.3221
Epoch 47/256
3063/3063 - 30s - loss: 0.2833 - val_loss: 0.2965
Epoch 48/256
3063/3063 - 30s - loss: 0.2824 - val_loss: 0.2923
Epoch 49/256
3063/3063 - 30s - loss: 0.2816 - val_loss: 0.2904
Epoch 50/256
3063/3063 - 30s - loss: 0.2818 - val_loss: 0.2925
Epoch 51/256
3063/3063 - 30s - loss: 0.2814 - val_loss: 0.2906
Epoch 52/256
3063/3063 - 30s - loss: 0.2802 - val_loss: 0.2953
Epoch 53/256
3063/3063 - 30s - loss: 0.2803 - val_loss: 0.2931
Epoch 54/256
3063/3063 - 30s - loss: 0.2800 - val_loss: 0.2913
Epoch 55/256
3063/3063 - 30s - loss: 0.2784 - val_loss: 0.3134
Epoch 56/256
3063/3063 - 30s - loss: 0.2779 - val_loss: 0.2934
Epoch 57/256
3063/3063 - 30s - loss: 0.2779 - val_loss: 0.2960
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 1)_64
			 1 [0.9474507074697645, 0.9515535571658977, 0.9498836553502876, 0.9501528039925924, 0.9478358444250969, 0.9445709248314531, 0.9506042432381276, 0.9489954064412446]
		LATENT DIM 2
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 32s - loss: 0.4187 - val_loss: 0.3664
Epoch 2/256
3063/3063 - 30s - loss: 0.3693 - val_loss: 0.3768
Epoch 3/256
3063/3063 - 30s - loss: 0.3617 - val_loss: 0.3558
Epoch 4/256
3063/3063 - 30s - loss: 0.3572 - val_loss: 0.3589
Epoch 5/256
3063/3063 - 30s - loss: 0.3540 - val_loss: 0.3515
Epoch 6/256
3063/3063 - 30s - loss: 0.3511 - val_loss: 0.3437
Epoch 7/256
3063/3063 - 30s - loss: 0.3471 - val_loss: 0.3459
Epoch 8/256
3063/3063 - 30s - loss: 0.3438 - val_loss: 0.3431
Epoch 9/256
3063/3063 - 30s - loss: 0.3422 - val_loss: 0.3378
Epoch 10/256
3063/3063 - 30s - loss: 0.3388 - val_loss: 0.3549
Epoch 11/256
3063/3063 - 30s - loss: 0.3354 - val_loss: 0.3395
Epoch 12/256
3063/3063 - 30s - loss: 0.3321 - val_loss: 0.3559
Epoch 13/256
3063/3063 - 30s - loss: 0.3298 - val_loss: 0.3267
Epoch 14/256
3063/3063 - 30s - loss: 0.3254 - val_loss: 0.3326
Epoch 15/256
3063/3063 - 30s - loss: 0.3214 - val_loss: 0.3234
Epoch 16/256
3063/3063 - 30s - loss: 0.3198 - val_loss: 0.3242
Epoch 17/256
3063/3063 - 30s - loss: 0.3159 - val_loss: 0.3150
Epoch 18/256
3063/3063 - 30s - loss: 0.3138 - val_loss: 0.3234
Epoch 19/256
3063/3063 - 30s - loss: 0.3131 - val_loss: 0.3142
Epoch 20/256
3063/3063 - 30s - loss: 0.3102 - val_loss: 0.3138
Epoch 21/256
3063/3063 - 30s - loss: 0.3099 - val_loss: 0.3094
Epoch 22/256
3063/3063 - 30s - loss: 0.3086 - val_loss: 0.3567
Epoch 23/256
3063/3063 - 30s - loss: 0.3069 - val_loss: 0.3128
Epoch 24/256
3063/3063 - 30s - loss: 0.3048 - val_loss: 0.3164
Epoch 25/256
3063/3063 - 30s - loss: 0.3048 - val_loss: 0.3062
Epoch 26/256
3063/3063 - 30s - loss: 0.3030 - val_loss: 0.3083
Epoch 27/256
3063/3063 - 30s - loss: 0.3014 - val_loss: 0.3212
Epoch 28/256
3063/3063 - 30s - loss: 0.3012 - val_loss: 0.3070
Epoch 29/256
3063/3063 - 30s - loss: 0.2999 - val_loss: 0.2983
Epoch 30/256
3063/3063 - 30s - loss: 0.2983 - val_loss: 0.3048
Epoch 31/256
3063/3063 - 30s - loss: 0.2977 - val_loss: 0.3035
Epoch 32/256
3063/3063 - 30s - loss: 0.2968 - val_loss: 0.3076
Epoch 33/256
3063/3063 - 30s - loss: 0.2959 - val_loss: 0.3046
Epoch 34/256
3063/3063 - 30s - loss: 0.2942 - val_loss: 0.3086
Epoch 35/256
3063/3063 - 30s - loss: 0.2935 - val_loss: 0.3106
Epoch 36/256
3063/3063 - 30s - loss: 0.2919 - val_loss: 0.3147
Epoch 37/256
3063/3063 - 30s - loss: 0.2907 - val_loss: 0.2976
Epoch 38/256
3063/3063 - 30s - loss: 0.2903 - val_loss: 0.3223
Epoch 39/256
3063/3063 - 30s - loss: 0.2888 - val_loss: 0.3020
Epoch 40/256
3063/3063 - 35s - loss: 0.2879 - val_loss: 0.2952
Epoch 41/256
3063/3063 - 30s - loss: 0.2880 - val_loss: 0.2908
Epoch 42/256
3063/3063 - 30s - loss: 0.2882 - val_loss: 0.2905
Epoch 43/256
3063/3063 - 30s - loss: 0.2858 - val_loss: 0.2903
Epoch 44/256
3063/3063 - 30s - loss: 0.2854 - val_loss: 0.2930
Epoch 45/256
3063/3063 - 30s - loss: 0.2852 - val_loss: 0.2960
Epoch 46/256
3063/3063 - 30s - loss: 0.2842 - val_loss: 0.2921
Epoch 47/256
3063/3063 - 30s - loss: 0.2826 - val_loss: 0.3199
Epoch 48/256
3063/3063 - 30s - loss: 0.2827 - val_loss: 0.2927
Epoch 49/256
3063/3063 - 30s - loss: 0.2819 - val_loss: 0.3059
Epoch 50/256
3063/3063 - 30s - loss: 0.2806 - val_loss: 0.2885
Epoch 51/256
3063/3063 - 30s - loss: 0.2801 - val_loss: 0.3125
Epoch 52/256
3063/3063 - 30s - loss: 0.2802 - val_loss: 0.2979
Epoch 53/256
3063/3063 - 30s - loss: 0.2785 - val_loss: 0.2975
Epoch 54/256
3063/3063 - 30s - loss: 0.2788 - val_loss: 0.2883
Epoch 55/256
3063/3063 - 30s - loss: 0.2788 - val_loss: 0.2934
Epoch 56/256
3063/3063 - 30s - loss: 0.2778 - val_loss: 0.2886
Epoch 57/256
3063/3063 - 30s - loss: 0.2762 - val_loss: 0.2864
Epoch 58/256
3063/3063 - 30s - loss: 0.2758 - val_loss: 0.2891
Epoch 59/256
3063/3063 - 30s - loss: 0.2757 - val_loss: 0.2978
Epoch 60/256
3063/3063 - 30s - loss: 0.2750 - val_loss: 0.2870
Epoch 61/256
3063/3063 - 30s - loss: 0.2743 - val_loss: 0.2875
Epoch 62/256
3063/3063 - 30s - loss: 0.2732 - val_loss: 0.2916
Epoch 63/256
3063/3063 - 30s - loss: 0.2723 - val_loss: 0.2932
Epoch 64/256
3063/3063 - 30s - loss: 0.2718 - val_loss: 0.2830
Epoch 65/256
3063/3063 - 30s - loss: 0.2710 - val_loss: 0.2896
Epoch 66/256
3063/3063 - 30s - loss: 0.2711 - val_loss: 0.2823
Epoch 67/256
3063/3063 - 30s - loss: 0.2696 - val_loss: 0.2849
Epoch 68/256
3063/3063 - 30s - loss: 0.2690 - val_loss: 0.2787
Epoch 69/256
3063/3063 - 30s - loss: 0.2676 - val_loss: 0.2808
Epoch 70/256
3063/3063 - 30s - loss: 0.2672 - val_loss: 0.2827
Epoch 71/256
3063/3063 - 30s - loss: 0.2669 - val_loss: 0.2896
Epoch 72/256
3063/3063 - 30s - loss: 0.2679 - val_loss: 0.3009
Epoch 73/256
3063/3063 - 30s - loss: 0.2658 - val_loss: 0.2816
Epoch 74/256
3063/3063 - 30s - loss: 0.2642 - val_loss: 0.2813
Epoch 75/256
3063/3063 - 30s - loss: 0.2648 - val_loss: 0.2769
Epoch 76/256
3063/3063 - 30s - loss: 0.2643 - val_loss: 0.2750
Epoch 77/256
3063/3063 - 30s - loss: 0.2625 - val_loss: 0.2737
Epoch 78/256
3063/3063 - 30s - loss: 0.2628 - val_loss: 0.2892
Epoch 79/256
3063/3063 - 30s - loss: 0.2615 - val_loss: 0.2879
Epoch 80/256
3063/3063 - 30s - loss: 0.2610 - val_loss: 0.2758
Epoch 81/256
3063/3063 - 30s - loss: 0.2610 - val_loss: 0.2766
Epoch 82/256
3063/3063 - 30s - loss: 0.2595 - val_loss: 0.3338
Epoch 83/256
3063/3063 - 30s - loss: 0.2595 - val_loss: 0.2749
Epoch 84/256
3063/3063 - 30s - loss: 0.2589 - val_loss: 0.2734
Epoch 85/256
3063/3063 - 30s - loss: 0.2577 - val_loss: 0.2777
Epoch 86/256
3063/3063 - 30s - loss: 0.2577 - val_loss: 0.2724
Epoch 87/256
3063/3063 - 30s - loss: 0.2566 - val_loss: 0.2752
Epoch 88/256
3063/3063 - 30s - loss: 0.2572 - val_loss: 0.2734
Epoch 89/256
3063/3063 - 30s - loss: 0.2548 - val_loss: 0.2823
Epoch 90/256
3063/3063 - 30s - loss: 0.2544 - val_loss: 0.2740
Epoch 91/256
3063/3063 - 30s - loss: 0.2547 - val_loss: 0.2786
Epoch 92/256
3063/3063 - 30s - loss: 0.2544 - val_loss: 0.3010
Epoch 93/256
3063/3063 - 30s - loss: 0.2542 - val_loss: 0.2851
Epoch 94/256
3063/3063 - 30s - loss: 0.2535 - val_loss: 0.2756
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 2)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.4000 - val_loss: 0.3758
Epoch 2/256
3063/3063 - 30s - loss: 0.3646 - val_loss: 0.3589
Epoch 3/256
3063/3063 - 35s - loss: 0.3575 - val_loss: 0.3585
Epoch 4/256
3063/3063 - 30s - loss: 0.3529 - val_loss: 0.3574
Epoch 5/256
3063/3063 - 30s - loss: 0.3498 - val_loss: 0.3445
Epoch 6/256
3063/3063 - 30s - loss: 0.3474 - val_loss: 0.3612
Epoch 7/256
3063/3063 - 30s - loss: 0.3448 - val_loss: 0.3420
Epoch 8/256
3063/3063 - 30s - loss: 0.3427 - val_loss: 0.3411
Epoch 9/256
3063/3063 - 30s - loss: 0.3407 - val_loss: 0.3443
Epoch 10/256
3063/3063 - 30s - loss: 0.3379 - val_loss: 0.3336
Epoch 11/256
3063/3063 - 30s - loss: 0.3371 - val_loss: 0.3327
Epoch 12/256
3063/3063 - 30s - loss: 0.3353 - val_loss: 0.3303
Epoch 13/256
3063/3063 - 30s - loss: 0.3320 - val_loss: 0.3288
Epoch 14/256
3063/3063 - 30s - loss: 0.3300 - val_loss: 0.3346
Epoch 15/256
3063/3063 - 30s - loss: 0.3272 - val_loss: 0.3290
Epoch 16/256
3063/3063 - 30s - loss: 0.3241 - val_loss: 0.3212
Epoch 17/256
3063/3063 - 30s - loss: 0.3205 - val_loss: 0.3280
Epoch 18/256
3063/3063 - 30s - loss: 0.3180 - val_loss: 0.3176
Epoch 19/256
3063/3063 - 30s - loss: 0.3151 - val_loss: 0.3275
Epoch 20/256
3063/3063 - 30s - loss: 0.3119 - val_loss: 0.3117
Epoch 21/256
3063/3063 - 30s - loss: 0.3104 - val_loss: 0.3207
Epoch 22/256
3063/3063 - 30s - loss: 0.3082 - val_loss: 0.3097
Epoch 23/256
3063/3063 - 30s - loss: 0.3067 - val_loss: 0.3058
Epoch 24/256
3063/3063 - 30s - loss: 0.3042 - val_loss: 0.3075
Epoch 25/256
3063/3063 - 30s - loss: 0.3030 - val_loss: 0.3005
Epoch 26/256
3063/3063 - 30s - loss: 0.3013 - val_loss: 0.3018
Epoch 27/256
3063/3063 - 30s - loss: 0.3000 - val_loss: 0.2978
Epoch 28/256
3063/3063 - 30s - loss: 0.2982 - val_loss: 0.3093
Epoch 29/256
3063/3063 - 30s - loss: 0.2955 - val_loss: 0.3009
Epoch 30/256
3063/3063 - 30s - loss: 0.2936 - val_loss: 0.2995
Epoch 31/256
3063/3063 - 30s - loss: 0.2920 - val_loss: 0.3018
Epoch 32/256
3063/3063 - 30s - loss: 0.2909 - val_loss: 0.2979
Epoch 33/256
3063/3063 - 30s - loss: 0.2894 - val_loss: 0.3093
Epoch 34/256
3063/3063 - 30s - loss: 0.2882 - val_loss: 0.2869
Epoch 35/256
3063/3063 - 30s - loss: 0.2868 - val_loss: 0.2907
Epoch 36/256
3063/3063 - 30s - loss: 0.2850 - val_loss: 0.2931
Epoch 37/256
3063/3063 - 30s - loss: 0.2837 - val_loss: 0.2923
Epoch 38/256
3063/3063 - 30s - loss: 0.2840 - val_loss: 0.2840
Epoch 39/256
3063/3063 - 30s - loss: 0.2817 - val_loss: 0.2840
Epoch 40/256
3063/3063 - 30s - loss: 0.2815 - val_loss: 0.2832
Epoch 41/256
3063/3063 - 30s - loss: 0.2809 - val_loss: 0.2872
Epoch 42/256
3063/3063 - 30s - loss: 0.2791 - val_loss: 0.2854
Epoch 43/256
3063/3063 - 30s - loss: 0.2787 - val_loss: 0.2977
Epoch 44/256
3063/3063 - 30s - loss: 0.2784 - val_loss: 0.2991
Epoch 45/256
3063/3063 - 30s - loss: 0.2775 - val_loss: 0.2841
Epoch 46/256
3063/3063 - 30s - loss: 0.2765 - val_loss: 0.2803
Epoch 47/256
3063/3063 - 30s - loss: 0.2754 - val_loss: 0.2886
Epoch 48/256
3063/3063 - 30s - loss: 0.2743 - val_loss: 0.2846
Epoch 49/256
3063/3063 - 30s - loss: 0.2735 - val_loss: 0.2873
Epoch 50/256
3063/3063 - 30s - loss: 0.2714 - val_loss: 0.2877
Epoch 51/256
3063/3063 - 30s - loss: 0.2721 - val_loss: 0.2909
Epoch 52/256
3063/3063 - 30s - loss: 0.2713 - val_loss: 0.3005
Epoch 53/256
3063/3063 - 30s - loss: 0.2707 - val_loss: 0.2789
Epoch 54/256
3063/3063 - 30s - loss: 0.2695 - val_loss: 0.2927
Epoch 55/256
3063/3063 - 30s - loss: 0.2686 - val_loss: 0.2848
Epoch 56/256
3063/3063 - 30s - loss: 0.2680 - val_loss: 0.2993
Epoch 57/256
3063/3063 - 30s - loss: 0.2680 - val_loss: 0.2856
Epoch 58/256
3063/3063 - 30s - loss: 0.2665 - val_loss: 0.2798
Epoch 59/256
3063/3063 - 30s - loss: 0.2664 - val_loss: 0.2803
Epoch 60/256
3063/3063 - 30s - loss: 0.2648 - val_loss: 0.2789
Epoch 61/256
3063/3063 - 30s - loss: 0.2646 - val_loss: 0.2737
Epoch 62/256
3063/3063 - 30s - loss: 0.2634 - val_loss: 0.2743
Epoch 63/256
3063/3063 - 30s - loss: 0.2631 - val_loss: 0.2819
Epoch 64/256
3063/3063 - 30s - loss: 0.2631 - val_loss: 0.2761
Epoch 65/256
3063/3063 - 30s - loss: 0.2609 - val_loss: 0.2766
Epoch 66/256
3063/3063 - 30s - loss: 0.2611 - val_loss: 0.2762
Epoch 67/256
3063/3063 - 30s - loss: 0.2604 - val_loss: 0.2754
Epoch 68/256
3063/3063 - 30s - loss: 0.2601 - val_loss: 0.2846
Epoch 69/256
3063/3063 - 30s - loss: 0.2594 - val_loss: 0.2774
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 2)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.4132 - val_loss: 0.3724
Epoch 2/256
3063/3063 - 30s - loss: 0.3714 - val_loss: 0.3574
Epoch 3/256
3063/3063 - 30s - loss: 0.3606 - val_loss: 0.3510
Epoch 4/256
3063/3063 - 30s - loss: 0.3563 - val_loss: 0.3508
Epoch 5/256
3063/3063 - 30s - loss: 0.3513 - val_loss: 0.3423
Epoch 6/256
3063/3063 - 30s - loss: 0.3482 - val_loss: 0.3468
Epoch 7/256
3063/3063 - 30s - loss: 0.3454 - val_loss: 0.3418
Epoch 8/256
3063/3063 - 30s - loss: 0.3421 - val_loss: 0.3404
Epoch 9/256
3063/3063 - 30s - loss: 0.3407 - val_loss: 0.3411
Epoch 10/256
3063/3063 - 30s - loss: 0.3378 - val_loss: 0.3353
Epoch 11/256
3063/3063 - 30s - loss: 0.3355 - val_loss: 0.3501
Epoch 12/256
3063/3063 - 30s - loss: 0.3345 - val_loss: 0.3406
Epoch 13/256
3063/3063 - 30s - loss: 0.3319 - val_loss: 0.3293
Epoch 14/256
3063/3063 - 30s - loss: 0.3302 - val_loss: 0.3368
Epoch 15/256
3063/3063 - 30s - loss: 0.3294 - val_loss: 0.3294
Epoch 16/256
3063/3063 - 30s - loss: 0.3270 - val_loss: 0.3240
Epoch 17/256
3063/3063 - 30s - loss: 0.3257 - val_loss: 0.3219
Epoch 18/256
3063/3063 - 30s - loss: 0.3239 - val_loss: 0.3277
Epoch 19/256
3063/3063 - 30s - loss: 0.3231 - val_loss: 0.3417
Epoch 20/256
3063/3063 - 30s - loss: 0.3194 - val_loss: 0.3186
Epoch 21/256
3063/3063 - 30s - loss: 0.3175 - val_loss: 0.3192
Epoch 22/256
3063/3063 - 30s - loss: 0.3139 - val_loss: 0.3637
Epoch 23/256
3063/3063 - 30s - loss: 0.3131 - val_loss: 0.3207
Epoch 24/256
3063/3063 - 30s - loss: 0.3111 - val_loss: 0.3118
Epoch 25/256
3063/3063 - 30s - loss: 0.3086 - val_loss: 0.3093
Epoch 26/256
3063/3063 - 30s - loss: 0.3075 - val_loss: 0.3371
Epoch 27/256
3063/3063 - 30s - loss: 0.3058 - val_loss: 0.3078
Epoch 28/256
3063/3063 - 30s - loss: 0.3037 - val_loss: 0.3048
Epoch 29/256
3063/3063 - 30s - loss: 0.3027 - val_loss: 0.3016
Epoch 30/256
3063/3063 - 30s - loss: 0.3007 - val_loss: 0.3028
Epoch 31/256
3063/3063 - 30s - loss: 0.2997 - val_loss: 0.2998
Epoch 32/256
3063/3063 - 30s - loss: 0.2988 - val_loss: 0.3210
Epoch 33/256
3063/3063 - 30s - loss: 0.2983 - val_loss: 0.3063
Epoch 34/256
3063/3063 - 30s - loss: 0.2966 - val_loss: 0.3118
Epoch 35/256
3063/3063 - 30s - loss: 0.2957 - val_loss: 0.3015
Epoch 36/256
3063/3063 - 30s - loss: 0.2956 - val_loss: 0.3045
Epoch 37/256
3063/3063 - 30s - loss: 0.2934 - val_loss: 0.3241
Epoch 38/256
3063/3063 - 30s - loss: 0.2929 - val_loss: 0.2965
Epoch 39/256
3063/3063 - 30s - loss: 0.2920 - val_loss: 0.2939
Epoch 40/256
3063/3063 - 30s - loss: 0.2907 - val_loss: 0.2933
Epoch 41/256
3063/3063 - 30s - loss: 0.2906 - val_loss: 0.3009
Epoch 42/256
3063/3063 - 30s - loss: 0.2884 - val_loss: 0.3056
Epoch 43/256
3063/3063 - 30s - loss: 0.2885 - val_loss: 0.2925
Epoch 44/256
3063/3063 - 30s - loss: 0.2870 - val_loss: 0.2973
Epoch 45/256
3063/3063 - 30s - loss: 0.2859 - val_loss: 0.2941
Epoch 46/256
3063/3063 - 30s - loss: 0.2857 - val_loss: 0.3052
Epoch 47/256
3063/3063 - 35s - loss: 0.2843 - val_loss: 0.2884
Epoch 48/256
3063/3063 - 30s - loss: 0.2836 - val_loss: 0.2907
Epoch 49/256
3063/3063 - 30s - loss: 0.2822 - val_loss: 0.2911
Epoch 50/256
3063/3063 - 30s - loss: 0.2820 - val_loss: 0.2935
Epoch 51/256
3063/3063 - 30s - loss: 0.2818 - val_loss: 0.3007
Epoch 52/256
3063/3063 - 30s - loss: 0.2804 - val_loss: 0.2896
Epoch 53/256
3063/3063 - 30s - loss: 0.2800 - val_loss: 0.2842
Epoch 54/256
3063/3063 - 30s - loss: 0.2785 - val_loss: 0.2955
Epoch 55/256
3063/3063 - 30s - loss: 0.2778 - val_loss: 0.2887
Epoch 56/256
3063/3063 - 30s - loss: 0.2769 - val_loss: 0.2908
Epoch 57/256
3063/3063 - 30s - loss: 0.2766 - val_loss: 0.2924
Epoch 58/256
3063/3063 - 30s - loss: 0.2759 - val_loss: 0.3144
Epoch 59/256
3063/3063 - 30s - loss: 0.2745 - val_loss: 0.2960
Epoch 60/256
3063/3063 - 30s - loss: 0.2731 - val_loss: 0.2825
Epoch 61/256
3063/3063 - 30s - loss: 0.2727 - val_loss: 0.2779
Epoch 62/256
3063/3063 - 30s - loss: 0.2731 - val_loss: 0.2849
Epoch 63/256
3063/3063 - 30s - loss: 0.2729 - val_loss: 0.2813
Epoch 64/256
3063/3063 - 30s - loss: 0.2707 - val_loss: 0.2908
Epoch 65/256
3063/3063 - 30s - loss: 0.2693 - val_loss: 0.2894
Epoch 66/256
3063/3063 - 30s - loss: 0.2697 - val_loss: 0.3205
Epoch 67/256
3063/3063 - 30s - loss: 0.2688 - val_loss: 0.2882
Epoch 68/256
3063/3063 - 30s - loss: 0.2688 - val_loss: 0.2781
Epoch 69/256
3063/3063 - 30s - loss: 0.2684 - val_loss: 0.2786
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 2)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.4193 - val_loss: 0.3692
Epoch 2/256
3063/3063 - 30s - loss: 0.3710 - val_loss: 0.3554
Epoch 3/256
3063/3063 - 30s - loss: 0.3602 - val_loss: 0.3538
Epoch 4/256
3063/3063 - 30s - loss: 0.3541 - val_loss: 0.3551
Epoch 5/256
3063/3063 - 30s - loss: 0.3526 - val_loss: 0.3436
Epoch 6/256
3063/3063 - 30s - loss: 0.3486 - val_loss: 0.3424
Epoch 7/256
3063/3063 - 30s - loss: 0.3469 - val_loss: 0.3481
Epoch 8/256
3063/3063 - 30s - loss: 0.3456 - val_loss: 0.3449
Epoch 9/256
3063/3063 - 30s - loss: 0.3431 - val_loss: 0.3407
Epoch 10/256
3063/3063 - 30s - loss: 0.3416 - val_loss: 0.3418
Epoch 11/256
3063/3063 - 30s - loss: 0.3404 - val_loss: 0.3342
Epoch 12/256
3063/3063 - 30s - loss: 0.3363 - val_loss: 0.3322
Epoch 13/256
3063/3063 - 30s - loss: 0.3353 - val_loss: 0.3422
Epoch 14/256
3063/3063 - 30s - loss: 0.3340 - val_loss: 0.3331
Epoch 15/256
3063/3063 - 30s - loss: 0.3328 - val_loss: 0.3353
Epoch 16/256
3063/3063 - 30s - loss: 0.3312 - val_loss: 0.3334
Epoch 17/256
3063/3063 - 30s - loss: 0.3291 - val_loss: 0.3301
Epoch 18/256
3063/3063 - 30s - loss: 0.3273 - val_loss: 0.3285
Epoch 19/256
3063/3063 - 30s - loss: 0.3244 - val_loss: 0.3239
Epoch 20/256
3063/3063 - 30s - loss: 0.3221 - val_loss: 0.3218
Epoch 21/256
3063/3063 - 30s - loss: 0.3201 - val_loss: 0.3215
Epoch 22/256
3063/3063 - 30s - loss: 0.3174 - val_loss: 0.3134
Epoch 23/256
3063/3063 - 30s - loss: 0.3154 - val_loss: 0.3149
Epoch 24/256
3063/3063 - 30s - loss: 0.3119 - val_loss: 0.3287
Epoch 25/256
3063/3063 - 30s - loss: 0.3094 - val_loss: 0.3107
Epoch 26/256
3063/3063 - 30s - loss: 0.3073 - val_loss: 0.3117
Epoch 27/256
3063/3063 - 30s - loss: 0.3058 - val_loss: 0.3382
Epoch 28/256
3063/3063 - 30s - loss: 0.3041 - val_loss: 0.3054
Epoch 29/256
3063/3063 - 30s - loss: 0.3010 - val_loss: 0.3088
Epoch 30/256
3063/3063 - 30s - loss: 0.2999 - val_loss: 0.3004
Epoch 31/256
3063/3063 - 30s - loss: 0.2980 - val_loss: 0.2974
Epoch 32/256
3063/3063 - 30s - loss: 0.2973 - val_loss: 0.3034
Epoch 33/256
3063/3063 - 30s - loss: 0.2952 - val_loss: 0.3022
Epoch 34/256
3063/3063 - 30s - loss: 0.2936 - val_loss: 0.2971
Epoch 35/256
3063/3063 - 30s - loss: 0.2926 - val_loss: 0.2914
Epoch 36/256
3063/3063 - 30s - loss: 0.2909 - val_loss: 0.3034
Epoch 37/256
3063/3063 - 30s - loss: 0.2901 - val_loss: 0.3116
Epoch 38/256
3063/3063 - 30s - loss: 0.2883 - val_loss: 0.3079
Epoch 39/256
3063/3063 - 30s - loss: 0.2876 - val_loss: 0.2919
Epoch 40/256
3063/3063 - 30s - loss: 0.2850 - val_loss: 0.2921
Epoch 41/256
3063/3063 - 30s - loss: 0.2855 - val_loss: 0.3037
Epoch 42/256
3063/3063 - 30s - loss: 0.2835 - val_loss: 0.2849
Epoch 43/256
3063/3063 - 30s - loss: 0.2821 - val_loss: 0.2865
Epoch 44/256
3063/3063 - 30s - loss: 0.2803 - val_loss: 0.2855
Epoch 45/256
3063/3063 - 30s - loss: 0.2797 - val_loss: 0.2876
Epoch 46/256
3063/3063 - 30s - loss: 0.2776 - val_loss: 0.2826
Epoch 47/256
3063/3063 - 30s - loss: 0.2767 - val_loss: 0.3071
Epoch 48/256
3063/3063 - 30s - loss: 0.2755 - val_loss: 0.2976
Epoch 49/256
3063/3063 - 30s - loss: 0.2741 - val_loss: 0.2772
Epoch 50/256
3063/3063 - 30s - loss: 0.2732 - val_loss: 0.2928
Epoch 51/256
3063/3063 - 30s - loss: 0.2722 - val_loss: 0.2779
Epoch 52/256
3063/3063 - 30s - loss: 0.2711 - val_loss: 0.2744
Epoch 53/256
3063/3063 - 30s - loss: 0.2700 - val_loss: 0.2747
Epoch 54/256
3063/3063 - 30s - loss: 0.2683 - val_loss: 0.2780
Epoch 55/256
3063/3063 - 35s - loss: 0.2686 - val_loss: 0.2737
Epoch 56/256
3063/3063 - 30s - loss: 0.2691 - val_loss: 0.2743
Epoch 57/256
3063/3063 - 30s - loss: 0.2676 - val_loss: 0.2739
Epoch 58/256
3063/3063 - 30s - loss: 0.2660 - val_loss: 0.2750
Epoch 59/256
3063/3063 - 30s - loss: 0.2653 - val_loss: 0.2738
Epoch 60/256
3063/3063 - 30s - loss: 0.2634 - val_loss: 0.2675
Epoch 61/256
3063/3063 - 30s - loss: 0.2632 - val_loss: 0.2785
Epoch 62/256
3063/3063 - 30s - loss: 0.2623 - val_loss: 0.2680
Epoch 63/256
3063/3063 - 30s - loss: 0.2620 - val_loss: 0.2653
Epoch 64/256
3063/3063 - 30s - loss: 0.2608 - val_loss: 0.2821
Epoch 65/256
3063/3063 - 30s - loss: 0.2610 - val_loss: 0.2734
Epoch 66/256
3063/3063 - 30s - loss: 0.2590 - val_loss: 0.2784
Epoch 67/256
3063/3063 - 30s - loss: 0.2584 - val_loss: 0.2675
Epoch 68/256
3063/3063 - 30s - loss: 0.2580 - val_loss: 0.2743
Epoch 69/256
3063/3063 - 30s - loss: 0.2573 - val_loss: 0.2674
Epoch 70/256
3063/3063 - 30s - loss: 0.2571 - val_loss: 0.2705
Epoch 71/256
3063/3063 - 30s - loss: 0.2562 - val_loss: 0.2704
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 2)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 32s - loss: 0.3942 - val_loss: 0.3594
Epoch 2/256
3063/3063 - 30s - loss: 0.3630 - val_loss: 0.3639
Epoch 3/256
3063/3063 - 31s - loss: 0.3526 - val_loss: 0.3496
Epoch 4/256
3063/3063 - 30s - loss: 0.3466 - val_loss: 0.3446
Epoch 5/256
3063/3063 - 31s - loss: 0.3435 - val_loss: 0.3424
Epoch 6/256
3063/3063 - 31s - loss: 0.3388 - val_loss: 0.3605
Epoch 7/256
3063/3063 - 30s - loss: 0.3365 - val_loss: 0.3359
Epoch 8/256
3063/3063 - 30s - loss: 0.3321 - val_loss: 0.3317
Epoch 9/256
3063/3063 - 30s - loss: 0.3275 - val_loss: 0.3334
Epoch 10/256
3063/3063 - 30s - loss: 0.3206 - val_loss: 0.3261
Epoch 11/256
3063/3063 - 30s - loss: 0.3163 - val_loss: 0.3202
Epoch 12/256
3063/3063 - 31s - loss: 0.3120 - val_loss: 0.3088
Epoch 13/256
3063/3063 - 30s - loss: 0.3082 - val_loss: 0.3172
Epoch 14/256
3063/3063 - 30s - loss: 0.3047 - val_loss: 0.3062
Epoch 15/256
3063/3063 - 30s - loss: 0.3026 - val_loss: 0.3110
Epoch 16/256
3063/3063 - 30s - loss: 0.3006 - val_loss: 0.3077
Epoch 17/256
3063/3063 - 30s - loss: 0.2979 - val_loss: 0.2941
Epoch 18/256
3063/3063 - 30s - loss: 0.2961 - val_loss: 0.2957
Epoch 19/256
3063/3063 - 30s - loss: 0.2939 - val_loss: 0.3170
Epoch 20/256
3063/3063 - 30s - loss: 0.2929 - val_loss: 0.2980
Epoch 21/256
3063/3063 - 30s - loss: 0.2911 - val_loss: 0.2883
Epoch 22/256
3063/3063 - 30s - loss: 0.2893 - val_loss: 0.2895
Epoch 23/256
3063/3063 - 30s - loss: 0.2878 - val_loss: 0.2893
Epoch 24/256
3063/3063 - 30s - loss: 0.2861 - val_loss: 0.3007
Epoch 25/256
3063/3063 - 30s - loss: 0.2851 - val_loss: 0.2846
Epoch 26/256
3063/3063 - 30s - loss: 0.2835 - val_loss: 0.2930
Epoch 27/256
3063/3063 - 30s - loss: 0.2829 - val_loss: 0.2895
Epoch 28/256
3063/3063 - 30s - loss: 0.2812 - val_loss: 0.2841
Epoch 29/256
3063/3063 - 30s - loss: 0.2794 - val_loss: 0.2910
Epoch 30/256
3063/3063 - 30s - loss: 0.2800 - val_loss: 0.2802
Epoch 31/256
3063/3063 - 30s - loss: 0.2783 - val_loss: 0.2840
Epoch 32/256
3063/3063 - 30s - loss: 0.2774 - val_loss: 0.2859
Epoch 33/256
3063/3063 - 30s - loss: 0.2760 - val_loss: 0.2896
Epoch 34/256
3063/3063 - 31s - loss: 0.2760 - val_loss: 0.2854
Epoch 35/256
3063/3063 - 30s - loss: 0.2755 - val_loss: 0.2790
Epoch 36/256
3063/3063 - 30s - loss: 0.2742 - val_loss: 0.2868
Epoch 37/256
3063/3063 - 30s - loss: 0.2733 - val_loss: 0.2848
Epoch 38/256
3063/3063 - 30s - loss: 0.2724 - val_loss: 0.2809
Epoch 39/256
3063/3063 - 30s - loss: 0.2712 - val_loss: 0.2922
Epoch 40/256
3063/3063 - 30s - loss: 0.2714 - val_loss: 0.2818
Epoch 41/256
3063/3063 - 30s - loss: 0.2700 - val_loss: 0.2755
Epoch 42/256
3063/3063 - 30s - loss: 0.2694 - val_loss: 0.2815
Epoch 43/256
3063/3063 - 30s - loss: 0.2687 - val_loss: 0.2774
Epoch 44/256
3063/3063 - 30s - loss: 0.2679 - val_loss: 0.2924
Epoch 45/256
3063/3063 - 30s - loss: 0.2679 - val_loss: 0.2907
Epoch 46/256
3063/3063 - 30s - loss: 0.2671 - val_loss: 0.2928
Epoch 47/256
3063/3063 - 30s - loss: 0.2662 - val_loss: 0.2829
Epoch 48/256
3063/3063 - 30s - loss: 0.2654 - val_loss: 0.2846
Epoch 49/256
3063/3063 - 30s - loss: 0.2643 - val_loss: 0.2762
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 2)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.3971 - val_loss: 0.3628
Epoch 2/256
3063/3063 - 30s - loss: 0.3658 - val_loss: 0.3570
Epoch 3/256
3063/3063 - 30s - loss: 0.3549 - val_loss: 0.3513
Epoch 4/256
3063/3063 - 30s - loss: 0.3501 - val_loss: 0.3434
Epoch 5/256
3063/3063 - 30s - loss: 0.3445 - val_loss: 0.3431
Epoch 6/256
3063/3063 - 30s - loss: 0.3418 - val_loss: 0.3424
Epoch 7/256
3063/3063 - 30s - loss: 0.3377 - val_loss: 0.3469
Epoch 8/256
3063/3063 - 30s - loss: 0.3334 - val_loss: 0.3312
Epoch 9/256
3063/3063 - 30s - loss: 0.3293 - val_loss: 0.3261
Epoch 10/256
3063/3063 - 30s - loss: 0.3235 - val_loss: 0.3255
Epoch 11/256
3063/3063 - 30s - loss: 0.3198 - val_loss: 0.3252
Epoch 12/256
3063/3063 - 30s - loss: 0.3164 - val_loss: 0.3169
Epoch 13/256
3063/3063 - 30s - loss: 0.3127 - val_loss: 0.3179
Epoch 14/256
3063/3063 - 30s - loss: 0.3099 - val_loss: 0.3144
Epoch 15/256
3063/3063 - 30s - loss: 0.3057 - val_loss: 0.3281
Epoch 16/256
3063/3063 - 30s - loss: 0.3030 - val_loss: 0.3169
Epoch 17/256
3063/3063 - 30s - loss: 0.3004 - val_loss: 0.2989
Epoch 18/256
3063/3063 - 30s - loss: 0.3001 - val_loss: 0.3043
Epoch 19/256
3063/3063 - 30s - loss: 0.2971 - val_loss: 0.2988
Epoch 20/256
3063/3063 - 30s - loss: 0.2948 - val_loss: 0.2971
Epoch 21/256
3063/3063 - 30s - loss: 0.2926 - val_loss: 0.2882
Epoch 22/256
3063/3063 - 30s - loss: 0.2904 - val_loss: 0.3125
Epoch 23/256
3063/3063 - 30s - loss: 0.2895 - val_loss: 0.2909
Epoch 24/256
3063/3063 - 30s - loss: 0.2883 - val_loss: 0.2943
Epoch 25/256
3063/3063 - 30s - loss: 0.2865 - val_loss: 0.2924
Epoch 26/256
3063/3063 - 30s - loss: 0.2861 - val_loss: 0.2926
Epoch 27/256
3063/3063 - 30s - loss: 0.2849 - val_loss: 0.2873
Epoch 28/256
3063/3063 - 30s - loss: 0.2833 - val_loss: 0.2873
Epoch 29/256
3063/3063 - 30s - loss: 0.2815 - val_loss: 0.2878
Epoch 30/256
3063/3063 - 30s - loss: 0.2803 - val_loss: 0.2820
Epoch 31/256
3063/3063 - 30s - loss: 0.2798 - val_loss: 0.2905
Epoch 32/256
3063/3063 - 30s - loss: 0.2784 - val_loss: 0.2871
Epoch 33/256
3063/3063 - 30s - loss: 0.2778 - val_loss: 0.2831
Epoch 34/256
3063/3063 - 30s - loss: 0.2775 - val_loss: 0.2848
Epoch 35/256
3063/3063 - 30s - loss: 0.2751 - val_loss: 0.2893
Epoch 36/256
3063/3063 - 30s - loss: 0.2757 - val_loss: 0.2862
Epoch 37/256
3063/3063 - 30s - loss: 0.2742 - val_loss: 0.2881
Epoch 38/256
3063/3063 - 30s - loss: 0.2730 - val_loss: 0.2821
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 2)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 32s - loss: 0.4202 - val_loss: 0.3680
Epoch 2/256
3063/3063 - 30s - loss: 0.3689 - val_loss: 0.3583
Epoch 3/256
3063/3063 - 30s - loss: 0.3599 - val_loss: 0.3515
Epoch 4/256
3063/3063 - 30s - loss: 0.3560 - val_loss: 0.3493
Epoch 5/256
3063/3063 - 30s - loss: 0.3516 - val_loss: 0.3496
Epoch 6/256
3063/3063 - 30s - loss: 0.3478 - val_loss: 0.3445
Epoch 7/256
3063/3063 - 30s - loss: 0.3460 - val_loss: 0.3398
Epoch 8/256
3063/3063 - 30s - loss: 0.3432 - val_loss: 0.3368
Epoch 9/256
3063/3063 - 30s - loss: 0.3401 - val_loss: 0.3562
Epoch 10/256
3063/3063 - 30s - loss: 0.3376 - val_loss: 0.3347
Epoch 11/256
3063/3063 - 30s - loss: 0.3357 - val_loss: 0.3329
Epoch 12/256
3063/3063 - 30s - loss: 0.3334 - val_loss: 0.3283
Epoch 13/256
3063/3063 - 30s - loss: 0.3309 - val_loss: 0.3297
Epoch 14/256
3063/3063 - 30s - loss: 0.3279 - val_loss: 0.3297
Epoch 15/256
3063/3063 - 30s - loss: 0.3254 - val_loss: 0.3274
Epoch 16/256
3063/3063 - 30s - loss: 0.3225 - val_loss: 0.3299
Epoch 17/256
3063/3063 - 30s - loss: 0.3197 - val_loss: 0.3222
Epoch 18/256
3063/3063 - 30s - loss: 0.3168 - val_loss: 0.3457
Epoch 19/256
3063/3063 - 30s - loss: 0.3147 - val_loss: 0.3246
Epoch 20/256
3063/3063 - 30s - loss: 0.3121 - val_loss: 0.3244
Epoch 21/256
3063/3063 - 30s - loss: 0.3088 - val_loss: 0.3197
Epoch 22/256
3063/3063 - 30s - loss: 0.3086 - val_loss: 0.3081
Epoch 23/256
3063/3063 - 30s - loss: 0.3064 - val_loss: 0.3115
Epoch 24/256
3063/3063 - 30s - loss: 0.3046 - val_loss: 0.3071
Epoch 25/256
3063/3063 - 30s - loss: 0.3018 - val_loss: 0.3176
Epoch 26/256
3063/3063 - 30s - loss: 0.3009 - val_loss: 0.3103
Epoch 27/256
3063/3063 - 30s - loss: 0.2985 - val_loss: 0.2982
Epoch 28/256
3063/3063 - 30s - loss: 0.2985 - val_loss: 0.3011
Epoch 29/256
3063/3063 - 30s - loss: 0.2973 - val_loss: 0.3170
Epoch 30/256
3063/3063 - 30s - loss: 0.2955 - val_loss: 0.3006
Epoch 31/256
3063/3063 - 30s - loss: 0.2940 - val_loss: 0.3052
Epoch 32/256
3063/3063 - 30s - loss: 0.2924 - val_loss: 0.2972
Epoch 33/256
3063/3063 - 30s - loss: 0.2913 - val_loss: 0.2926
Epoch 34/256
3063/3063 - 30s - loss: 0.2909 - val_loss: 0.2959
Epoch 35/256
3063/3063 - 30s - loss: 0.2899 - val_loss: 0.3073
Epoch 36/256
3063/3063 - 30s - loss: 0.2878 - val_loss: 0.2886
Epoch 37/256
3063/3063 - 30s - loss: 0.2858 - val_loss: 0.2902
Epoch 38/256
3063/3063 - 30s - loss: 0.2853 - val_loss: 0.2899
Epoch 39/256
3063/3063 - 30s - loss: 0.2842 - val_loss: 0.2921
Epoch 40/256
3063/3063 - 30s - loss: 0.2838 - val_loss: 0.2935
Epoch 41/256
3063/3063 - 30s - loss: 0.2826 - val_loss: 0.2869
Epoch 42/256
3063/3063 - 30s - loss: 0.2808 - val_loss: 0.2839
Epoch 43/256
3063/3063 - 30s - loss: 0.2795 - val_loss: 0.2906
Epoch 44/256
3063/3063 - 30s - loss: 0.2785 - val_loss: 0.2827
Epoch 45/256
3063/3063 - 30s - loss: 0.2782 - val_loss: 0.2803
Epoch 46/256
3063/3063 - 30s - loss: 0.2775 - val_loss: 0.2849
Epoch 47/256
3063/3063 - 30s - loss: 0.2765 - val_loss: 0.2850
Epoch 48/256
3063/3063 - 30s - loss: 0.2750 - val_loss: 0.2829
Epoch 49/256
3063/3063 - 30s - loss: 0.2742 - val_loss: 0.2821
Epoch 50/256
3063/3063 - 30s - loss: 0.2746 - val_loss: 0.2770
Epoch 51/256
3063/3063 - 30s - loss: 0.2724 - val_loss: 0.2843
Epoch 52/256
3063/3063 - 35s - loss: 0.2716 - val_loss: 0.2853
Epoch 53/256
3063/3063 - 30s - loss: 0.2725 - val_loss: 0.2803
Epoch 54/256
3063/3063 - 30s - loss: 0.2709 - val_loss: 0.2846
Epoch 55/256
3063/3063 - 30s - loss: 0.2692 - val_loss: 0.2711
Epoch 56/256
3063/3063 - 30s - loss: 0.2686 - val_loss: 0.2751
Epoch 57/256
3063/3063 - 30s - loss: 0.2683 - val_loss: 0.2745
Epoch 58/256
3063/3063 - 30s - loss: 0.2664 - val_loss: 0.2790
Epoch 59/256
3063/3063 - 30s - loss: 0.2666 - val_loss: 0.2734
Epoch 60/256
3063/3063 - 30s - loss: 0.2654 - val_loss: 0.2880
Epoch 61/256
3063/3063 - 30s - loss: 0.2646 - val_loss: 0.2773
Epoch 62/256
3063/3063 - 30s - loss: 0.2628 - val_loss: 0.2709
Epoch 63/256
3063/3063 - 30s - loss: 0.2648 - val_loss: 0.2720
Epoch 64/256
3063/3063 - 30s - loss: 0.2631 - val_loss: 0.2787
Epoch 65/256
3063/3063 - 30s - loss: 0.2623 - val_loss: 0.2709
Epoch 66/256
3063/3063 - 30s - loss: 0.2615 - val_loss: 0.2869
Epoch 67/256
3063/3063 - 30s - loss: 0.2613 - val_loss: 0.2742
Epoch 68/256
3063/3063 - 30s - loss: 0.2597 - val_loss: 0.2743
Epoch 69/256
3063/3063 - 30s - loss: 0.2595 - val_loss: 0.2719
Epoch 70/256
3063/3063 - 30s - loss: 0.2590 - val_loss: 0.2687
Epoch 71/256
3063/3063 - 30s - loss: 0.2580 - val_loss: 0.2670
Epoch 72/256
3063/3063 - 30s - loss: 0.2562 - val_loss: 0.2925
Epoch 73/256
3063/3063 - 30s - loss: 0.2566 - val_loss: 0.2694
Epoch 74/256
3063/3063 - 30s - loss: 0.2562 - val_loss: 0.2734
Epoch 75/256
3063/3063 - 30s - loss: 0.2552 - val_loss: 0.2731
Epoch 76/256
3063/3063 - 30s - loss: 0.2550 - val_loss: 0.2685
Epoch 77/256
3063/3063 - 30s - loss: 0.2540 - val_loss: 0.2685
Epoch 78/256
3063/3063 - 30s - loss: 0.2528 - val_loss: 0.2681
Epoch 79/256
3063/3063 - 30s - loss: 0.2530 - val_loss: 0.3075
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 2)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.3942 - val_loss: 0.3584
Epoch 2/256
3063/3063 - 30s - loss: 0.3661 - val_loss: 0.3497
Epoch 3/256
3063/3063 - 30s - loss: 0.3544 - val_loss: 0.3467
Epoch 4/256
3063/3063 - 30s - loss: 0.3480 - val_loss: 0.3400
Epoch 5/256
3063/3063 - 30s - loss: 0.3441 - val_loss: 0.3377
Epoch 6/256
3063/3063 - 30s - loss: 0.3406 - val_loss: 0.3354
Epoch 7/256
3063/3063 - 30s - loss: 0.3377 - val_loss: 0.3519
Epoch 8/256
3063/3063 - 30s - loss: 0.3335 - val_loss: 0.3313
Epoch 9/256
3063/3063 - 30s - loss: 0.3295 - val_loss: 0.3263
Epoch 10/256
3063/3063 - 30s - loss: 0.3258 - val_loss: 0.3212
Epoch 11/256
3063/3063 - 30s - loss: 0.3207 - val_loss: 0.3185
Epoch 12/256
3063/3063 - 30s - loss: 0.3163 - val_loss: 0.3158
Epoch 13/256
3063/3063 - 30s - loss: 0.3123 - val_loss: 0.3079
Epoch 14/256
3063/3063 - 30s - loss: 0.3097 - val_loss: 0.3070
Epoch 15/256
3063/3063 - 30s - loss: 0.3061 - val_loss: 0.3237
Epoch 16/256
3063/3063 - 30s - loss: 0.3032 - val_loss: 0.3242
Epoch 17/256
3063/3063 - 30s - loss: 0.3012 - val_loss: 0.3127
Epoch 18/256
3063/3063 - 30s - loss: 0.2986 - val_loss: 0.2944
Epoch 19/256
3063/3063 - 30s - loss: 0.2960 - val_loss: 0.3044
Epoch 20/256
3063/3063 - 30s - loss: 0.2946 - val_loss: 0.2962
Epoch 21/256
3063/3063 - 30s - loss: 0.2926 - val_loss: 0.2992
Epoch 22/256
3063/3063 - 30s - loss: 0.2915 - val_loss: 0.2973
Epoch 23/256
3063/3063 - 30s - loss: 0.2899 - val_loss: 0.2983
Epoch 24/256
3063/3063 - 30s - loss: 0.2887 - val_loss: 0.3018
Epoch 25/256
3063/3063 - 30s - loss: 0.2859 - val_loss: 0.2895
Epoch 26/256
3063/3063 - 30s - loss: 0.2863 - val_loss: 0.2876
Epoch 27/256
3063/3063 - 30s - loss: 0.2844 - val_loss: 0.3133
Epoch 28/256
3063/3063 - 30s - loss: 0.2843 - val_loss: 0.2890
Epoch 29/256
3063/3063 - 30s - loss: 0.2833 - val_loss: 0.2934
Epoch 30/256
3063/3063 - 30s - loss: 0.2819 - val_loss: 0.3045
Epoch 31/256
3063/3063 - 30s - loss: 0.2801 - val_loss: 0.2834
Epoch 32/256
3063/3063 - 30s - loss: 0.2791 - val_loss: 0.2810
Epoch 33/256
3063/3063 - 30s - loss: 0.2791 - val_loss: 0.2875
Epoch 34/256
3063/3063 - 30s - loss: 0.2778 - val_loss: 0.2891
Epoch 35/256
3063/3063 - 30s - loss: 0.2770 - val_loss: 0.3008
Epoch 36/256
3063/3063 - 30s - loss: 0.2756 - val_loss: 0.2859
Epoch 37/256
3063/3063 - 30s - loss: 0.2756 - val_loss: 0.2842
Epoch 38/256
3063/3063 - 30s - loss: 0.2735 - val_loss: 0.2885
Epoch 39/256
3063/3063 - 30s - loss: 0.2729 - val_loss: 0.2891
Epoch 40/256
3063/3063 - 30s - loss: 0.2720 - val_loss: 0.2811
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 2)_64
			 2 [0.9543174087139416, 0.9547097179589477, 0.9529224494840138, 0.9569876382704114, 0.9540374821924992, 0.9521935165176931, 0.9565746266084885, 0.9520652458669345]
		LATENT DIM 4
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 32s - loss: 0.4200 - val_loss: 0.3883
Epoch 2/256
3063/3063 - 30s - loss: 0.3777 - val_loss: 0.3761
Epoch 3/256
3063/3063 - 30s - loss: 0.3653 - val_loss: 0.3576
Epoch 4/256
3063/3063 - 30s - loss: 0.3585 - val_loss: 0.3547
Epoch 5/256
3063/3063 - 30s - loss: 0.3546 - val_loss: 0.3534
Epoch 6/256
3063/3063 - 30s - loss: 0.3519 - val_loss: 0.3508
Epoch 7/256
3063/3063 - 30s - loss: 0.3492 - val_loss: 0.3548
Epoch 8/256
3063/3063 - 30s - loss: 0.3475 - val_loss: 0.3488
Epoch 9/256
3063/3063 - 30s - loss: 0.3466 - val_loss: 0.3443
Epoch 10/256
3063/3063 - 30s - loss: 0.3453 - val_loss: 0.3481
Epoch 11/256
3063/3063 - 35s - loss: 0.3441 - val_loss: 0.3527
Epoch 12/256
3063/3063 - 30s - loss: 0.3427 - val_loss: 0.3468
Epoch 13/256
3063/3063 - 30s - loss: 0.3425 - val_loss: 0.3397
Epoch 14/256
3063/3063 - 30s - loss: 0.3418 - val_loss: 0.3427
Epoch 15/256
3063/3063 - 30s - loss: 0.3399 - val_loss: 0.3403
Epoch 16/256
3063/3063 - 30s - loss: 0.3392 - val_loss: 0.3505
Epoch 17/256
3063/3063 - 30s - loss: 0.3371 - val_loss: 0.3378
Epoch 18/256
3063/3063 - 30s - loss: 0.3360 - val_loss: 0.3431
Epoch 19/256
3063/3063 - 30s - loss: 0.3352 - val_loss: 0.3377
Epoch 20/256
3063/3063 - 30s - loss: 0.3341 - val_loss: 0.3367
Epoch 21/256
3063/3063 - 30s - loss: 0.3315 - val_loss: 0.3356
Epoch 22/256
3063/3063 - 30s - loss: 0.3312 - val_loss: 0.3744
Epoch 23/256
3063/3063 - 30s - loss: 0.3291 - val_loss: 0.3343
Epoch 24/256
3063/3063 - 30s - loss: 0.3280 - val_loss: 0.3559
Epoch 25/256
3063/3063 - 30s - loss: 0.3269 - val_loss: 0.3257
Epoch 26/256
3063/3063 - 30s - loss: 0.3255 - val_loss: 0.3270
Epoch 27/256
3063/3063 - 30s - loss: 0.3238 - val_loss: 0.3395
Epoch 28/256
3063/3063 - 30s - loss: 0.3229 - val_loss: 0.3278
Epoch 29/256
3063/3063 - 30s - loss: 0.3211 - val_loss: 0.3203
Epoch 30/256
3063/3063 - 30s - loss: 0.3179 - val_loss: 0.3224
Epoch 31/256
3063/3063 - 30s - loss: 0.3173 - val_loss: 0.3208
Epoch 32/256
3063/3063 - 30s - loss: 0.3147 - val_loss: 0.3259
Epoch 33/256
3063/3063 - 30s - loss: 0.3139 - val_loss: 0.3230
Epoch 34/256
3063/3063 - 30s - loss: 0.3105 - val_loss: 0.3205
Epoch 35/256
3063/3063 - 30s - loss: 0.3097 - val_loss: 0.3199
Epoch 36/256
3063/3063 - 30s - loss: 0.3079 - val_loss: 0.3128
Epoch 37/256
3063/3063 - 30s - loss: 0.3071 - val_loss: 0.3093
Epoch 38/256
3063/3063 - 30s - loss: 0.3049 - val_loss: 0.3247
Epoch 39/256
3063/3063 - 30s - loss: 0.3038 - val_loss: 0.3160
Epoch 40/256
3063/3063 - 30s - loss: 0.3031 - val_loss: 0.3090
Epoch 41/256
3063/3063 - 30s - loss: 0.3016 - val_loss: 0.3033
Epoch 42/256
3063/3063 - 30s - loss: 0.3012 - val_loss: 0.3061
Epoch 43/256
3063/3063 - 30s - loss: 0.3001 - val_loss: 0.3046
Epoch 44/256
3063/3063 - 30s - loss: 0.2991 - val_loss: 0.3070
Epoch 45/256
3063/3063 - 30s - loss: 0.2979 - val_loss: 0.3043
Epoch 46/256
3063/3063 - 30s - loss: 0.2973 - val_loss: 0.3062
Epoch 47/256
3063/3063 - 30s - loss: 0.2962 - val_loss: 0.3124
Epoch 48/256
3063/3063 - 30s - loss: 0.2950 - val_loss: 0.3061
Epoch 49/256
3063/3063 - 30s - loss: 0.2938 - val_loss: 0.3012
Epoch 50/256
3063/3063 - 30s - loss: 0.2924 - val_loss: 0.2963
Epoch 51/256
3063/3063 - 30s - loss: 0.2920 - val_loss: 0.3044
Epoch 52/256
3063/3063 - 30s - loss: 0.2913 - val_loss: 0.3010
Epoch 53/256
3063/3063 - 30s - loss: 0.2897 - val_loss: 0.2999
Epoch 54/256
3063/3063 - 30s - loss: 0.2887 - val_loss: 0.2951
Epoch 55/256
3063/3063 - 30s - loss: 0.2876 - val_loss: 0.2932
Epoch 56/256
3063/3063 - 30s - loss: 0.2869 - val_loss: 0.2931
Epoch 57/256
3063/3063 - 30s - loss: 0.2847 - val_loss: 0.2963
Epoch 58/256
3063/3063 - 35s - loss: 0.2837 - val_loss: 0.2969
Epoch 59/256
3063/3063 - 30s - loss: 0.2833 - val_loss: 0.2894
Epoch 60/256
3063/3063 - 30s - loss: 0.2824 - val_loss: 0.2938
Epoch 61/256
3063/3063 - 30s - loss: 0.2803 - val_loss: 0.2865
Epoch 62/256
3063/3063 - 30s - loss: 0.2790 - val_loss: 0.2914
Epoch 63/256
3063/3063 - 30s - loss: 0.2776 - val_loss: 0.2992
Epoch 64/256
3063/3063 - 30s - loss: 0.2771 - val_loss: 0.2907
Epoch 65/256
3063/3063 - 30s - loss: 0.2749 - val_loss: 0.2888
Epoch 66/256
3063/3063 - 30s - loss: 0.2747 - val_loss: 0.2907
Epoch 67/256
3063/3063 - 30s - loss: 0.2735 - val_loss: 0.2893
Epoch 68/256
3063/3063 - 30s - loss: 0.2725 - val_loss: 0.2812
Epoch 69/256
3063/3063 - 30s - loss: 0.2710 - val_loss: 0.2809
Epoch 70/256
3063/3063 - 30s - loss: 0.2696 - val_loss: 0.2799
Epoch 71/256
3063/3063 - 30s - loss: 0.2704 - val_loss: 0.2807
Epoch 72/256
3063/3063 - 30s - loss: 0.2693 - val_loss: 0.3139
Epoch 73/256
3063/3063 - 30s - loss: 0.2676 - val_loss: 0.2827
Epoch 74/256
3063/3063 - 30s - loss: 0.2671 - val_loss: 0.2797
Epoch 75/256
3063/3063 - 30s - loss: 0.2670 - val_loss: 0.2749
Epoch 76/256
3063/3063 - 30s - loss: 0.2663 - val_loss: 0.2770
Epoch 77/256
3063/3063 - 30s - loss: 0.2648 - val_loss: 0.2788
Epoch 78/256
3063/3063 - 30s - loss: 0.2632 - val_loss: 0.2767
Epoch 79/256
3063/3063 - 30s - loss: 0.2631 - val_loss: 0.2933
Epoch 80/256
3063/3063 - 35s - loss: 0.2625 - val_loss: 0.2744
Epoch 81/256
3063/3063 - 30s - loss: 0.2631 - val_loss: 0.2757
Epoch 82/256
3063/3063 - 30s - loss: 0.2616 - val_loss: 0.3009
Epoch 83/256
3063/3063 - 30s - loss: 0.2603 - val_loss: 0.2785
Epoch 84/256
3063/3063 - 30s - loss: 0.2609 - val_loss: 0.2776
Epoch 85/256
3063/3063 - 30s - loss: 0.2597 - val_loss: 0.2766
Epoch 86/256
3063/3063 - 30s - loss: 0.2598 - val_loss: 0.2757
Epoch 87/256
3063/3063 - 30s - loss: 0.2583 - val_loss: 0.2830
Epoch 88/256
3063/3063 - 30s - loss: 0.2588 - val_loss: 0.2736
Epoch 89/256
3063/3063 - 30s - loss: 0.2571 - val_loss: 0.2754
Epoch 90/256
3063/3063 - 30s - loss: 0.2571 - val_loss: 0.2759
Epoch 91/256
3063/3063 - 35s - loss: 0.2568 - val_loss: 0.2780
Epoch 92/256
3063/3063 - 30s - loss: 0.2564 - val_loss: 0.2831
Epoch 93/256
3063/3063 - 30s - loss: 0.2557 - val_loss: 0.2738
Epoch 94/256
3063/3063 - 30s - loss: 0.2551 - val_loss: 0.2851
Epoch 95/256
3063/3063 - 30s - loss: 0.2551 - val_loss: 0.2806
Epoch 96/256
3063/3063 - 30s - loss: 0.2539 - val_loss: 0.2712
Epoch 97/256
3063/3063 - 30s - loss: 0.2530 - val_loss: 0.2797
Epoch 98/256
3063/3063 - 30s - loss: 0.2526 - val_loss: 0.2787
Epoch 99/256
3063/3063 - 30s - loss: 0.2535 - val_loss: 0.2732
Epoch 100/256
3063/3063 - 30s - loss: 0.2529 - val_loss: 0.2702
Epoch 101/256
3063/3063 - 30s - loss: 0.2525 - val_loss: 0.2710
Epoch 102/256
3063/3063 - 31s - loss: 0.2515 - val_loss: 0.2767
Epoch 103/256
3063/3063 - 34s - loss: 0.2510 - val_loss: 0.2697
Epoch 104/256
3063/3063 - 33s - loss: 0.2519 - val_loss: 0.2735
Epoch 105/256
3063/3063 - 34s - loss: 0.2496 - val_loss: 0.2732
Epoch 106/256
3063/3063 - 34s - loss: 0.2496 - val_loss: 0.2775
Epoch 107/256
3063/3063 - 34s - loss: 0.2488 - val_loss: 0.2652
Epoch 108/256
3063/3063 - 34s - loss: 0.2492 - val_loss: 0.2678
Epoch 109/256
3063/3063 - 34s - loss: 0.2482 - val_loss: 0.2685
Epoch 110/256
3063/3063 - 34s - loss: 0.2480 - val_loss: 0.2748
Epoch 111/256
3063/3063 - 34s - loss: 0.2477 - val_loss: 0.2726
Epoch 112/256
3063/3063 - 34s - loss: 0.2464 - val_loss: 0.2721
Epoch 113/256
3063/3063 - 34s - loss: 0.2466 - val_loss: 0.2698
Epoch 114/256
3063/3063 - 34s - loss: 0.2462 - val_loss: 0.2722
Epoch 115/256
3063/3063 - 34s - loss: 0.2454 - val_loss: 0.2719
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 4)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 35s - loss: 0.4181 - val_loss: 0.3789
Epoch 2/256
3063/3063 - 34s - loss: 0.3807 - val_loss: 0.3619
Epoch 3/256
3063/3063 - 34s - loss: 0.3660 - val_loss: 0.3716
Epoch 4/256
3063/3063 - 34s - loss: 0.3596 - val_loss: 0.3609
Epoch 5/256
3063/3063 - 33s - loss: 0.3556 - val_loss: 0.3506
Epoch 6/256
3063/3063 - 34s - loss: 0.3525 - val_loss: 0.3583
Epoch 7/256
3063/3063 - 33s - loss: 0.3487 - val_loss: 0.3468
Epoch 8/256
3063/3063 - 34s - loss: 0.3462 - val_loss: 0.3426
Epoch 9/256
3063/3063 - 33s - loss: 0.3442 - val_loss: 0.3380
Epoch 10/256
3063/3063 - 34s - loss: 0.3405 - val_loss: 0.3369
Epoch 11/256
3063/3063 - 33s - loss: 0.3396 - val_loss: 0.3333
Epoch 12/256
3063/3063 - 34s - loss: 0.3371 - val_loss: 0.3323
Epoch 13/256
3063/3063 - 33s - loss: 0.3345 - val_loss: 0.3334
Epoch 14/256
3063/3063 - 33s - loss: 0.3334 - val_loss: 0.3393
Epoch 15/256
3063/3063 - 33s - loss: 0.3313 - val_loss: 0.3379
Epoch 16/256
3063/3063 - 33s - loss: 0.3300 - val_loss: 0.3274
Epoch 17/256
3063/3063 - 33s - loss: 0.3282 - val_loss: 0.3243
Epoch 18/256
3063/3063 - 33s - loss: 0.3258 - val_loss: 0.3339
Epoch 19/256
3063/3063 - 33s - loss: 0.3242 - val_loss: 0.3302
Epoch 20/256
3063/3063 - 34s - loss: 0.3222 - val_loss: 0.3205
Epoch 21/256
3063/3063 - 33s - loss: 0.3206 - val_loss: 0.3280
Epoch 22/256
3063/3063 - 34s - loss: 0.3185 - val_loss: 0.3248
Epoch 23/256
3063/3063 - 33s - loss: 0.3166 - val_loss: 0.3180
Epoch 24/256
3063/3063 - 34s - loss: 0.3148 - val_loss: 0.3140
Epoch 25/256
3063/3063 - 33s - loss: 0.3131 - val_loss: 0.3149
Epoch 26/256
3063/3063 - 34s - loss: 0.3121 - val_loss: 0.3129
Epoch 27/256
3063/3063 - 33s - loss: 0.3104 - val_loss: 0.3125
Epoch 28/256
3063/3063 - 33s - loss: 0.3088 - val_loss: 0.3334
Epoch 29/256
3063/3063 - 33s - loss: 0.3073 - val_loss: 0.3115
Epoch 30/256
3063/3063 - 33s - loss: 0.3054 - val_loss: 0.3072
Epoch 31/256
3063/3063 - 33s - loss: 0.3038 - val_loss: 0.3137
Epoch 32/256
3063/3063 - 33s - loss: 0.3036 - val_loss: 0.3061
Epoch 33/256
3063/3063 - 33s - loss: 0.3018 - val_loss: 0.3165
Epoch 34/256
3063/3063 - 33s - loss: 0.3007 - val_loss: 0.3016
Epoch 35/256
3063/3063 - 34s - loss: 0.2996 - val_loss: 0.3131
Epoch 36/256
3063/3063 - 33s - loss: 0.2978 - val_loss: 0.3015
Epoch 37/256
3063/3063 - 33s - loss: 0.2966 - val_loss: 0.3013
Epoch 38/256
3063/3063 - 33s - loss: 0.2961 - val_loss: 0.2997
Epoch 39/256
3063/3063 - 33s - loss: 0.2942 - val_loss: 0.2949
Epoch 40/256
3063/3063 - 33s - loss: 0.2925 - val_loss: 0.2941
Epoch 41/256
3063/3063 - 33s - loss: 0.2906 - val_loss: 0.2995
Epoch 42/256
3063/3063 - 33s - loss: 0.2891 - val_loss: 0.2925
Epoch 43/256
3063/3063 - 33s - loss: 0.2876 - val_loss: 0.2902
Epoch 44/256
3063/3063 - 33s - loss: 0.2855 - val_loss: 0.2933
Epoch 45/256
3063/3063 - 33s - loss: 0.2841 - val_loss: 0.2857
Epoch 46/256
3063/3063 - 33s - loss: 0.2821 - val_loss: 0.2967
Epoch 47/256
3063/3063 - 33s - loss: 0.2815 - val_loss: 0.2869
Epoch 48/256
3063/3063 - 33s - loss: 0.2792 - val_loss: 0.2838
Epoch 49/256
3063/3063 - 33s - loss: 0.2775 - val_loss: 0.2835
Epoch 50/256
3063/3063 - 34s - loss: 0.2755 - val_loss: 0.2803
Epoch 51/256
3063/3063 - 34s - loss: 0.2752 - val_loss: 0.2930
Epoch 52/256
3063/3063 - 33s - loss: 0.2729 - val_loss: 0.3105
Epoch 53/256
3063/3063 - 33s - loss: 0.2731 - val_loss: 0.2792
Epoch 54/256
3063/3063 - 34s - loss: 0.2707 - val_loss: 0.2844
Epoch 55/256
3063/3063 - 33s - loss: 0.2700 - val_loss: 0.2794
Epoch 56/256
3063/3063 - 33s - loss: 0.2690 - val_loss: 0.2841
Epoch 57/256
3063/3063 - 33s - loss: 0.2678 - val_loss: 0.2829
Epoch 58/256
3063/3063 - 33s - loss: 0.2668 - val_loss: 0.2763
Epoch 59/256
3063/3063 - 38s - loss: 0.2662 - val_loss: 0.2749
Epoch 60/256
3063/3063 - 33s - loss: 0.2656 - val_loss: 0.2897
Epoch 61/256
3063/3063 - 33s - loss: 0.2647 - val_loss: 0.2704
Epoch 62/256
3063/3063 - 33s - loss: 0.2638 - val_loss: 0.2724
Epoch 63/256
3063/3063 - 33s - loss: 0.2631 - val_loss: 0.2801
Epoch 64/256
3063/3063 - 33s - loss: 0.2622 - val_loss: 0.2706
Epoch 65/256
3063/3063 - 33s - loss: 0.2600 - val_loss: 0.2716
Epoch 66/256
3063/3063 - 33s - loss: 0.2602 - val_loss: 0.2707
Epoch 67/256
3063/3063 - 33s - loss: 0.2594 - val_loss: 0.2699
Epoch 68/256
3063/3063 - 33s - loss: 0.2586 - val_loss: 0.2736
Epoch 69/256
3063/3063 - 33s - loss: 0.2578 - val_loss: 0.2726
Epoch 70/256
3063/3063 - 34s - loss: 0.2560 - val_loss: 0.2749
Epoch 71/256
3063/3063 - 34s - loss: 0.2566 - val_loss: 0.2712
Epoch 72/256
3063/3063 - 34s - loss: 0.2559 - val_loss: 0.2708
Epoch 73/256
3063/3063 - 34s - loss: 0.2558 - val_loss: 0.2674
Epoch 74/256
3063/3063 - 34s - loss: 0.2548 - val_loss: 0.2698
Epoch 75/256
3063/3063 - 34s - loss: 0.2538 - val_loss: 0.2713
Epoch 76/256
3063/3063 - 34s - loss: 0.2541 - val_loss: 0.3013
Epoch 77/256
3063/3063 - 33s - loss: 0.2531 - val_loss: 0.2775
Epoch 78/256
3063/3063 - 34s - loss: 0.2519 - val_loss: 0.2719
Epoch 79/256
3063/3063 - 34s - loss: 0.2515 - val_loss: 0.2667
Epoch 80/256
3063/3063 - 34s - loss: 0.2514 - val_loss: 0.2695
Epoch 81/256
3063/3063 - 34s - loss: 0.2508 - val_loss: 0.2646
Epoch 82/256
3063/3063 - 33s - loss: 0.2495 - val_loss: 0.2622
Epoch 83/256
3063/3063 - 33s - loss: 0.2498 - val_loss: 0.2667
Epoch 84/256
3063/3063 - 34s - loss: 0.2493 - val_loss: 0.2704
Epoch 85/256
3063/3063 - 33s - loss: 0.2482 - val_loss: 0.2641
Epoch 86/256
3063/3063 - 33s - loss: 0.2474 - val_loss: 0.2705
Epoch 87/256
3063/3063 - 33s - loss: 0.2465 - val_loss: 0.2634
Epoch 88/256
3063/3063 - 33s - loss: 0.2461 - val_loss: 0.2641
Epoch 89/256
3063/3063 - 33s - loss: 0.2456 - val_loss: 0.2661
Epoch 90/256
3063/3063 - 33s - loss: 0.2454 - val_loss: 0.2666
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 4)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 35s - loss: 0.4140 - val_loss: 0.3671
Epoch 2/256
3063/3063 - 33s - loss: 0.3693 - val_loss: 0.3543
Epoch 3/256
3063/3063 - 33s - loss: 0.3600 - val_loss: 0.3516
Epoch 4/256
3063/3063 - 34s - loss: 0.3557 - val_loss: 0.3495
Epoch 5/256
3063/3063 - 34s - loss: 0.3513 - val_loss: 0.3424
Epoch 6/256
3063/3063 - 34s - loss: 0.3484 - val_loss: 0.3465
Epoch 7/256
3063/3063 - 34s - loss: 0.3456 - val_loss: 0.3427
Epoch 8/256
3063/3063 - 34s - loss: 0.3420 - val_loss: 0.3433
Epoch 9/256
3063/3063 - 34s - loss: 0.3406 - val_loss: 0.3360
Epoch 10/256
3063/3063 - 34s - loss: 0.3382 - val_loss: 0.3369
Epoch 11/256
3063/3063 - 34s - loss: 0.3362 - val_loss: 0.3595
Epoch 12/256
3063/3063 - 34s - loss: 0.3356 - val_loss: 0.3441
Epoch 13/256
3063/3063 - 34s - loss: 0.3334 - val_loss: 0.3299
Epoch 14/256
3063/3063 - 34s - loss: 0.3320 - val_loss: 0.3323
Epoch 15/256
3063/3063 - 34s - loss: 0.3313 - val_loss: 0.3337
Epoch 16/256
3063/3063 - 34s - loss: 0.3291 - val_loss: 0.3279
Epoch 17/256
3063/3063 - 34s - loss: 0.3277 - val_loss: 0.3297
Epoch 18/256
3063/3063 - 34s - loss: 0.3269 - val_loss: 0.3335
Epoch 19/256
3063/3063 - 34s - loss: 0.3261 - val_loss: 0.3343
Epoch 20/256
3063/3063 - 34s - loss: 0.3229 - val_loss: 0.3237
Epoch 21/256
3063/3063 - 34s - loss: 0.3216 - val_loss: 0.3212
Epoch 22/256
3063/3063 - 34s - loss: 0.3188 - val_loss: 0.3577
Epoch 23/256
3063/3063 - 34s - loss: 0.3183 - val_loss: 0.3215
Epoch 24/256
3063/3063 - 34s - loss: 0.3160 - val_loss: 0.3195
Epoch 25/256
3063/3063 - 34s - loss: 0.3142 - val_loss: 0.3168
Epoch 26/256
3063/3063 - 34s - loss: 0.3123 - val_loss: 0.3213
Epoch 27/256
3063/3063 - 34s - loss: 0.3101 - val_loss: 0.3110
Epoch 28/256
3063/3063 - 34s - loss: 0.3083 - val_loss: 0.3153
Epoch 29/256
3063/3063 - 34s - loss: 0.3078 - val_loss: 0.3064
Epoch 30/256
3063/3063 - 34s - loss: 0.3059 - val_loss: 0.3114
Epoch 31/256
3063/3063 - 33s - loss: 0.3043 - val_loss: 0.3061
Epoch 32/256
3063/3063 - 34s - loss: 0.3029 - val_loss: 0.3309
Epoch 33/256
3063/3063 - 34s - loss: 0.3025 - val_loss: 0.3048
Epoch 34/256
3063/3063 - 34s - loss: 0.2998 - val_loss: 0.3102
Epoch 35/256
3063/3063 - 34s - loss: 0.2982 - val_loss: 0.3036
Epoch 36/256
3063/3063 - 35s - loss: 0.2967 - val_loss: 0.3060
Epoch 37/256
3063/3063 - 34s - loss: 0.2944 - val_loss: 0.3148
Epoch 38/256
3063/3063 - 34s - loss: 0.2934 - val_loss: 0.2947
Epoch 39/256
3063/3063 - 34s - loss: 0.2905 - val_loss: 0.2994
Epoch 40/256
3063/3063 - 34s - loss: 0.2899 - val_loss: 0.2879
Epoch 41/256
3063/3063 - 34s - loss: 0.2876 - val_loss: 0.2981
Epoch 42/256
3063/3063 - 34s - loss: 0.2839 - val_loss: 0.2955
Epoch 43/256
3063/3063 - 34s - loss: 0.2837 - val_loss: 0.2881
Epoch 44/256
3063/3063 - 34s - loss: 0.2812 - val_loss: 0.2860
Epoch 45/256
3063/3063 - 34s - loss: 0.2795 - val_loss: 0.2812
Epoch 46/256
3063/3063 - 34s - loss: 0.2791 - val_loss: 0.2851
Epoch 47/256
3063/3063 - 34s - loss: 0.2765 - val_loss: 0.2820
Epoch 48/256
3063/3063 - 34s - loss: 0.2760 - val_loss: 0.2781
Epoch 49/256
3063/3063 - 34s - loss: 0.2751 - val_loss: 0.2816
Epoch 50/256
3063/3063 - 34s - loss: 0.2730 - val_loss: 0.2779
Epoch 51/256
3063/3063 - 34s - loss: 0.2736 - val_loss: 0.2909
Epoch 52/256
3063/3063 - 34s - loss: 0.2715 - val_loss: 0.2735
Epoch 53/256
3063/3063 - 34s - loss: 0.2711 - val_loss: 0.2755
Epoch 54/256
3063/3063 - 34s - loss: 0.2693 - val_loss: 0.2906
Epoch 55/256
3063/3063 - 34s - loss: 0.2683 - val_loss: 0.2753
Epoch 56/256
3063/3063 - 34s - loss: 0.2675 - val_loss: 0.2791
Epoch 57/256
3063/3063 - 33s - loss: 0.2667 - val_loss: 0.2794
Epoch 58/256
3063/3063 - 34s - loss: 0.2660 - val_loss: 0.2865
Epoch 59/256
3063/3063 - 34s - loss: 0.2651 - val_loss: 0.2765
Epoch 60/256
3063/3063 - 34s - loss: 0.2634 - val_loss: 0.2742
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 4)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 36s - loss: 0.3970 - val_loss: 0.3639
Epoch 2/256
3063/3063 - 34s - loss: 0.3644 - val_loss: 0.3526
Epoch 3/256
3063/3063 - 34s - loss: 0.3562 - val_loss: 0.3586
Epoch 4/256
3063/3063 - 34s - loss: 0.3501 - val_loss: 0.3483
Epoch 5/256
3063/3063 - 34s - loss: 0.3480 - val_loss: 0.3502
Epoch 6/256
3063/3063 - 34s - loss: 0.3444 - val_loss: 0.3389
Epoch 7/256
3063/3063 - 34s - loss: 0.3425 - val_loss: 0.3421
Epoch 8/256
3063/3063 - 34s - loss: 0.3407 - val_loss: 0.3367
Epoch 9/256
3063/3063 - 34s - loss: 0.3377 - val_loss: 0.3381
Epoch 10/256
3063/3063 - 34s - loss: 0.3359 - val_loss: 0.3430
Epoch 11/256
3063/3063 - 34s - loss: 0.3327 - val_loss: 0.3282
Epoch 12/256
3063/3063 - 34s - loss: 0.3272 - val_loss: 0.3217
Epoch 13/256
3063/3063 - 34s - loss: 0.3234 - val_loss: 0.3430
Epoch 14/256
3063/3063 - 35s - loss: 0.3195 - val_loss: 0.3137
Epoch 15/256
3063/3063 - 34s - loss: 0.3152 - val_loss: 0.3164
Epoch 16/256
3063/3063 - 34s - loss: 0.3108 - val_loss: 0.3289
Epoch 17/256
3063/3063 - 34s - loss: 0.3069 - val_loss: 0.3045
Epoch 18/256
3063/3063 - 34s - loss: 0.3027 - val_loss: 0.2970
Epoch 19/256
3063/3063 - 34s - loss: 0.2987 - val_loss: 0.3119
Epoch 20/256
3063/3063 - 35s - loss: 0.2956 - val_loss: 0.3045
Epoch 21/256
3063/3063 - 34s - loss: 0.2934 - val_loss: 0.2920
Epoch 22/256
3063/3063 - 34s - loss: 0.2900 - val_loss: 0.2905
Epoch 23/256
3063/3063 - 34s - loss: 0.2884 - val_loss: 0.2881
Epoch 24/256
3063/3063 - 34s - loss: 0.2856 - val_loss: 0.2959
Epoch 25/256
3063/3063 - 34s - loss: 0.2834 - val_loss: 0.2854
Epoch 26/256
3063/3063 - 34s - loss: 0.2803 - val_loss: 0.2857
Epoch 27/256
3063/3063 - 34s - loss: 0.2807 - val_loss: 0.3100
Epoch 28/256
3063/3063 - 34s - loss: 0.2783 - val_loss: 0.2890
Epoch 29/256
3063/3063 - 34s - loss: 0.2760 - val_loss: 0.2784
Epoch 30/256
3063/3063 - 34s - loss: 0.2748 - val_loss: 0.2798
Epoch 31/256
3063/3063 - 34s - loss: 0.2734 - val_loss: 0.2762
Epoch 32/256
3063/3063 - 34s - loss: 0.2721 - val_loss: 0.2738
Epoch 33/256
3063/3063 - 33s - loss: 0.2713 - val_loss: 0.2735
Epoch 34/256
3063/3063 - 33s - loss: 0.2693 - val_loss: 0.2752
Epoch 35/256
3063/3063 - 33s - loss: 0.2688 - val_loss: 0.2733
Epoch 36/256
3063/3063 - 33s - loss: 0.2672 - val_loss: 0.2697
Epoch 37/256
3063/3063 - 33s - loss: 0.2659 - val_loss: 0.2819
Epoch 38/256
3063/3063 - 33s - loss: 0.2656 - val_loss: 0.2712
Epoch 39/256
3063/3063 - 33s - loss: 0.2645 - val_loss: 0.2725
Epoch 40/256
3063/3063 - 34s - loss: 0.2633 - val_loss: 0.2721
Epoch 41/256
3063/3063 - 33s - loss: 0.2631 - val_loss: 0.2891
Epoch 42/256
3063/3063 - 33s - loss: 0.2610 - val_loss: 0.2684
Epoch 43/256
3063/3063 - 33s - loss: 0.2606 - val_loss: 0.2669
Epoch 44/256
3063/3063 - 33s - loss: 0.2602 - val_loss: 0.2702
Epoch 45/256
3063/3063 - 33s - loss: 0.2600 - val_loss: 0.2736
Epoch 46/256
3063/3063 - 34s - loss: 0.2585 - val_loss: 0.2682
Epoch 47/256
3063/3063 - 33s - loss: 0.2582 - val_loss: 0.2933
Epoch 48/256
3063/3063 - 34s - loss: 0.2565 - val_loss: 0.2674
Epoch 49/256
3063/3063 - 33s - loss: 0.2563 - val_loss: 0.2627
Epoch 50/256
3063/3063 - 34s - loss: 0.2552 - val_loss: 0.2726
Epoch 51/256
3063/3063 - 33s - loss: 0.2554 - val_loss: 0.2669
Epoch 52/256
3063/3063 - 33s - loss: 0.2548 - val_loss: 0.2664
Epoch 53/256
3063/3063 - 33s - loss: 0.2530 - val_loss: 0.2638
Epoch 54/256
3063/3063 - 33s - loss: 0.2509 - val_loss: 0.2669
Epoch 55/256
3063/3063 - 33s - loss: 0.2525 - val_loss: 0.2622
Epoch 56/256
3063/3063 - 33s - loss: 0.2515 - val_loss: 0.2657
Epoch 57/256
3063/3063 - 33s - loss: 0.2519 - val_loss: 0.2664
Epoch 58/256
3063/3063 - 33s - loss: 0.2505 - val_loss: 0.2617
Epoch 59/256
3063/3063 - 33s - loss: 0.2508 - val_loss: 0.2657
Epoch 60/256
3063/3063 - 33s - loss: 0.2488 - val_loss: 0.2643
Epoch 61/256
3063/3063 - 33s - loss: 0.2482 - val_loss: 0.2846
Epoch 62/256
3063/3063 - 33s - loss: 0.2479 - val_loss: 0.2675
Epoch 63/256
3063/3063 - 33s - loss: 0.2465 - val_loss: 0.2605
Epoch 64/256
3063/3063 - 33s - loss: 0.2468 - val_loss: 0.2765
Epoch 65/256
3063/3063 - 33s - loss: 0.2471 - val_loss: 0.2683
Epoch 66/256
3063/3063 - 33s - loss: 0.2445 - val_loss: 0.2616
Epoch 67/256
3063/3063 - 33s - loss: 0.2449 - val_loss: 0.2614
Epoch 68/256
3063/3063 - 33s - loss: 0.2440 - val_loss: 0.2852
Epoch 69/256
3063/3063 - 33s - loss: 0.2442 - val_loss: 0.2610
Epoch 70/256
3063/3063 - 34s - loss: 0.2429 - val_loss: 0.2675
Epoch 71/256
3063/3063 - 33s - loss: 0.2425 - val_loss: 0.2778
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 4)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 34s - loss: 0.4213 - val_loss: 0.3751
Epoch 2/256
3063/3063 - 33s - loss: 0.3729 - val_loss: 0.3643
Epoch 3/256
3063/3063 - 33s - loss: 0.3616 - val_loss: 0.3568
Epoch 4/256
3063/3063 - 33s - loss: 0.3554 - val_loss: 0.3507
Epoch 5/256
3063/3063 - 33s - loss: 0.3528 - val_loss: 0.3577
Epoch 6/256
3063/3063 - 33s - loss: 0.3504 - val_loss: 0.3701
Epoch 7/256
3063/3063 - 33s - loss: 0.3495 - val_loss: 0.3542
Epoch 8/256
3063/3063 - 33s - loss: 0.3464 - val_loss: 0.3461
Epoch 9/256
3063/3063 - 33s - loss: 0.3444 - val_loss: 0.3541
Epoch 10/256
3063/3063 - 33s - loss: 0.3425 - val_loss: 0.3452
Epoch 11/256
3063/3063 - 33s - loss: 0.3400 - val_loss: 0.3383
Epoch 12/256
3063/3063 - 33s - loss: 0.3381 - val_loss: 0.3354
Epoch 13/256
3063/3063 - 33s - loss: 0.3361 - val_loss: 0.3509
Epoch 14/256
3063/3063 - 33s - loss: 0.3341 - val_loss: 0.3309
Epoch 15/256
3063/3063 - 34s - loss: 0.3329 - val_loss: 0.3340
Epoch 16/256
3063/3063 - 33s - loss: 0.3317 - val_loss: 0.3439
Epoch 17/256
3063/3063 - 33s - loss: 0.3308 - val_loss: 0.3305
Epoch 18/256
3063/3063 - 33s - loss: 0.3294 - val_loss: 0.3284
Epoch 19/256
3063/3063 - 33s - loss: 0.3285 - val_loss: 0.3322
Epoch 20/256
3063/3063 - 33s - loss: 0.3268 - val_loss: 0.3412
Epoch 21/256
3063/3063 - 33s - loss: 0.3256 - val_loss: 0.3303
Epoch 22/256
3063/3063 - 33s - loss: 0.3243 - val_loss: 0.3280
Epoch 23/256
3063/3063 - 33s - loss: 0.3220 - val_loss: 0.3223
Epoch 24/256
3063/3063 - 33s - loss: 0.3198 - val_loss: 0.3234
Epoch 25/256
3063/3063 - 33s - loss: 0.3175 - val_loss: 0.3231
Epoch 26/256
3063/3063 - 33s - loss: 0.3155 - val_loss: 0.3250
Epoch 27/256
3063/3063 - 33s - loss: 0.3140 - val_loss: 0.3156
Epoch 28/256
3063/3063 - 33s - loss: 0.3107 - val_loss: 0.3114
Epoch 29/256
3063/3063 - 33s - loss: 0.3097 - val_loss: 0.3161
Epoch 30/256
3063/3063 - 32s - loss: 0.3085 - val_loss: 0.3208
Epoch 31/256
3063/3063 - 33s - loss: 0.3065 - val_loss: 0.3087
Epoch 32/256
3063/3063 - 32s - loss: 0.3054 - val_loss: 0.3096
Epoch 33/256
3063/3063 - 33s - loss: 0.3034 - val_loss: 0.3096
Epoch 34/256
3063/3063 - 33s - loss: 0.3022 - val_loss: 0.3065
Epoch 35/256
3063/3063 - 33s - loss: 0.3005 - val_loss: 0.3048
Epoch 36/256
3063/3063 - 33s - loss: 0.2992 - val_loss: 0.3114
Epoch 37/256
3063/3063 - 33s - loss: 0.2986 - val_loss: 0.3108
Epoch 38/256
3063/3063 - 33s - loss: 0.2977 - val_loss: 0.2998
Epoch 39/256
3063/3063 - 33s - loss: 0.2954 - val_loss: 0.3240
Epoch 40/256
3063/3063 - 33s - loss: 0.2945 - val_loss: 0.2968
Epoch 41/256
3063/3063 - 33s - loss: 0.2933 - val_loss: 0.3030
Epoch 42/256
3063/3063 - 33s - loss: 0.2915 - val_loss: 0.2972
Epoch 43/256
3063/3063 - 34s - loss: 0.2900 - val_loss: 0.2967
Epoch 44/256
3063/3063 - 33s - loss: 0.2885 - val_loss: 0.2953
Epoch 45/256
3063/3063 - 33s - loss: 0.2882 - val_loss: 0.3250
Epoch 46/256
3063/3063 - 33s - loss: 0.2858 - val_loss: 0.2993
Epoch 47/256
3063/3063 - 33s - loss: 0.2853 - val_loss: 0.2935
Epoch 48/256
3063/3063 - 33s - loss: 0.2826 - val_loss: 0.2927
Epoch 49/256
3063/3063 - 33s - loss: 0.2812 - val_loss: 0.3070
Epoch 50/256
3063/3063 - 33s - loss: 0.2807 - val_loss: 0.2963
Epoch 51/256
3063/3063 - 33s - loss: 0.2801 - val_loss: 0.2921
Epoch 52/256
3063/3063 - 33s - loss: 0.2777 - val_loss: 0.2860
Epoch 53/256
3063/3063 - 33s - loss: 0.2779 - val_loss: 0.3033
Epoch 54/256
3063/3063 - 33s - loss: 0.2756 - val_loss: 0.2906
Epoch 55/256
3063/3063 - 33s - loss: 0.2748 - val_loss: 0.2819
Epoch 56/256
3063/3063 - 33s - loss: 0.2739 - val_loss: 0.2865
Epoch 57/256
3063/3063 - 34s - loss: 0.2729 - val_loss: 0.2878
Epoch 58/256
3063/3063 - 33s - loss: 0.2721 - val_loss: 0.2789
Epoch 59/256
3063/3063 - 33s - loss: 0.2712 - val_loss: 0.2948
Epoch 60/256
3063/3063 - 33s - loss: 0.2704 - val_loss: 0.2796
Epoch 61/256
3063/3063 - 33s - loss: 0.2697 - val_loss: 0.2790
Epoch 62/256
3063/3063 - 33s - loss: 0.2685 - val_loss: 0.2817
Epoch 63/256
3063/3063 - 33s - loss: 0.2684 - val_loss: 0.2798
Epoch 64/256
3063/3063 - 33s - loss: 0.2677 - val_loss: 0.2987
Epoch 65/256
3063/3063 - 33s - loss: 0.2674 - val_loss: 0.2837
Epoch 66/256
3063/3063 - 33s - loss: 0.2656 - val_loss: 0.2789
Epoch 67/256
3063/3063 - 33s - loss: 0.2647 - val_loss: 0.2725
Epoch 68/256
3063/3063 - 33s - loss: 0.2642 - val_loss: 0.2824
Epoch 69/256
3063/3063 - 33s - loss: 0.2637 - val_loss: 0.2882
Epoch 70/256
3063/3063 - 33s - loss: 0.2628 - val_loss: 0.2817
Epoch 71/256
3063/3063 - 33s - loss: 0.2618 - val_loss: 0.2745
Epoch 72/256
3063/3063 - 33s - loss: 0.2608 - val_loss: 0.2756
Epoch 73/256
3063/3063 - 33s - loss: 0.2598 - val_loss: 0.2852
Epoch 74/256
3063/3063 - 33s - loss: 0.2608 - val_loss: 0.2884
Epoch 75/256
3063/3063 - 33s - loss: 0.2588 - val_loss: 0.2789
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 4)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 35s - loss: 0.4206 - val_loss: 0.3732
Epoch 2/256
3063/3063 - 33s - loss: 0.3699 - val_loss: 0.3638
Epoch 3/256
3063/3063 - 33s - loss: 0.3588 - val_loss: 0.3623
Epoch 4/256
3063/3063 - 33s - loss: 0.3552 - val_loss: 0.3533
Epoch 5/256
3063/3063 - 33s - loss: 0.3509 - val_loss: 0.3490
Epoch 6/256
3063/3063 - 33s - loss: 0.3487 - val_loss: 0.3460
Epoch 7/256
3063/3063 - 33s - loss: 0.3449 - val_loss: 0.3541
Epoch 8/256
3063/3063 - 33s - loss: 0.3435 - val_loss: 0.3441
Epoch 9/256
3063/3063 - 32s - loss: 0.3409 - val_loss: 0.3381
Epoch 10/256
3063/3063 - 33s - loss: 0.3385 - val_loss: 0.3414
Epoch 11/256
3063/3063 - 32s - loss: 0.3374 - val_loss: 0.3359
Epoch 12/256
3063/3063 - 33s - loss: 0.3368 - val_loss: 0.3339
Epoch 13/256
3063/3063 - 32s - loss: 0.3345 - val_loss: 0.3345
Epoch 14/256
3063/3063 - 33s - loss: 0.3329 - val_loss: 0.3304
Epoch 15/256
3063/3063 - 33s - loss: 0.3308 - val_loss: 0.3676
Epoch 16/256
3063/3063 - 33s - loss: 0.3291 - val_loss: 0.3425
Epoch 17/256
3063/3063 - 33s - loss: 0.3272 - val_loss: 0.3253
Epoch 18/256
3063/3063 - 34s - loss: 0.3254 - val_loss: 0.3212
Epoch 19/256
3063/3063 - 33s - loss: 0.3238 - val_loss: 0.3246
Epoch 20/256
3063/3063 - 34s - loss: 0.3211 - val_loss: 0.3270
Epoch 21/256
3063/3063 - 33s - loss: 0.3187 - val_loss: 0.3215
Epoch 22/256
3063/3063 - 34s - loss: 0.3159 - val_loss: 0.3357
Epoch 23/256
3063/3063 - 33s - loss: 0.3126 - val_loss: 0.3180
Epoch 24/256
3063/3063 - 34s - loss: 0.3109 - val_loss: 0.3127
Epoch 25/256
3063/3063 - 33s - loss: 0.3069 - val_loss: 0.3117
Epoch 26/256
3063/3063 - 34s - loss: 0.3049 - val_loss: 0.3047
Epoch 27/256
3063/3063 - 33s - loss: 0.3001 - val_loss: 0.3006
Epoch 28/256
3063/3063 - 34s - loss: 0.2972 - val_loss: 0.3032
Epoch 29/256
3063/3063 - 33s - loss: 0.2943 - val_loss: 0.2980
Epoch 30/256
3063/3063 - 34s - loss: 0.2918 - val_loss: 0.2962
Epoch 31/256
3063/3063 - 33s - loss: 0.2903 - val_loss: 0.3032
Epoch 32/256
3063/3063 - 34s - loss: 0.2877 - val_loss: 0.2980
Epoch 33/256
3063/3063 - 33s - loss: 0.2862 - val_loss: 0.2915
Epoch 34/256
3063/3063 - 33s - loss: 0.2842 - val_loss: 0.2835
Epoch 35/256
3063/3063 - 33s - loss: 0.2817 - val_loss: 0.2841
Epoch 36/256
3063/3063 - 34s - loss: 0.2811 - val_loss: 0.2863
Epoch 37/256
3063/3063 - 33s - loss: 0.2778 - val_loss: 0.2950
Epoch 38/256
3063/3063 - 33s - loss: 0.2781 - val_loss: 0.2830
Epoch 39/256
3063/3063 - 37s - loss: 0.2768 - val_loss: 0.2886
Epoch 40/256
3063/3063 - 33s - loss: 0.2751 - val_loss: 0.2788
Epoch 41/256
3063/3063 - 34s - loss: 0.2738 - val_loss: 0.2830
Epoch 42/256
3063/3063 - 33s - loss: 0.2729 - val_loss: 0.2830
Epoch 43/256
3063/3063 - 33s - loss: 0.2710 - val_loss: 0.2731
Epoch 44/256
3063/3063 - 33s - loss: 0.2705 - val_loss: 0.2750
Epoch 45/256
3063/3063 - 33s - loss: 0.2687 - val_loss: 0.2721
Epoch 46/256
3063/3063 - 34s - loss: 0.2677 - val_loss: 0.2797
Epoch 47/256
3063/3063 - 33s - loss: 0.2671 - val_loss: 0.2801
Epoch 48/256
3063/3063 - 34s - loss: 0.2662 - val_loss: 0.2721
Epoch 49/256
3063/3063 - 34s - loss: 0.2659 - val_loss: 0.2824
Epoch 50/256
3063/3063 - 33s - loss: 0.2650 - val_loss: 0.2725
Epoch 51/256
3063/3063 - 33s - loss: 0.2641 - val_loss: 0.2692
Epoch 52/256
3063/3063 - 34s - loss: 0.2635 - val_loss: 0.2729
Epoch 53/256
3063/3063 - 33s - loss: 0.2620 - val_loss: 0.2691
Epoch 54/256
3063/3063 - 34s - loss: 0.2614 - val_loss: 0.2680
Epoch 55/256
3063/3063 - 33s - loss: 0.2610 - val_loss: 0.2705
Epoch 56/256
3063/3063 - 33s - loss: 0.2597 - val_loss: 0.2716
Epoch 57/256
3063/3063 - 33s - loss: 0.2597 - val_loss: 0.2788
Epoch 58/256
3063/3063 - 33s - loss: 0.2586 - val_loss: 0.2693
Epoch 59/256
3063/3063 - 33s - loss: 0.2577 - val_loss: 0.2751
Epoch 60/256
3063/3063 - 33s - loss: 0.2565 - val_loss: 0.2669
Epoch 61/256
3063/3063 - 33s - loss: 0.2564 - val_loss: 0.2646
Epoch 62/256
3063/3063 - 33s - loss: 0.2561 - val_loss: 0.2762
Epoch 63/256
3063/3063 - 33s - loss: 0.2551 - val_loss: 0.2686
Epoch 64/256
3063/3063 - 33s - loss: 0.2537 - val_loss: 0.2786
Epoch 65/256
3063/3063 - 34s - loss: 0.2533 - val_loss: 0.2682
Epoch 66/256
3063/3063 - 33s - loss: 0.2531 - val_loss: 0.2797
Epoch 67/256
3063/3063 - 34s - loss: 0.2522 - val_loss: 0.2713
Epoch 68/256
3063/3063 - 33s - loss: 0.2509 - val_loss: 0.2607
Epoch 69/256
3063/3063 - 33s - loss: 0.2504 - val_loss: 0.2710
Epoch 70/256
3063/3063 - 33s - loss: 0.2498 - val_loss: 0.2658
Epoch 71/256
3063/3063 - 33s - loss: 0.2504 - val_loss: 0.2636
Epoch 72/256
3063/3063 - 34s - loss: 0.2481 - val_loss: 0.2643
Epoch 73/256
3063/3063 - 34s - loss: 0.2491 - val_loss: 0.2787
Epoch 74/256
3063/3063 - 34s - loss: 0.2486 - val_loss: 0.2614
Epoch 75/256
3063/3063 - 34s - loss: 0.2479 - val_loss: 0.2671
Epoch 76/256
3063/3063 - 34s - loss: 0.2468 - val_loss: 0.2672
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 4)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 34s - loss: 0.3982 - val_loss: 0.3597
Epoch 2/256
3063/3063 - 32s - loss: 0.3651 - val_loss: 0.3568
Epoch 3/256
3063/3063 - 33s - loss: 0.3538 - val_loss: 0.3438
Epoch 4/256
3063/3063 - 33s - loss: 0.3492 - val_loss: 0.3444
Epoch 5/256
3063/3063 - 33s - loss: 0.3431 - val_loss: 0.3417
Epoch 6/256
3063/3063 - 33s - loss: 0.3376 - val_loss: 0.3323
Epoch 7/256
3063/3063 - 33s - loss: 0.3332 - val_loss: 0.3281
Epoch 8/256
3063/3063 - 33s - loss: 0.3281 - val_loss: 0.3232
Epoch 9/256
3063/3063 - 33s - loss: 0.3218 - val_loss: 0.3191
Epoch 10/256
3063/3063 - 33s - loss: 0.3156 - val_loss: 0.3164
Epoch 11/256
3063/3063 - 33s - loss: 0.3121 - val_loss: 0.3141
Epoch 12/256
3063/3063 - 33s - loss: 0.3068 - val_loss: 0.3015
Epoch 13/256
3063/3063 - 33s - loss: 0.3038 - val_loss: 0.2993
Epoch 14/256
3063/3063 - 33s - loss: 0.3007 - val_loss: 0.2960
Epoch 15/256
3063/3063 - 33s - loss: 0.2982 - val_loss: 0.3058
Epoch 16/256
3063/3063 - 33s - loss: 0.2957 - val_loss: 0.3162
Epoch 17/256
3063/3063 - 33s - loss: 0.2939 - val_loss: 0.3085
Epoch 18/256
3063/3063 - 33s - loss: 0.2913 - val_loss: 0.3179
Epoch 19/256
3063/3063 - 33s - loss: 0.2900 - val_loss: 0.3023
Epoch 20/256
3063/3063 - 33s - loss: 0.2887 - val_loss: 0.3027
Epoch 21/256
3063/3063 - 33s - loss: 0.2861 - val_loss: 0.2941
Epoch 22/256
3063/3063 - 33s - loss: 0.2863 - val_loss: 0.2873
Epoch 23/256
3063/3063 - 33s - loss: 0.2844 - val_loss: 0.2980
Epoch 24/256
3063/3063 - 33s - loss: 0.2834 - val_loss: 0.2825
Epoch 25/256
3063/3063 - 33s - loss: 0.2811 - val_loss: 0.2957
Epoch 26/256
3063/3063 - 33s - loss: 0.2806 - val_loss: 0.2915
Epoch 27/256
3063/3063 - 33s - loss: 0.2784 - val_loss: 0.2822
Epoch 28/256
3063/3063 - 33s - loss: 0.2785 - val_loss: 0.2796
Epoch 29/256
3063/3063 - 33s - loss: 0.2765 - val_loss: 0.2926
Epoch 30/256
3063/3063 - 33s - loss: 0.2760 - val_loss: 0.2817
Epoch 31/256
3063/3063 - 33s - loss: 0.2737 - val_loss: 0.2839
Epoch 32/256
3063/3063 - 33s - loss: 0.2724 - val_loss: 0.2771
Epoch 33/256
3063/3063 - 33s - loss: 0.2719 - val_loss: 0.2762
Epoch 34/256
3063/3063 - 33s - loss: 0.2707 - val_loss: 0.2808
Epoch 35/256
3063/3063 - 33s - loss: 0.2694 - val_loss: 0.2806
Epoch 36/256
3063/3063 - 33s - loss: 0.2682 - val_loss: 0.2766
Epoch 37/256
3063/3063 - 33s - loss: 0.2661 - val_loss: 0.2765
Epoch 38/256
3063/3063 - 33s - loss: 0.2658 - val_loss: 0.2730
Epoch 39/256
3063/3063 - 34s - loss: 0.2651 - val_loss: 0.2710
Epoch 40/256
3063/3063 - 34s - loss: 0.2640 - val_loss: 0.2717
Epoch 41/256
3063/3063 - 33s - loss: 0.2644 - val_loss: 0.2772
Epoch 42/256
3063/3063 - 33s - loss: 0.2610 - val_loss: 0.2713
Epoch 43/256
3063/3063 - 33s - loss: 0.2607 - val_loss: 0.2722
Epoch 44/256
3063/3063 - 34s - loss: 0.2597 - val_loss: 0.2647
Epoch 45/256
3063/3063 - 33s - loss: 0.2582 - val_loss: 0.2686
Epoch 46/256
3063/3063 - 34s - loss: 0.2578 - val_loss: 0.2693
Epoch 47/256
3063/3063 - 33s - loss: 0.2567 - val_loss: 0.2738
Epoch 48/256
3063/3063 - 34s - loss: 0.2557 - val_loss: 0.2689
Epoch 49/256
3063/3063 - 33s - loss: 0.2541 - val_loss: 0.2709
Epoch 50/256
3063/3063 - 33s - loss: 0.2544 - val_loss: 0.2673
Epoch 51/256
3063/3063 - 33s - loss: 0.2537 - val_loss: 0.2633
Epoch 52/256
3063/3063 - 33s - loss: 0.2524 - val_loss: 0.2696
Epoch 53/256
3063/3063 - 33s - loss: 0.2514 - val_loss: 0.2716
Epoch 54/256
3063/3063 - 33s - loss: 0.2514 - val_loss: 0.2696
Epoch 55/256
3063/3063 - 33s - loss: 0.2496 - val_loss: 0.2582
Epoch 56/256
3063/3063 - 33s - loss: 0.2492 - val_loss: 0.2619
Epoch 57/256
3063/3063 - 33s - loss: 0.2480 - val_loss: 0.2688
Epoch 58/256
3063/3063 - 34s - loss: 0.2467 - val_loss: 0.2664
Epoch 59/256
3063/3063 - 34s - loss: 0.2468 - val_loss: 0.2609
Epoch 60/256
3063/3063 - 34s - loss: 0.2453 - val_loss: 0.2702
Epoch 61/256
3063/3063 - 33s - loss: 0.2450 - val_loss: 0.2610
Epoch 62/256
3063/3063 - 33s - loss: 0.2422 - val_loss: 0.2614
Epoch 63/256
3063/3063 - 33s - loss: 0.2441 - val_loss: 0.2548
Epoch 64/256
3063/3063 - 33s - loss: 0.2424 - val_loss: 0.2599
Epoch 65/256
3063/3063 - 33s - loss: 0.2415 - val_loss: 0.2655
Epoch 66/256
3063/3063 - 33s - loss: 0.2413 - val_loss: 0.2706
Epoch 67/256
3063/3063 - 33s - loss: 0.2410 - val_loss: 0.2580
Epoch 68/256
3063/3063 - 33s - loss: 0.2398 - val_loss: 0.2587
Epoch 69/256
3063/3063 - 34s - loss: 0.2389 - val_loss: 0.2605
Epoch 70/256
3063/3063 - 33s - loss: 0.2378 - val_loss: 0.2605
Epoch 71/256
3063/3063 - 34s - loss: 0.2375 - val_loss: 0.2571
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 4)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 35s - loss: 0.3950 - val_loss: 0.3570
Epoch 2/256
3063/3063 - 34s - loss: 0.3646 - val_loss: 0.3509
Epoch 3/256
3063/3063 - 33s - loss: 0.3556 - val_loss: 0.3468
Epoch 4/256
3063/3063 - 33s - loss: 0.3504 - val_loss: 0.3424
Epoch 5/256
3063/3063 - 33s - loss: 0.3470 - val_loss: 0.3375
Epoch 6/256
3063/3063 - 33s - loss: 0.3437 - val_loss: 0.3387
Epoch 7/256
3063/3063 - 33s - loss: 0.3410 - val_loss: 0.3483
Epoch 8/256
3063/3063 - 33s - loss: 0.3368 - val_loss: 0.3343
Epoch 9/256
3063/3063 - 33s - loss: 0.3324 - val_loss: 0.3274
Epoch 10/256
3063/3063 - 33s - loss: 0.3273 - val_loss: 0.3273
Epoch 11/256
3063/3063 - 33s - loss: 0.3221 - val_loss: 0.3154
Epoch 12/256
3063/3063 - 32s - loss: 0.3178 - val_loss: 0.3129
Epoch 13/256
3063/3063 - 33s - loss: 0.3129 - val_loss: 0.3109
Epoch 14/256
3063/3063 - 33s - loss: 0.3099 - val_loss: 0.3133
Epoch 15/256
3063/3063 - 33s - loss: 0.3054 - val_loss: 0.3022
Epoch 16/256
3063/3063 - 33s - loss: 0.3020 - val_loss: 0.3116
Epoch 17/256
3063/3063 - 32s - loss: 0.2978 - val_loss: 0.3125
Epoch 18/256
3063/3063 - 29s - loss: 0.2948 - val_loss: 0.2939
Epoch 19/256
3063/3063 - 29s - loss: 0.2903 - val_loss: 0.3235
Epoch 20/256
3063/3063 - 29s - loss: 0.2877 - val_loss: 0.2995
Epoch 21/256
3063/3063 - 29s - loss: 0.2845 - val_loss: 0.2934
Epoch 22/256
3063/3063 - 29s - loss: 0.2826 - val_loss: 0.2899
Epoch 23/256
3063/3063 - 29s - loss: 0.2808 - val_loss: 0.2866
Epoch 24/256
3063/3063 - 29s - loss: 0.2794 - val_loss: 0.2813
Epoch 25/256
3063/3063 - 29s - loss: 0.2766 - val_loss: 0.2782
Epoch 26/256
3063/3063 - 29s - loss: 0.2759 - val_loss: 0.2751
Epoch 27/256
3063/3063 - 29s - loss: 0.2746 - val_loss: 0.2982
Epoch 28/256
3063/3063 - 29s - loss: 0.2724 - val_loss: 0.2775
Epoch 29/256
3063/3063 - 29s - loss: 0.2715 - val_loss: 0.2752
Epoch 30/256
3063/3063 - 29s - loss: 0.2695 - val_loss: 0.2850
Epoch 31/256
3063/3063 - 29s - loss: 0.2681 - val_loss: 0.2816
Epoch 32/256
3063/3063 - 29s - loss: 0.2676 - val_loss: 0.2727
Epoch 33/256
3063/3063 - 29s - loss: 0.2665 - val_loss: 0.2757
Epoch 34/256
3063/3063 - 29s - loss: 0.2646 - val_loss: 0.2767
Epoch 35/256
3063/3063 - 29s - loss: 0.2642 - val_loss: 0.2781
Epoch 36/256
3063/3063 - 29s - loss: 0.2624 - val_loss: 0.2730
Epoch 37/256
3063/3063 - 29s - loss: 0.2621 - val_loss: 0.2828
Epoch 38/256
3063/3063 - 29s - loss: 0.2614 - val_loss: 0.2644
Epoch 39/256
3063/3063 - 29s - loss: 0.2605 - val_loss: 0.2830
Epoch 40/256
3063/3063 - 29s - loss: 0.2587 - val_loss: 0.2702
Epoch 41/256
3063/3063 - 29s - loss: 0.2588 - val_loss: 0.3102
Epoch 42/256
3063/3063 - 29s - loss: 0.2579 - val_loss: 0.2648
Epoch 43/256
3063/3063 - 29s - loss: 0.2569 - val_loss: 0.2754
Epoch 44/256
3063/3063 - 29s - loss: 0.2561 - val_loss: 0.2746
Epoch 45/256
3063/3063 - 29s - loss: 0.2553 - val_loss: 0.2824
Epoch 46/256
3063/3063 - 29s - loss: 0.2541 - val_loss: 0.2728
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 4)_64
			 4 [0.9576411027492151, 0.9583347570504264, 0.954462535634335, 0.9586679151835935, 0.9550764641857699, 0.9582287366089184, 0.9604444902246281, 0.9576491211579572]
		LATENT DIM 8
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 30s - loss: 0.3980 - val_loss: 0.3615
Epoch 2/256
3063/3063 - 29s - loss: 0.3625 - val_loss: 0.3537
Epoch 3/256
3063/3063 - 29s - loss: 0.3560 - val_loss: 0.3504
Epoch 4/256
3063/3063 - 29s - loss: 0.3510 - val_loss: 0.3548
Epoch 5/256
3063/3063 - 29s - loss: 0.3477 - val_loss: 0.3504
Epoch 6/256
3063/3063 - 29s - loss: 0.3447 - val_loss: 0.3427
Epoch 7/256
3063/3063 - 29s - loss: 0.3415 - val_loss: 0.3486
Epoch 8/256
3063/3063 - 29s - loss: 0.3388 - val_loss: 0.3380
Epoch 9/256
3063/3063 - 29s - loss: 0.3373 - val_loss: 0.3404
Epoch 10/256
3063/3063 - 29s - loss: 0.3342 - val_loss: 0.3371
Epoch 11/256
3063/3063 - 29s - loss: 0.3302 - val_loss: 0.3343
Epoch 12/256
3063/3063 - 29s - loss: 0.3250 - val_loss: 0.3297
Epoch 13/256
3063/3063 - 29s - loss: 0.3195 - val_loss: 0.3147
Epoch 14/256
3063/3063 - 29s - loss: 0.3131 - val_loss: 0.3167
Epoch 15/256
3063/3063 - 29s - loss: 0.3079 - val_loss: 0.3176
Epoch 16/256
3063/3063 - 29s - loss: 0.3035 - val_loss: 0.2980
Epoch 17/256
3063/3063 - 29s - loss: 0.2989 - val_loss: 0.3003
Epoch 18/256
3063/3063 - 29s - loss: 0.2955 - val_loss: 0.3005
Epoch 19/256
3063/3063 - 29s - loss: 0.2933 - val_loss: 0.3102
Epoch 20/256
3063/3063 - 29s - loss: 0.2901 - val_loss: 0.2877
Epoch 21/256
3063/3063 - 29s - loss: 0.2874 - val_loss: 0.2907
Epoch 22/256
3063/3063 - 29s - loss: 0.2846 - val_loss: 0.3243
Epoch 23/256
3063/3063 - 29s - loss: 0.2808 - val_loss: 0.2854
Epoch 24/256
3063/3063 - 29s - loss: 0.2785 - val_loss: 0.2811
Epoch 25/256
3063/3063 - 29s - loss: 0.2764 - val_loss: 0.2831
Epoch 26/256
3063/3063 - 29s - loss: 0.2731 - val_loss: 0.2765
Epoch 27/256
3063/3063 - 34s - loss: 0.2699 - val_loss: 0.2797
Epoch 28/256
3063/3063 - 29s - loss: 0.2680 - val_loss: 0.2717
Epoch 29/256
3063/3063 - 29s - loss: 0.2655 - val_loss: 0.2681
Epoch 30/256
3063/3063 - 29s - loss: 0.2631 - val_loss: 0.2854
Epoch 31/256
3063/3063 - 29s - loss: 0.2626 - val_loss: 0.2735
Epoch 32/256
3063/3063 - 29s - loss: 0.2611 - val_loss: 0.2766
Epoch 33/256
3063/3063 - 29s - loss: 0.2602 - val_loss: 0.2739
Epoch 34/256
3063/3063 - 29s - loss: 0.2572 - val_loss: 0.2749
Epoch 35/256
3063/3063 - 29s - loss: 0.2562 - val_loss: 0.2728
Epoch 36/256
3063/3063 - 29s - loss: 0.2549 - val_loss: 0.2714
Epoch 37/256
3063/3063 - 29s - loss: 0.2542 - val_loss: 0.2703
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 8)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.3955 - val_loss: 0.3730
Epoch 2/256
3063/3063 - 29s - loss: 0.3651 - val_loss: 0.3535
Epoch 3/256
3063/3063 - 29s - loss: 0.3566 - val_loss: 0.3581
Epoch 4/256
3063/3063 - 29s - loss: 0.3513 - val_loss: 0.3538
Epoch 5/256
3063/3063 - 29s - loss: 0.3482 - val_loss: 0.3550
Epoch 6/256
3063/3063 - 29s - loss: 0.3461 - val_loss: 0.3580
Epoch 7/256
3063/3063 - 34s - loss: 0.3441 - val_loss: 0.3464
Epoch 8/256
3063/3063 - 29s - loss: 0.3424 - val_loss: 0.3391
Epoch 9/256
3063/3063 - 29s - loss: 0.3401 - val_loss: 0.3433
Epoch 10/256
3063/3063 - 29s - loss: 0.3374 - val_loss: 0.3339
Epoch 11/256
3063/3063 - 29s - loss: 0.3365 - val_loss: 0.3348
Epoch 12/256
3063/3063 - 29s - loss: 0.3337 - val_loss: 0.3284
Epoch 13/256
3063/3063 - 29s - loss: 0.3291 - val_loss: 0.3235
Epoch 14/256
3063/3063 - 29s - loss: 0.3253 - val_loss: 0.3251
Epoch 15/256
3063/3063 - 29s - loss: 0.3200 - val_loss: 0.3220
Epoch 16/256
3063/3063 - 29s - loss: 0.3145 - val_loss: 0.3116
Epoch 17/256
3063/3063 - 29s - loss: 0.3101 - val_loss: 0.3106
Epoch 18/256
3063/3063 - 28s - loss: 0.3055 - val_loss: 0.3087
Epoch 19/256
3063/3063 - 29s - loss: 0.3018 - val_loss: 0.3123
Epoch 20/256
3063/3063 - 29s - loss: 0.2980 - val_loss: 0.3002
Epoch 21/256
3063/3063 - 29s - loss: 0.2943 - val_loss: 0.3161
Epoch 22/256
3063/3063 - 29s - loss: 0.2905 - val_loss: 0.2875
Epoch 23/256
3063/3063 - 29s - loss: 0.2869 - val_loss: 0.2848
Epoch 24/256
3063/3063 - 29s - loss: 0.2828 - val_loss: 0.2888
Epoch 25/256
3063/3063 - 29s - loss: 0.2796 - val_loss: 0.2799
Epoch 26/256
3063/3063 - 29s - loss: 0.2770 - val_loss: 0.2804
Epoch 27/256
3063/3063 - 29s - loss: 0.2756 - val_loss: 0.2804
Epoch 28/256
3063/3063 - 29s - loss: 0.2728 - val_loss: 0.2868
Epoch 29/256
3063/3063 - 29s - loss: 0.2707 - val_loss: 0.2722
Epoch 30/256
3063/3063 - 29s - loss: 0.2683 - val_loss: 0.2772
Epoch 31/256
3063/3063 - 29s - loss: 0.2669 - val_loss: 0.2792
Epoch 32/256
3063/3063 - 29s - loss: 0.2664 - val_loss: 0.2659
Epoch 33/256
3063/3063 - 29s - loss: 0.2656 - val_loss: 0.2898
Epoch 34/256
3063/3063 - 29s - loss: 0.2641 - val_loss: 0.2748
Epoch 35/256
3063/3063 - 29s - loss: 0.2623 - val_loss: 0.2693
Epoch 36/256
3063/3063 - 29s - loss: 0.2605 - val_loss: 0.2638
Epoch 37/256
3063/3063 - 29s - loss: 0.2592 - val_loss: 0.2699
Epoch 38/256
3063/3063 - 29s - loss: 0.2590 - val_loss: 0.2605
Epoch 39/256
3063/3063 - 29s - loss: 0.2579 - val_loss: 0.2689
Epoch 40/256
3063/3063 - 29s - loss: 0.2567 - val_loss: 0.2623
Epoch 41/256
3063/3063 - 29s - loss: 0.2550 - val_loss: 0.2780
Epoch 42/256
3063/3063 - 29s - loss: 0.2546 - val_loss: 0.2602
Epoch 43/256
3063/3063 - 29s - loss: 0.2528 - val_loss: 0.2717
Epoch 44/256
3063/3063 - 29s - loss: 0.2525 - val_loss: 0.2708
Epoch 45/256
3063/3063 - 29s - loss: 0.2520 - val_loss: 0.2683
Epoch 46/256
3063/3063 - 34s - loss: 0.2505 - val_loss: 0.2589
Epoch 47/256
3063/3063 - 29s - loss: 0.2501 - val_loss: 0.2645
Epoch 48/256
3063/3063 - 29s - loss: 0.2489 - val_loss: 0.2652
Epoch 49/256
3063/3063 - 29s - loss: 0.2479 - val_loss: 0.2608
Epoch 50/256
3063/3063 - 29s - loss: 0.2471 - val_loss: 0.2596
Epoch 51/256
3063/3063 - 29s - loss: 0.2466 - val_loss: 0.2676
Epoch 52/256
3063/3063 - 29s - loss: 0.2456 - val_loss: 0.2818
Epoch 53/256
3063/3063 - 29s - loss: 0.2451 - val_loss: 0.2567
Epoch 54/256
3063/3063 - 29s - loss: 0.2443 - val_loss: 0.2619
Epoch 55/256
3063/3063 - 29s - loss: 0.2431 - val_loss: 0.2623
Epoch 56/256
3063/3063 - 29s - loss: 0.2427 - val_loss: 0.2724
Epoch 57/256
3063/3063 - 29s - loss: 0.2423 - val_loss: 0.2561
Epoch 58/256
3063/3063 - 29s - loss: 0.2407 - val_loss: 0.2558
Epoch 59/256
3063/3063 - 29s - loss: 0.2411 - val_loss: 0.2538
Epoch 60/256
3063/3063 - 29s - loss: 0.2401 - val_loss: 0.2730
Epoch 61/256
3063/3063 - 29s - loss: 0.2392 - val_loss: 0.2622
Epoch 62/256
3063/3063 - 29s - loss: 0.2376 - val_loss: 0.2588
Epoch 63/256
3063/3063 - 29s - loss: 0.2375 - val_loss: 0.2583
Epoch 64/256
3063/3063 - 29s - loss: 0.2383 - val_loss: 0.2534
Epoch 65/256
3063/3063 - 29s - loss: 0.2358 - val_loss: 0.2562
Epoch 66/256
3063/3063 - 29s - loss: 0.2360 - val_loss: 0.2543
Epoch 67/256
3063/3063 - 29s - loss: 0.2354 - val_loss: 0.2653
Epoch 68/256
3063/3063 - 29s - loss: 0.2345 - val_loss: 0.2562
Epoch 69/256
3063/3063 - 29s - loss: 0.2342 - val_loss: 0.2542
Epoch 70/256
3063/3063 - 29s - loss: 0.2326 - val_loss: 0.2546
Epoch 71/256
3063/3063 - 29s - loss: 0.2317 - val_loss: 0.2550
Epoch 72/256
3063/3063 - 29s - loss: 0.2325 - val_loss: 0.2643
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 8)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 30s - loss: 0.3977 - val_loss: 0.3624
Epoch 2/256
3063/3063 - 29s - loss: 0.3619 - val_loss: 0.3528
Epoch 3/256
3063/3063 - 29s - loss: 0.3533 - val_loss: 0.3470
Epoch 4/256
3063/3063 - 29s - loss: 0.3492 - val_loss: 0.3451
Epoch 5/256
3063/3063 - 29s - loss: 0.3452 - val_loss: 0.3419
Epoch 6/256
3063/3063 - 29s - loss: 0.3426 - val_loss: 0.3472
Epoch 7/256
3063/3063 - 29s - loss: 0.3402 - val_loss: 0.3378
Epoch 8/256
3063/3063 - 29s - loss: 0.3371 - val_loss: 0.3337
Epoch 9/256
3063/3063 - 29s - loss: 0.3346 - val_loss: 0.3365
Epoch 10/256
3063/3063 - 29s - loss: 0.3308 - val_loss: 0.3299
Epoch 11/256
3063/3063 - 29s - loss: 0.3266 - val_loss: 0.3400
Epoch 12/256
3063/3063 - 29s - loss: 0.3212 - val_loss: 0.3280
Epoch 13/256
3063/3063 - 29s - loss: 0.3134 - val_loss: 0.3071
Epoch 14/256
3063/3063 - 29s - loss: 0.3064 - val_loss: 0.3223
Epoch 15/256
3063/3063 - 29s - loss: 0.3004 - val_loss: 0.3000
Epoch 16/256
3063/3063 - 29s - loss: 0.2948 - val_loss: 0.2893
Epoch 17/256
3063/3063 - 29s - loss: 0.2904 - val_loss: 0.2868
Epoch 18/256
3063/3063 - 29s - loss: 0.2882 - val_loss: 0.3037
Epoch 19/256
3063/3063 - 29s - loss: 0.2857 - val_loss: 0.2991
Epoch 20/256
3063/3063 - 29s - loss: 0.2825 - val_loss: 0.2938
Epoch 21/256
3063/3063 - 29s - loss: 0.2794 - val_loss: 0.2911
Epoch 22/256
3063/3063 - 29s - loss: 0.2764 - val_loss: 0.3005
Epoch 23/256
3063/3063 - 29s - loss: 0.2748 - val_loss: 0.2844
Epoch 24/256
3063/3063 - 29s - loss: 0.2731 - val_loss: 0.2734
Epoch 25/256
3063/3063 - 29s - loss: 0.2714 - val_loss: 0.2703
Epoch 26/256
3063/3063 - 29s - loss: 0.2692 - val_loss: 0.2792
Epoch 27/256
3063/3063 - 29s - loss: 0.2685 - val_loss: 0.2731
Epoch 28/256
3063/3063 - 34s - loss: 0.2655 - val_loss: 0.2672
Epoch 29/256
3063/3063 - 29s - loss: 0.2647 - val_loss: 0.2708
Epoch 30/256
3063/3063 - 29s - loss: 0.2631 - val_loss: 0.2670
Epoch 31/256
3063/3063 - 29s - loss: 0.2622 - val_loss: 0.2671
Epoch 32/256
3063/3063 - 29s - loss: 0.2603 - val_loss: 0.2883
Epoch 33/256
3063/3063 - 29s - loss: 0.2590 - val_loss: 0.2680
Epoch 34/256
3063/3063 - 29s - loss: 0.2580 - val_loss: 0.2670
Epoch 35/256
3063/3063 - 29s - loss: 0.2560 - val_loss: 0.2692
Epoch 36/256
3063/3063 - 29s - loss: 0.2556 - val_loss: 0.2660
Epoch 37/256
3063/3063 - 29s - loss: 0.2549 - val_loss: 0.2831
Epoch 38/256
3063/3063 - 29s - loss: 0.2527 - val_loss: 0.2578
Epoch 39/256
3063/3063 - 29s - loss: 0.2518 - val_loss: 0.2575
Epoch 40/256
3063/3063 - 29s - loss: 0.2509 - val_loss: 0.2548
Epoch 41/256
3063/3063 - 29s - loss: 0.2510 - val_loss: 0.2760
Epoch 42/256
3063/3063 - 29s - loss: 0.2487 - val_loss: 0.2716
Epoch 43/256
3063/3063 - 29s - loss: 0.2484 - val_loss: 0.2603
Epoch 44/256
3063/3063 - 29s - loss: 0.2465 - val_loss: 0.2569
Epoch 45/256
3063/3063 - 29s - loss: 0.2460 - val_loss: 0.2544
Epoch 46/256
3063/3063 - 29s - loss: 0.2453 - val_loss: 0.2556
Epoch 47/256
3063/3063 - 29s - loss: 0.2449 - val_loss: 0.2532
Epoch 48/256
3063/3063 - 29s - loss: 0.2432 - val_loss: 0.2532
Epoch 49/256
3063/3063 - 29s - loss: 0.2428 - val_loss: 0.2603
Epoch 50/256
3063/3063 - 29s - loss: 0.2423 - val_loss: 0.2532
Epoch 51/256
3063/3063 - 29s - loss: 0.2432 - val_loss: 0.2592
Epoch 52/256
3063/3063 - 29s - loss: 0.2412 - val_loss: 0.2538
Epoch 53/256
3063/3063 - 29s - loss: 0.2408 - val_loss: 0.2513
Epoch 54/256
3063/3063 - 29s - loss: 0.2396 - val_loss: 0.2685
Epoch 55/256
3063/3063 - 29s - loss: 0.2381 - val_loss: 0.2542
Epoch 56/256
3063/3063 - 29s - loss: 0.2382 - val_loss: 0.2513
Epoch 57/256
3063/3063 - 29s - loss: 0.2371 - val_loss: 0.2592
Epoch 58/256
3063/3063 - 29s - loss: 0.2366 - val_loss: 0.2542
Epoch 59/256
3063/3063 - 29s - loss: 0.2351 - val_loss: 0.2546
Epoch 60/256
3063/3063 - 29s - loss: 0.2352 - val_loss: 0.2532
Epoch 61/256
3063/3063 - 29s - loss: 0.2340 - val_loss: 0.2506
Epoch 62/256
3063/3063 - 29s - loss: 0.2335 - val_loss: 0.2577
Epoch 63/256
3063/3063 - 29s - loss: 0.2336 - val_loss: 0.2500
Epoch 64/256
3063/3063 - 29s - loss: 0.2322 - val_loss: 0.2704
Epoch 65/256
3063/3063 - 29s - loss: 0.2315 - val_loss: 0.2556
Epoch 66/256
3063/3063 - 29s - loss: 0.2314 - val_loss: 0.2896
Epoch 67/256
3063/3063 - 29s - loss: 0.2299 - val_loss: 0.2622
Epoch 68/256
3063/3063 - 29s - loss: 0.2304 - val_loss: 0.2492
Epoch 69/256
3063/3063 - 29s - loss: 0.2295 - val_loss: 0.2564
Epoch 70/256
3063/3063 - 29s - loss: 0.2288 - val_loss: 0.2582
Epoch 71/256
3063/3063 - 29s - loss: 0.2278 - val_loss: 0.2606
Epoch 72/256
3063/3063 - 29s - loss: 0.2269 - val_loss: 0.2527
Epoch 73/256
3063/3063 - 29s - loss: 0.2264 - val_loss: 0.2541
Epoch 74/256
3063/3063 - 29s - loss: 0.2257 - val_loss: 0.2499
Epoch 75/256
3063/3063 - 29s - loss: 0.2260 - val_loss: 0.2532
Epoch 76/256
3063/3063 - 29s - loss: 0.2256 - val_loss: 0.2603
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 8)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.3949 - val_loss: 0.3641
Epoch 2/256
3063/3063 - 29s - loss: 0.3625 - val_loss: 0.3503
Epoch 3/256
3063/3063 - 29s - loss: 0.3550 - val_loss: 0.3525
Epoch 4/256
3063/3063 - 29s - loss: 0.3496 - val_loss: 0.3612
Epoch 5/256
3063/3063 - 29s - loss: 0.3475 - val_loss: 0.3443
Epoch 6/256
3063/3063 - 29s - loss: 0.3431 - val_loss: 0.3371
Epoch 7/256
3063/3063 - 29s - loss: 0.3403 - val_loss: 0.3422
Epoch 8/256
3063/3063 - 29s - loss: 0.3374 - val_loss: 0.3371
Epoch 9/256
3063/3063 - 29s - loss: 0.3338 - val_loss: 0.3357
Epoch 10/256
3063/3063 - 29s - loss: 0.3302 - val_loss: 0.3328
Epoch 11/256
3063/3063 - 29s - loss: 0.3254 - val_loss: 0.3201
Epoch 12/256
3063/3063 - 29s - loss: 0.3167 - val_loss: 0.3086
Epoch 13/256
3063/3063 - 29s - loss: 0.3102 - val_loss: 0.3252
Epoch 14/256
3063/3063 - 29s - loss: 0.3049 - val_loss: 0.3011
Epoch 15/256
3063/3063 - 29s - loss: 0.2996 - val_loss: 0.3063
Epoch 16/256
3063/3063 - 29s - loss: 0.2945 - val_loss: 0.3079
Epoch 17/256
3063/3063 - 29s - loss: 0.2902 - val_loss: 0.3005
Epoch 18/256
3063/3063 - 29s - loss: 0.2854 - val_loss: 0.2799
Epoch 19/256
3063/3063 - 29s - loss: 0.2823 - val_loss: 0.2768
Epoch 20/256
3063/3063 - 29s - loss: 0.2791 - val_loss: 0.2889
Epoch 21/256
3063/3063 - 29s - loss: 0.2785 - val_loss: 0.2750
Epoch 22/256
3063/3063 - 29s - loss: 0.2740 - val_loss: 0.2798
Epoch 23/256
3063/3063 - 29s - loss: 0.2731 - val_loss: 0.2738
Epoch 24/256
3063/3063 - 29s - loss: 0.2703 - val_loss: 0.2890
Epoch 25/256
3063/3063 - 34s - loss: 0.2690 - val_loss: 0.2747
Epoch 26/256
3063/3063 - 29s - loss: 0.2666 - val_loss: 0.2696
Epoch 27/256
3063/3063 - 29s - loss: 0.2659 - val_loss: 0.2932
Epoch 28/256
3063/3063 - 29s - loss: 0.2649 - val_loss: 0.2762
Epoch 29/256
3063/3063 - 29s - loss: 0.2621 - val_loss: 0.2657
Epoch 30/256
3063/3063 - 29s - loss: 0.2620 - val_loss: 0.2679
Epoch 31/256
3063/3063 - 29s - loss: 0.2598 - val_loss: 0.2631
Epoch 32/256
3063/3063 - 29s - loss: 0.2592 - val_loss: 0.2715
Epoch 33/256
3063/3063 - 29s - loss: 0.2592 - val_loss: 0.2652
Epoch 34/256
3063/3063 - 29s - loss: 0.2563 - val_loss: 0.2664
Epoch 35/256
3063/3063 - 29s - loss: 0.2570 - val_loss: 0.2620
Epoch 36/256
3063/3063 - 34s - loss: 0.2544 - val_loss: 0.2635
Epoch 37/256
3063/3063 - 29s - loss: 0.2540 - val_loss: 0.2614
Epoch 38/256
3063/3063 - 29s - loss: 0.2534 - val_loss: 0.2720
Epoch 39/256
3063/3063 - 29s - loss: 0.2525 - val_loss: 0.2664
Epoch 40/256
3063/3063 - 29s - loss: 0.2514 - val_loss: 0.2584
Epoch 41/256
3063/3063 - 29s - loss: 0.2514 - val_loss: 0.2770
Epoch 42/256
3063/3063 - 29s - loss: 0.2496 - val_loss: 0.2595
Epoch 43/256
3063/3063 - 29s - loss: 0.2496 - val_loss: 0.2574
Epoch 44/256
3063/3063 - 29s - loss: 0.2483 - val_loss: 0.2664
Epoch 45/256
3063/3063 - 29s - loss: 0.2476 - val_loss: 0.2628
Epoch 46/256
3063/3063 - 29s - loss: 0.2464 - val_loss: 0.2563
Epoch 47/256
3063/3063 - 29s - loss: 0.2468 - val_loss: 0.2794
Epoch 48/256
3063/3063 - 29s - loss: 0.2455 - val_loss: 0.2618
Epoch 49/256
3063/3063 - 29s - loss: 0.2450 - val_loss: 0.2549
Epoch 50/256
3063/3063 - 29s - loss: 0.2437 - val_loss: 0.2690
Epoch 51/256
3063/3063 - 29s - loss: 0.2427 - val_loss: 0.2588
Epoch 52/256
3063/3063 - 29s - loss: 0.2426 - val_loss: 0.2534
Epoch 53/256
3063/3063 - 29s - loss: 0.2419 - val_loss: 0.2621
Epoch 54/256
3063/3063 - 29s - loss: 0.2406 - val_loss: 0.2551
Epoch 55/256
3063/3063 - 29s - loss: 0.2412 - val_loss: 0.2539
Epoch 56/256
3063/3063 - 29s - loss: 0.2400 - val_loss: 0.2555
Epoch 57/256
3063/3063 - 29s - loss: 0.2407 - val_loss: 0.2601
Epoch 58/256
3063/3063 - 29s - loss: 0.2386 - val_loss: 0.2576
Epoch 59/256
3063/3063 - 29s - loss: 0.2386 - val_loss: 0.2569
Epoch 60/256
3063/3063 - 29s - loss: 0.2365 - val_loss: 0.2687
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 8)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.3946 - val_loss: 0.3625
Epoch 2/256
3063/3063 - 29s - loss: 0.3618 - val_loss: 0.3501
Epoch 3/256
3063/3063 - 29s - loss: 0.3540 - val_loss: 0.3503
Epoch 4/256
3063/3063 - 29s - loss: 0.3495 - val_loss: 0.3446
Epoch 5/256
3063/3063 - 29s - loss: 0.3469 - val_loss: 0.3467
Epoch 6/256
3063/3063 - 29s - loss: 0.3447 - val_loss: 0.3711
Epoch 7/256
3063/3063 - 29s - loss: 0.3440 - val_loss: 0.3498
Epoch 8/256
3063/3063 - 29s - loss: 0.3410 - val_loss: 0.3407
Epoch 9/256
3063/3063 - 29s - loss: 0.3395 - val_loss: 0.3401
Epoch 10/256
3063/3063 - 29s - loss: 0.3380 - val_loss: 0.3458
Epoch 11/256
3063/3063 - 29s - loss: 0.3360 - val_loss: 0.3420
Epoch 12/256
3063/3063 - 29s - loss: 0.3351 - val_loss: 0.3373
Epoch 13/256
3063/3063 - 29s - loss: 0.3335 - val_loss: 0.3358
Epoch 14/256
3063/3063 - 29s - loss: 0.3305 - val_loss: 0.3291
Epoch 15/256
3063/3063 - 29s - loss: 0.3260 - val_loss: 0.3271
Epoch 16/256
3063/3063 - 29s - loss: 0.3201 - val_loss: 0.3232
Epoch 17/256
3063/3063 - 29s - loss: 0.3136 - val_loss: 0.3102
Epoch 18/256
3063/3063 - 29s - loss: 0.3086 - val_loss: 0.3063
Epoch 19/256
3063/3063 - 29s - loss: 0.3042 - val_loss: 0.3198
Epoch 20/256
3063/3063 - 29s - loss: 0.3012 - val_loss: 0.3060
Epoch 21/256
3063/3063 - 29s - loss: 0.2975 - val_loss: 0.2989
Epoch 22/256
3063/3063 - 29s - loss: 0.2935 - val_loss: 0.2941
Epoch 23/256
3063/3063 - 29s - loss: 0.2907 - val_loss: 0.2931
Epoch 24/256
3063/3063 - 29s - loss: 0.2886 - val_loss: 0.3067
Epoch 25/256
3063/3063 - 29s - loss: 0.2865 - val_loss: 0.2919
Epoch 26/256
3063/3063 - 29s - loss: 0.2840 - val_loss: 0.2995
Epoch 27/256
3063/3063 - 29s - loss: 0.2823 - val_loss: 0.2904
Epoch 28/256
3063/3063 - 29s - loss: 0.2784 - val_loss: 0.2846
Epoch 29/256
3063/3063 - 29s - loss: 0.2765 - val_loss: 0.2808
Epoch 30/256
3063/3063 - 29s - loss: 0.2746 - val_loss: 0.2748
Epoch 31/256
3063/3063 - 29s - loss: 0.2741 - val_loss: 0.2811
Epoch 32/256
3063/3063 - 29s - loss: 0.2718 - val_loss: 0.2780
Epoch 33/256
3063/3063 - 29s - loss: 0.2703 - val_loss: 0.2751
Epoch 34/256
3063/3063 - 29s - loss: 0.2691 - val_loss: 0.2789
Epoch 35/256
3063/3063 - 29s - loss: 0.2675 - val_loss: 0.2764
Epoch 36/256
3063/3063 - 29s - loss: 0.2669 - val_loss: 0.2735
Epoch 37/256
3063/3063 - 29s - loss: 0.2647 - val_loss: 0.2802
Epoch 38/256
3063/3063 - 29s - loss: 0.2639 - val_loss: 0.2673
Epoch 39/256
3063/3063 - 29s - loss: 0.2630 - val_loss: 0.2907
Epoch 40/256
3063/3063 - 29s - loss: 0.2618 - val_loss: 0.2680
Epoch 41/256
3063/3063 - 29s - loss: 0.2613 - val_loss: 0.2721
Epoch 42/256
3063/3063 - 29s - loss: 0.2596 - val_loss: 0.2685
Epoch 43/256
3063/3063 - 29s - loss: 0.2586 - val_loss: 0.2700
Epoch 44/256
3063/3063 - 29s - loss: 0.2579 - val_loss: 0.2944
Epoch 45/256
3063/3063 - 29s - loss: 0.2581 - val_loss: 0.3007
Epoch 46/256
3063/3063 - 29s - loss: 0.2565 - val_loss: 0.2827
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 8)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 30s - loss: 0.4002 - val_loss: 0.3631
Epoch 2/256
3063/3063 - 29s - loss: 0.3656 - val_loss: 0.3574
Epoch 3/256
3063/3063 - 29s - loss: 0.3548 - val_loss: 0.3503
Epoch 4/256
3063/3063 - 29s - loss: 0.3506 - val_loss: 0.3463
Epoch 5/256
3063/3063 - 29s - loss: 0.3462 - val_loss: 0.3498
Epoch 6/256
3063/3063 - 29s - loss: 0.3445 - val_loss: 0.3450
Epoch 7/256
3063/3063 - 29s - loss: 0.3412 - val_loss: 0.3486
Epoch 8/256
3063/3063 - 29s - loss: 0.3381 - val_loss: 0.3440
Epoch 9/256
3063/3063 - 29s - loss: 0.3361 - val_loss: 0.3314
Epoch 10/256
3063/3063 - 29s - loss: 0.3328 - val_loss: 0.3412
Epoch 11/256
3063/3063 - 29s - loss: 0.3300 - val_loss: 0.3423
Epoch 12/256
3063/3063 - 29s - loss: 0.3237 - val_loss: 0.3195
Epoch 13/256
3063/3063 - 29s - loss: 0.3153 - val_loss: 0.3093
Epoch 14/256
3063/3063 - 29s - loss: 0.3093 - val_loss: 0.3069
Epoch 15/256
3063/3063 - 29s - loss: 0.3003 - val_loss: 0.3059
Epoch 16/256
3063/3063 - 29s - loss: 0.2946 - val_loss: 0.2958
Epoch 17/256
3063/3063 - 29s - loss: 0.2897 - val_loss: 0.2871
Epoch 18/256
3063/3063 - 29s - loss: 0.2870 - val_loss: 0.2810
Epoch 19/256
3063/3063 - 29s - loss: 0.2828 - val_loss: 0.2840
Epoch 20/256
3063/3063 - 29s - loss: 0.2798 - val_loss: 0.2826
Epoch 21/256
3063/3063 - 29s - loss: 0.2767 - val_loss: 0.2737
Epoch 22/256
3063/3063 - 29s - loss: 0.2744 - val_loss: 0.2872
Epoch 23/256
3063/3063 - 29s - loss: 0.2722 - val_loss: 0.2750
Epoch 24/256
3063/3063 - 29s - loss: 0.2714 - val_loss: 0.2756
Epoch 25/256
3063/3063 - 29s - loss: 0.2677 - val_loss: 0.2782
Epoch 26/256
3063/3063 - 29s - loss: 0.2679 - val_loss: 0.2699
Epoch 27/256
3063/3063 - 29s - loss: 0.2657 - val_loss: 0.2681
Epoch 28/256
3063/3063 - 29s - loss: 0.2645 - val_loss: 0.2650
Epoch 29/256
3063/3063 - 29s - loss: 0.2627 - val_loss: 0.2666
Epoch 30/256
3063/3063 - 29s - loss: 0.2610 - val_loss: 0.2671
Epoch 31/256
3063/3063 - 29s - loss: 0.2598 - val_loss: 0.2815
Epoch 32/256
3063/3063 - 29s - loss: 0.2584 - val_loss: 0.2691
Epoch 33/256
3063/3063 - 29s - loss: 0.2573 - val_loss: 0.2705
Epoch 34/256
3063/3063 - 29s - loss: 0.2575 - val_loss: 0.2622
Epoch 35/256
3063/3063 - 29s - loss: 0.2555 - val_loss: 0.2727
Epoch 36/256
3063/3063 - 29s - loss: 0.2552 - val_loss: 0.2650
Epoch 37/256
3063/3063 - 29s - loss: 0.2529 - val_loss: 0.2672
Epoch 38/256
3063/3063 - 29s - loss: 0.2524 - val_loss: 0.2620
Epoch 39/256
3063/3063 - 29s - loss: 0.2516 - val_loss: 0.2693
Epoch 40/256
3063/3063 - 29s - loss: 0.2508 - val_loss: 0.2614
Epoch 41/256
3063/3063 - 29s - loss: 0.2495 - val_loss: 0.2649
Epoch 42/256
3063/3063 - 29s - loss: 0.2487 - val_loss: 0.2556
Epoch 43/256
3063/3063 - 29s - loss: 0.2474 - val_loss: 0.2663
Epoch 44/256
3063/3063 - 29s - loss: 0.2479 - val_loss: 0.2788
Epoch 45/256
3063/3063 - 29s - loss: 0.2462 - val_loss: 0.2586
Epoch 46/256
3063/3063 - 29s - loss: 0.2447 - val_loss: 0.2740
Epoch 47/256
3063/3063 - 29s - loss: 0.2446 - val_loss: 0.2594
Epoch 48/256
3063/3063 - 29s - loss: 0.2428 - val_loss: 0.2570
Epoch 49/256
3063/3063 - 29s - loss: 0.2423 - val_loss: 0.2605
Epoch 50/256
3063/3063 - 29s - loss: 0.2411 - val_loss: 0.2644
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 8)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 30s - loss: 0.4024 - val_loss: 0.3608
Epoch 2/256
3063/3063 - 29s - loss: 0.3651 - val_loss: 0.3607
Epoch 3/256
3063/3063 - 29s - loss: 0.3540 - val_loss: 0.3443
Epoch 4/256
3063/3063 - 29s - loss: 0.3491 - val_loss: 0.3484
Epoch 5/256
3063/3063 - 29s - loss: 0.3448 - val_loss: 0.3452
Epoch 6/256
3063/3063 - 29s - loss: 0.3406 - val_loss: 0.3340
Epoch 7/256
3063/3063 - 29s - loss: 0.3380 - val_loss: 0.3313
Epoch 8/256
3063/3063 - 29s - loss: 0.3332 - val_loss: 0.3260
Epoch 9/256
3063/3063 - 29s - loss: 0.3260 - val_loss: 0.3326
Epoch 10/256
3063/3063 - 29s - loss: 0.3179 - val_loss: 0.3151
Epoch 11/256
3063/3063 - 29s - loss: 0.3122 - val_loss: 0.3071
Epoch 12/256
3063/3063 - 29s - loss: 0.3062 - val_loss: 0.3028
Epoch 13/256
3063/3063 - 29s - loss: 0.3028 - val_loss: 0.3025
Epoch 14/256
3063/3063 - 29s - loss: 0.2987 - val_loss: 0.2984
Epoch 15/256
3063/3063 - 29s - loss: 0.2945 - val_loss: 0.2948
Epoch 16/256
3063/3063 - 29s - loss: 0.2899 - val_loss: 0.3198
Epoch 17/256
3063/3063 - 29s - loss: 0.2869 - val_loss: 0.2942
Epoch 18/256
3063/3063 - 29s - loss: 0.2840 - val_loss: 0.2968
Epoch 19/256
3063/3063 - 29s - loss: 0.2820 - val_loss: 0.2830
Epoch 20/256
3063/3063 - 29s - loss: 0.2792 - val_loss: 0.3050
Epoch 21/256
3063/3063 - 29s - loss: 0.2755 - val_loss: 0.2889
Epoch 22/256
3063/3063 - 29s - loss: 0.2750 - val_loss: 0.2766
Epoch 23/256
3063/3063 - 29s - loss: 0.2726 - val_loss: 0.2879
Epoch 24/256
3063/3063 - 29s - loss: 0.2711 - val_loss: 0.2732
Epoch 25/256
3063/3063 - 29s - loss: 0.2685 - val_loss: 0.2939
Epoch 26/256
3063/3063 - 29s - loss: 0.2672 - val_loss: 0.2714
Epoch 27/256
3063/3063 - 29s - loss: 0.2640 - val_loss: 0.2648
Epoch 28/256
3063/3063 - 29s - loss: 0.2647 - val_loss: 0.2734
Epoch 29/256
3063/3063 - 29s - loss: 0.2627 - val_loss: 0.2850
Epoch 30/256
3063/3063 - 29s - loss: 0.2622 - val_loss: 0.2719
Epoch 31/256
3063/3063 - 29s - loss: 0.2597 - val_loss: 0.2679
Epoch 32/256
3063/3063 - 29s - loss: 0.2588 - val_loss: 0.2685
Epoch 33/256
3063/3063 - 29s - loss: 0.2582 - val_loss: 0.2678
Epoch 34/256
3063/3063 - 29s - loss: 0.2561 - val_loss: 0.2639
Epoch 35/256
3063/3063 - 29s - loss: 0.2562 - val_loss: 0.2649
Epoch 36/256
3063/3063 - 29s - loss: 0.2542 - val_loss: 0.2604
Epoch 37/256
3063/3063 - 29s - loss: 0.2531 - val_loss: 0.2595
Epoch 38/256
3063/3063 - 29s - loss: 0.2527 - val_loss: 0.2733
Epoch 39/256
3063/3063 - 29s - loss: 0.2510 - val_loss: 0.2620
Epoch 40/256
3063/3063 - 29s - loss: 0.2516 - val_loss: 0.2608
Epoch 41/256
3063/3063 - 29s - loss: 0.2507 - val_loss: 0.2655
Epoch 42/256
3063/3063 - 29s - loss: 0.2481 - val_loss: 0.2592
Epoch 43/256
3063/3063 - 29s - loss: 0.2485 - val_loss: 0.2618
Epoch 44/256
3063/3063 - 29s - loss: 0.2473 - val_loss: 0.2605
Epoch 45/256
3063/3063 - 29s - loss: 0.2463 - val_loss: 0.2649
Epoch 46/256
3063/3063 - 29s - loss: 0.2460 - val_loss: 0.2610
Epoch 47/256
3063/3063 - 29s - loss: 0.2446 - val_loss: 0.2699
Epoch 48/256
3063/3063 - 29s - loss: 0.2433 - val_loss: 0.2601
Epoch 49/256
3063/3063 - 29s - loss: 0.2430 - val_loss: 0.2715
Epoch 50/256
3063/3063 - 29s - loss: 0.2431 - val_loss: 0.2549
Epoch 51/256
3063/3063 - 29s - loss: 0.2411 - val_loss: 0.2579
Epoch 52/256
3063/3063 - 29s - loss: 0.2409 - val_loss: 0.2796
Epoch 53/256
3063/3063 - 29s - loss: 0.2403 - val_loss: 0.2574
Epoch 54/256
3063/3063 - 29s - loss: 0.2402 - val_loss: 0.2611
Epoch 55/256
3063/3063 - 29s - loss: 0.2392 - val_loss: 0.2551
Epoch 56/256
3063/3063 - 29s - loss: 0.2380 - val_loss: 0.2583
Epoch 57/256
3063/3063 - 29s - loss: 0.2382 - val_loss: 0.2583
Epoch 58/256
3063/3063 - 29s - loss: 0.2373 - val_loss: 0.2730
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 8)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.3986 - val_loss: 0.3572
Epoch 2/256
3063/3063 - 29s - loss: 0.3661 - val_loss: 0.3506
Epoch 3/256
3063/3063 - 29s - loss: 0.3570 - val_loss: 0.3523
Epoch 4/256
3063/3063 - 29s - loss: 0.3511 - val_loss: 0.3455
Epoch 5/256
3063/3063 - 29s - loss: 0.3475 - val_loss: 0.3398
Epoch 6/256
3063/3063 - 29s - loss: 0.3436 - val_loss: 0.3405
Epoch 7/256
3063/3063 - 29s - loss: 0.3412 - val_loss: 0.3493
Epoch 8/256
3063/3063 - 29s - loss: 0.3370 - val_loss: 0.3334
Epoch 9/256
3063/3063 - 29s - loss: 0.3348 - val_loss: 0.3321
Epoch 10/256
3063/3063 - 29s - loss: 0.3314 - val_loss: 0.3267
Epoch 11/256
3063/3063 - 29s - loss: 0.3265 - val_loss: 0.3166
Epoch 12/256
3063/3063 - 29s - loss: 0.3209 - val_loss: 0.3215
Epoch 13/256
3063/3063 - 29s - loss: 0.3141 - val_loss: 0.3110
Epoch 14/256
3063/3063 - 29s - loss: 0.3091 - val_loss: 0.3035
Epoch 15/256
3063/3063 - 29s - loss: 0.3033 - val_loss: 0.2965
Epoch 16/256
3063/3063 - 29s - loss: 0.2982 - val_loss: 0.3112
Epoch 17/256
3063/3063 - 29s - loss: 0.2946 - val_loss: 0.3126
Epoch 18/256
3063/3063 - 29s - loss: 0.2905 - val_loss: 0.2813
Epoch 19/256
3063/3063 - 29s - loss: 0.2860 - val_loss: 0.3098
Epoch 20/256
3063/3063 - 29s - loss: 0.2845 - val_loss: 0.2917
Epoch 21/256
3063/3063 - 29s - loss: 0.2799 - val_loss: 0.2813
Epoch 22/256
3063/3063 - 29s - loss: 0.2788 - val_loss: 0.2932
Epoch 23/256
3063/3063 - 29s - loss: 0.2771 - val_loss: 0.3040
Epoch 24/256
3063/3063 - 29s - loss: 0.2746 - val_loss: 0.2749
Epoch 25/256
3063/3063 - 29s - loss: 0.2726 - val_loss: 0.2705
Epoch 26/256
3063/3063 - 29s - loss: 0.2715 - val_loss: 0.2755
Epoch 27/256
3063/3063 - 29s - loss: 0.2699 - val_loss: 0.2927
Epoch 28/256
3063/3063 - 29s - loss: 0.2685 - val_loss: 0.2804
Epoch 29/256
3063/3063 - 29s - loss: 0.2677 - val_loss: 0.2775
Epoch 30/256
3063/3063 - 29s - loss: 0.2651 - val_loss: 0.2884
Epoch 31/256
3063/3063 - 29s - loss: 0.2644 - val_loss: 0.2753
Epoch 32/256
3063/3063 - 29s - loss: 0.2630 - val_loss: 0.2702
Epoch 33/256
3063/3063 - 29s - loss: 0.2621 - val_loss: 0.2822
Epoch 34/256
3063/3063 - 29s - loss: 0.2607 - val_loss: 0.2753
Epoch 35/256
3063/3063 - 29s - loss: 0.2602 - val_loss: 0.2755
Epoch 36/256
3063/3063 - 29s - loss: 0.2580 - val_loss: 0.2733
Epoch 37/256
3063/3063 - 29s - loss: 0.2577 - val_loss: 0.2741
Epoch 38/256
3063/3063 - 29s - loss: 0.2567 - val_loss: 0.2623
Epoch 39/256
3063/3063 - 29s - loss: 0.2555 - val_loss: 0.2918
Epoch 40/256
3063/3063 - 29s - loss: 0.2547 - val_loss: 0.2649
Epoch 41/256
3063/3063 - 29s - loss: 0.2542 - val_loss: 0.2860
Epoch 42/256
3063/3063 - 29s - loss: 0.2529 - val_loss: 0.2635
Epoch 43/256
3063/3063 - 29s - loss: 0.2519 - val_loss: 0.2761
Epoch 44/256
3063/3063 - 29s - loss: 0.2513 - val_loss: 0.2722
Epoch 45/256
3063/3063 - 29s - loss: 0.2512 - val_loss: 0.2663
Epoch 46/256
3063/3063 - 29s - loss: 0.2493 - val_loss: 0.2784
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 8)_64
			 8 [0.956466508925035, 0.9609606323008799, 0.9623319435306334, 0.9610633566002781, 0.9567375798076394, 0.9601488146661858, 0.9607720883055061, 0.9586842792263771]
		LATENT DIM 16
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.4016 - val_loss: 0.3629
Epoch 2/256
3063/3063 - 29s - loss: 0.3616 - val_loss: 0.3721
Epoch 3/256
3063/3063 - 29s - loss: 0.3531 - val_loss: 0.3454
Epoch 4/256
3063/3063 - 29s - loss: 0.3480 - val_loss: 0.3589
Epoch 5/256
3063/3063 - 29s - loss: 0.3451 - val_loss: 0.3415
Epoch 6/256
3063/3063 - 29s - loss: 0.3415 - val_loss: 0.3371
Epoch 7/256
3063/3063 - 34s - loss: 0.3390 - val_loss: 0.3441
Epoch 8/256
3063/3063 - 29s - loss: 0.3356 - val_loss: 0.3332
Epoch 9/256
3063/3063 - 29s - loss: 0.3342 - val_loss: 0.3330
Epoch 10/256
3063/3063 - 29s - loss: 0.3299 - val_loss: 0.3355
Epoch 11/256
3063/3063 - 29s - loss: 0.3245 - val_loss: 0.3251
Epoch 12/256
3063/3063 - 34s - loss: 0.3183 - val_loss: 0.3125
Epoch 13/256
3063/3063 - 29s - loss: 0.3116 - val_loss: 0.3035
Epoch 14/256
3063/3063 - 29s - loss: 0.3036 - val_loss: 0.3163
Epoch 15/256
3063/3063 - 29s - loss: 0.2977 - val_loss: 0.3001
Epoch 16/256
3063/3063 - 29s - loss: 0.2921 - val_loss: 0.2858
Epoch 17/256
3063/3063 - 29s - loss: 0.2874 - val_loss: 0.2810
Epoch 18/256
3063/3063 - 29s - loss: 0.2812 - val_loss: 0.3005
Epoch 19/256
3063/3063 - 29s - loss: 0.2782 - val_loss: 0.2927
Epoch 20/256
3063/3063 - 29s - loss: 0.2759 - val_loss: 0.2750
Epoch 21/256
3063/3063 - 29s - loss: 0.2732 - val_loss: 0.2856
Epoch 22/256
3063/3063 - 29s - loss: 0.2702 - val_loss: 0.3344
Epoch 23/256
3063/3063 - 29s - loss: 0.2684 - val_loss: 0.2712
Epoch 24/256
3063/3063 - 29s - loss: 0.2661 - val_loss: 0.2871
Epoch 25/256
3063/3063 - 29s - loss: 0.2651 - val_loss: 0.2704
Epoch 26/256
3063/3063 - 29s - loss: 0.2629 - val_loss: 0.2667
Epoch 27/256
3063/3063 - 29s - loss: 0.2610 - val_loss: 0.2835
Epoch 28/256
3063/3063 - 29s - loss: 0.2598 - val_loss: 0.2636
Epoch 29/256
3063/3063 - 29s - loss: 0.2585 - val_loss: 0.2651
Epoch 30/256
3063/3063 - 29s - loss: 0.2563 - val_loss: 0.2671
Epoch 31/256
3063/3063 - 29s - loss: 0.2550 - val_loss: 0.2696
Epoch 32/256
3063/3063 - 29s - loss: 0.2542 - val_loss: 0.2712
Epoch 33/256
3063/3063 - 29s - loss: 0.2535 - val_loss: 0.2590
Epoch 34/256
3063/3063 - 29s - loss: 0.2505 - val_loss: 0.2632
Epoch 35/256
3063/3063 - 29s - loss: 0.2495 - val_loss: 0.2627
Epoch 36/256
3063/3063 - 29s - loss: 0.2483 - val_loss: 0.2605
Epoch 37/256
3063/3063 - 29s - loss: 0.2468 - val_loss: 0.2645
Epoch 38/256
3063/3063 - 29s - loss: 0.2465 - val_loss: 0.2726
Epoch 39/256
3063/3063 - 29s - loss: 0.2439 - val_loss: 0.2775
Epoch 40/256
3063/3063 - 29s - loss: 0.2430 - val_loss: 0.2588
Epoch 41/256
3063/3063 - 29s - loss: 0.2429 - val_loss: 0.2589
Epoch 42/256
3063/3063 - 29s - loss: 0.2428 - val_loss: 0.2585
Epoch 43/256
3063/3063 - 29s - loss: 0.2412 - val_loss: 0.2529
Epoch 44/256
3063/3063 - 29s - loss: 0.2397 - val_loss: 0.2554
Epoch 45/256
3063/3063 - 29s - loss: 0.2392 - val_loss: 0.2556
Epoch 46/256
3063/3063 - 29s - loss: 0.2380 - val_loss: 0.2647
Epoch 47/256
3063/3063 - 29s - loss: 0.2379 - val_loss: 0.2736
Epoch 48/256
3063/3063 - 29s - loss: 0.2367 - val_loss: 0.2588
Epoch 49/256
3063/3063 - 29s - loss: 0.2364 - val_loss: 0.2504
Epoch 50/256
3063/3063 - 29s - loss: 0.2345 - val_loss: 0.2529
Epoch 51/256
3063/3063 - 29s - loss: 0.2344 - val_loss: 0.2644
Epoch 52/256
3063/3063 - 29s - loss: 0.2330 - val_loss: 0.2684
Epoch 53/256
3063/3063 - 29s - loss: 0.2314 - val_loss: 0.2569
Epoch 54/256
3063/3063 - 29s - loss: 0.2315 - val_loss: 0.2470
Epoch 55/256
3063/3063 - 29s - loss: 0.2311 - val_loss: 0.2515
Epoch 56/256
3063/3063 - 29s - loss: 0.2300 - val_loss: 0.2490
Epoch 57/256
3063/3063 - 29s - loss: 0.2284 - val_loss: 0.2535
Epoch 58/256
3063/3063 - 29s - loss: 0.2287 - val_loss: 0.2609
Epoch 59/256
3063/3063 - 29s - loss: 0.2275 - val_loss: 0.2534
Epoch 60/256
3063/3063 - 29s - loss: 0.2268 - val_loss: 0.2492
Epoch 61/256
3063/3063 - 29s - loss: 0.2261 - val_loss: 0.2563
Epoch 62/256
3063/3063 - 29s - loss: 0.2250 - val_loss: 0.2547
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 16)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.3963 - val_loss: 0.3675
Epoch 2/256
3063/3063 - 29s - loss: 0.3626 - val_loss: 0.3522
Epoch 3/256
3063/3063 - 29s - loss: 0.3547 - val_loss: 0.3618
Epoch 4/256
3063/3063 - 29s - loss: 0.3503 - val_loss: 0.3541
Epoch 5/256
3063/3063 - 29s - loss: 0.3467 - val_loss: 0.3458
Epoch 6/256
3063/3063 - 29s - loss: 0.3448 - val_loss: 0.3500
Epoch 7/256
3063/3063 - 29s - loss: 0.3409 - val_loss: 0.3417
Epoch 8/256
3063/3063 - 29s - loss: 0.3387 - val_loss: 0.3393
Epoch 9/256
3063/3063 - 29s - loss: 0.3358 - val_loss: 0.3359
Epoch 10/256
3063/3063 - 29s - loss: 0.3325 - val_loss: 0.3318
Epoch 11/256
3063/3063 - 29s - loss: 0.3309 - val_loss: 0.3266
Epoch 12/256
3063/3063 - 29s - loss: 0.3280 - val_loss: 0.3238
Epoch 13/256
3063/3063 - 29s - loss: 0.3245 - val_loss: 0.3265
Epoch 14/256
3063/3063 - 29s - loss: 0.3214 - val_loss: 0.3226
Epoch 15/256
3063/3063 - 29s - loss: 0.3172 - val_loss: 0.3203
Epoch 16/256
3063/3063 - 29s - loss: 0.3121 - val_loss: 0.3115
Epoch 17/256
3063/3063 - 29s - loss: 0.3069 - val_loss: 0.3128
Epoch 18/256
3063/3063 - 29s - loss: 0.3024 - val_loss: 0.3061
Epoch 19/256
3063/3063 - 29s - loss: 0.2988 - val_loss: 0.3111
Epoch 20/256
3063/3063 - 29s - loss: 0.2943 - val_loss: 0.2921
Epoch 21/256
3063/3063 - 29s - loss: 0.2901 - val_loss: 0.3076
Epoch 22/256
3063/3063 - 29s - loss: 0.2872 - val_loss: 0.2844
Epoch 23/256
3063/3063 - 29s - loss: 0.2836 - val_loss: 0.2836
Epoch 24/256
3063/3063 - 29s - loss: 0.2804 - val_loss: 0.2876
Epoch 25/256
3063/3063 - 29s - loss: 0.2771 - val_loss: 0.2766
Epoch 26/256
3063/3063 - 29s - loss: 0.2748 - val_loss: 0.2875
Epoch 27/256
3063/3063 - 29s - loss: 0.2726 - val_loss: 0.2713
Epoch 28/256
3063/3063 - 29s - loss: 0.2708 - val_loss: 0.2765
Epoch 29/256
3063/3063 - 29s - loss: 0.2673 - val_loss: 0.2763
Epoch 30/256
3063/3063 - 29s - loss: 0.2655 - val_loss: 0.2692
Epoch 31/256
3063/3063 - 29s - loss: 0.2644 - val_loss: 0.2807
Epoch 32/256
3063/3063 - 29s - loss: 0.2629 - val_loss: 0.2656
Epoch 33/256
3063/3063 - 29s - loss: 0.2609 - val_loss: 0.2748
Epoch 34/256
3063/3063 - 29s - loss: 0.2592 - val_loss: 0.2697
Epoch 35/256
3063/3063 - 29s - loss: 0.2573 - val_loss: 0.2672
Epoch 36/256
3063/3063 - 29s - loss: 0.2556 - val_loss: 0.2601
Epoch 37/256
3063/3063 - 29s - loss: 0.2541 - val_loss: 0.2692
Epoch 38/256
3063/3063 - 29s - loss: 0.2539 - val_loss: 0.2595
Epoch 39/256
3063/3063 - 29s - loss: 0.2531 - val_loss: 0.2576
Epoch 40/256
3063/3063 - 29s - loss: 0.2503 - val_loss: 0.2584
Epoch 41/256
3063/3063 - 29s - loss: 0.2499 - val_loss: 0.2671
Epoch 42/256
3063/3063 - 29s - loss: 0.2489 - val_loss: 0.2557
Epoch 43/256
3063/3063 - 29s - loss: 0.2473 - val_loss: 0.2615
Epoch 44/256
3063/3063 - 29s - loss: 0.2469 - val_loss: 0.2574
Epoch 45/256
3063/3063 - 29s - loss: 0.2457 - val_loss: 0.2631
Epoch 46/256
3063/3063 - 29s - loss: 0.2443 - val_loss: 0.2622
Epoch 47/256
3063/3063 - 29s - loss: 0.2441 - val_loss: 0.2622
Epoch 48/256
3063/3063 - 29s - loss: 0.2436 - val_loss: 0.2603
Epoch 49/256
3063/3063 - 29s - loss: 0.2420 - val_loss: 0.2595
Epoch 50/256
3063/3063 - 29s - loss: 0.2398 - val_loss: 0.2566
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 16)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 30s - loss: 0.3929 - val_loss: 0.3646
Epoch 2/256
3063/3063 - 29s - loss: 0.3606 - val_loss: 0.3577
Epoch 3/256
3063/3063 - 29s - loss: 0.3528 - val_loss: 0.3494
Epoch 4/256
3063/3063 - 29s - loss: 0.3493 - val_loss: 0.3476
Epoch 5/256
3063/3063 - 29s - loss: 0.3452 - val_loss: 0.3419
Epoch 6/256
3063/3063 - 29s - loss: 0.3424 - val_loss: 0.3465
Epoch 7/256
3063/3063 - 29s - loss: 0.3392 - val_loss: 0.3361
Epoch 8/256
3063/3063 - 29s - loss: 0.3352 - val_loss: 0.3311
Epoch 9/256
3063/3063 - 29s - loss: 0.3329 - val_loss: 0.3379
Epoch 10/256
3063/3063 - 29s - loss: 0.3289 - val_loss: 0.3230
Epoch 11/256
3063/3063 - 29s - loss: 0.3230 - val_loss: 0.3392
Epoch 12/256
3063/3063 - 29s - loss: 0.3175 - val_loss: 0.3189
Epoch 13/256
3063/3063 - 29s - loss: 0.3092 - val_loss: 0.3081
Epoch 14/256
3063/3063 - 29s - loss: 0.3013 - val_loss: 0.2981
Epoch 15/256
3063/3063 - 29s - loss: 0.2955 - val_loss: 0.2895
Epoch 16/256
3063/3063 - 29s - loss: 0.2900 - val_loss: 0.2911
Epoch 17/256
3063/3063 - 29s - loss: 0.2857 - val_loss: 0.2891
Epoch 18/256
3063/3063 - 29s - loss: 0.2824 - val_loss: 0.2964
Epoch 19/256
3063/3063 - 29s - loss: 0.2808 - val_loss: 0.2919
Epoch 20/256
3063/3063 - 29s - loss: 0.2774 - val_loss: 0.2899
Epoch 21/256
3063/3063 - 29s - loss: 0.2755 - val_loss: 0.2862
Epoch 22/256
3063/3063 - 29s - loss: 0.2725 - val_loss: 0.3062
Epoch 23/256
3063/3063 - 29s - loss: 0.2706 - val_loss: 0.2780
Epoch 24/256
3063/3063 - 29s - loss: 0.2696 - val_loss: 0.2738
Epoch 25/256
3063/3063 - 29s - loss: 0.2672 - val_loss: 0.2712
Epoch 26/256
3063/3063 - 29s - loss: 0.2662 - val_loss: 0.2737
Epoch 27/256
3063/3063 - 29s - loss: 0.2642 - val_loss: 0.2715
Epoch 28/256
3063/3063 - 29s - loss: 0.2624 - val_loss: 0.2677
Epoch 29/256
3063/3063 - 29s - loss: 0.2609 - val_loss: 0.2819
Epoch 30/256
3063/3063 - 29s - loss: 0.2595 - val_loss: 0.2684
Epoch 31/256
3063/3063 - 29s - loss: 0.2580 - val_loss: 0.2662
Epoch 32/256
3063/3063 - 29s - loss: 0.2553 - val_loss: 0.2747
Epoch 33/256
3063/3063 - 29s - loss: 0.2555 - val_loss: 0.2682
Epoch 34/256
3063/3063 - 29s - loss: 0.2536 - val_loss: 0.2801
Epoch 35/256
3063/3063 - 29s - loss: 0.2522 - val_loss: 0.2680
Epoch 36/256
3063/3063 - 29s - loss: 0.2515 - val_loss: 0.2606
Epoch 37/256
3063/3063 - 29s - loss: 0.2504 - val_loss: 0.2821
Epoch 38/256
3063/3063 - 29s - loss: 0.2494 - val_loss: 0.2565
Epoch 39/256
3063/3063 - 29s - loss: 0.2486 - val_loss: 0.2587
Epoch 40/256
3063/3063 - 29s - loss: 0.2475 - val_loss: 0.2595
Epoch 41/256
3063/3063 - 29s - loss: 0.2476 - val_loss: 0.2646
Epoch 42/256
3063/3063 - 29s - loss: 0.2453 - val_loss: 0.2896
Epoch 43/256
3063/3063 - 29s - loss: 0.2446 - val_loss: 0.2595
Epoch 44/256
3063/3063 - 29s - loss: 0.2437 - val_loss: 0.2559
Epoch 45/256
3063/3063 - 29s - loss: 0.2430 - val_loss: 0.2558
Epoch 46/256
3063/3063 - 29s - loss: 0.2422 - val_loss: 0.2601
Epoch 47/256
3063/3063 - 29s - loss: 0.2415 - val_loss: 0.2542
Epoch 48/256
3063/3063 - 29s - loss: 0.2403 - val_loss: 0.2540
Epoch 49/256
3063/3063 - 29s - loss: 0.2395 - val_loss: 0.2825
Epoch 50/256
3063/3063 - 29s - loss: 0.2379 - val_loss: 0.2581
Epoch 51/256
3063/3063 - 29s - loss: 0.2389 - val_loss: 0.2699
Epoch 52/256
3063/3063 - 29s - loss: 0.2369 - val_loss: 0.2562
Epoch 53/256
3063/3063 - 29s - loss: 0.2363 - val_loss: 0.2536
Epoch 54/256
3063/3063 - 29s - loss: 0.2346 - val_loss: 0.2720
Epoch 55/256
3063/3063 - 29s - loss: 0.2342 - val_loss: 0.2494
Epoch 56/256
3063/3063 - 29s - loss: 0.2337 - val_loss: 0.2573
Epoch 57/256
3063/3063 - 29s - loss: 0.2333 - val_loss: 0.2547
Epoch 58/256
3063/3063 - 29s - loss: 0.2322 - val_loss: 0.2603
Epoch 59/256
3063/3063 - 29s - loss: 0.2318 - val_loss: 0.2517
Epoch 60/256
3063/3063 - 29s - loss: 0.2306 - val_loss: 0.2532
Epoch 61/256
3063/3063 - 29s - loss: 0.2303 - val_loss: 0.2513
Epoch 62/256
3063/3063 - 29s - loss: 0.2294 - val_loss: 0.2597
Epoch 63/256
3063/3063 - 29s - loss: 0.2301 - val_loss: 0.2526
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 16)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.3933 - val_loss: 0.3603
Epoch 2/256
3063/3063 - 29s - loss: 0.3615 - val_loss: 0.3487
Epoch 3/256
3063/3063 - 29s - loss: 0.3538 - val_loss: 0.3530
Epoch 4/256
3063/3063 - 29s - loss: 0.3488 - val_loss: 0.3455
Epoch 5/256
3063/3063 - 29s - loss: 0.3465 - val_loss: 0.3412
Epoch 6/256
3063/3063 - 29s - loss: 0.3428 - val_loss: 0.3378
Epoch 7/256
3063/3063 - 29s - loss: 0.3410 - val_loss: 0.3408
Epoch 8/256
3063/3063 - 29s - loss: 0.3394 - val_loss: 0.3362
Epoch 9/256
3063/3063 - 29s - loss: 0.3368 - val_loss: 0.3426
Epoch 10/256
3063/3063 - 29s - loss: 0.3352 - val_loss: 0.3341
Epoch 11/256
3063/3063 - 29s - loss: 0.3331 - val_loss: 0.3303
Epoch 12/256
3063/3063 - 34s - loss: 0.3290 - val_loss: 0.3259
Epoch 13/256
3063/3063 - 29s - loss: 0.3255 - val_loss: 0.3368
Epoch 14/256
3063/3063 - 29s - loss: 0.3203 - val_loss: 0.3170
Epoch 15/256
3063/3063 - 29s - loss: 0.3137 - val_loss: 0.3088
Epoch 16/256
3063/3063 - 29s - loss: 0.3046 - val_loss: 0.3056
Epoch 17/256
3063/3063 - 29s - loss: 0.2966 - val_loss: 0.2979
Epoch 18/256
3063/3063 - 29s - loss: 0.2901 - val_loss: 0.2890
Epoch 19/256
3063/3063 - 29s - loss: 0.2855 - val_loss: 0.2941
Epoch 20/256
3063/3063 - 29s - loss: 0.2816 - val_loss: 0.2901
Epoch 21/256
3063/3063 - 29s - loss: 0.2809 - val_loss: 0.2778
Epoch 22/256
3063/3063 - 29s - loss: 0.2764 - val_loss: 0.2847
Epoch 23/256
3063/3063 - 29s - loss: 0.2743 - val_loss: 0.2940
Epoch 24/256
3063/3063 - 29s - loss: 0.2721 - val_loss: 0.2753
Epoch 25/256
3063/3063 - 29s - loss: 0.2690 - val_loss: 0.2888
Epoch 26/256
3063/3063 - 29s - loss: 0.2669 - val_loss: 0.2703
Epoch 27/256
3063/3063 - 29s - loss: 0.2672 - val_loss: 0.2914
Epoch 28/256
3063/3063 - 29s - loss: 0.2652 - val_loss: 0.2809
Epoch 29/256
3063/3063 - 29s - loss: 0.2623 - val_loss: 0.2695
Epoch 30/256
3063/3063 - 29s - loss: 0.2614 - val_loss: 0.2642
Epoch 31/256
3063/3063 - 29s - loss: 0.2604 - val_loss: 0.2637
Epoch 32/256
3063/3063 - 29s - loss: 0.2592 - val_loss: 0.2638
Epoch 33/256
3063/3063 - 29s - loss: 0.2582 - val_loss: 0.2686
Epoch 34/256
3063/3063 - 29s - loss: 0.2568 - val_loss: 0.2640
Epoch 35/256
3063/3063 - 29s - loss: 0.2557 - val_loss: 0.2603
Epoch 36/256
3063/3063 - 29s - loss: 0.2548 - val_loss: 0.2600
Epoch 37/256
3063/3063 - 29s - loss: 0.2527 - val_loss: 0.2698
Epoch 38/256
3063/3063 - 29s - loss: 0.2523 - val_loss: 0.2659
Epoch 39/256
3063/3063 - 29s - loss: 0.2514 - val_loss: 0.2626
Epoch 40/256
3063/3063 - 29s - loss: 0.2501 - val_loss: 0.2566
Epoch 41/256
3063/3063 - 29s - loss: 0.2497 - val_loss: 0.2889
Epoch 42/256
3063/3063 - 29s - loss: 0.2485 - val_loss: 0.2591
Epoch 43/256
3063/3063 - 29s - loss: 0.2476 - val_loss: 0.2603
Epoch 44/256
3063/3063 - 29s - loss: 0.2459 - val_loss: 0.2567
Epoch 45/256
3063/3063 - 29s - loss: 0.2461 - val_loss: 0.2593
Epoch 46/256
3063/3063 - 29s - loss: 0.2448 - val_loss: 0.2534
Epoch 47/256
3063/3063 - 29s - loss: 0.2440 - val_loss: 0.2816
Epoch 48/256
3063/3063 - 29s - loss: 0.2427 - val_loss: 0.2569
Epoch 49/256
3063/3063 - 29s - loss: 0.2423 - val_loss: 0.2546
Epoch 50/256
3063/3063 - 29s - loss: 0.2412 - val_loss: 0.2561
Epoch 51/256
3063/3063 - 29s - loss: 0.2406 - val_loss: 0.2528
Epoch 52/256
3063/3063 - 29s - loss: 0.2396 - val_loss: 0.2577
Epoch 53/256
3063/3063 - 29s - loss: 0.2395 - val_loss: 0.2581
Epoch 54/256
3063/3063 - 29s - loss: 0.2383 - val_loss: 0.2580
Epoch 55/256
3063/3063 - 29s - loss: 0.2380 - val_loss: 0.2539
Epoch 56/256
3063/3063 - 29s - loss: 0.2378 - val_loss: 0.2569
Epoch 57/256
3063/3063 - 29s - loss: 0.2370 - val_loss: 0.2559
Epoch 58/256
3063/3063 - 29s - loss: 0.2358 - val_loss: 0.2504
Epoch 59/256
3063/3063 - 29s - loss: 0.2353 - val_loss: 0.2587
Epoch 60/256
3063/3063 - 29s - loss: 0.2334 - val_loss: 0.2560
Epoch 61/256
3063/3063 - 29s - loss: 0.2324 - val_loss: 0.2596
Epoch 62/256
3063/3063 - 29s - loss: 0.2326 - val_loss: 0.2530
Epoch 63/256
3063/3063 - 29s - loss: 0.2316 - val_loss: 0.2551
Epoch 64/256
3063/3063 - 29s - loss: 0.2314 - val_loss: 0.2752
Epoch 65/256
3063/3063 - 29s - loss: 0.2302 - val_loss: 0.2516
Epoch 66/256
3063/3063 - 29s - loss: 0.2285 - val_loss: 0.2522
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 16)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.3953 - val_loss: 0.3628
Epoch 2/256
3063/3063 - 29s - loss: 0.3599 - val_loss: 0.3493
Epoch 3/256
3063/3063 - 29s - loss: 0.3520 - val_loss: 0.3433
Epoch 4/256
3063/3063 - 29s - loss: 0.3475 - val_loss: 0.3451
Epoch 5/256
3063/3063 - 29s - loss: 0.3445 - val_loss: 0.3402
Epoch 6/256
3063/3063 - 29s - loss: 0.3411 - val_loss: 0.3634
Epoch 7/256
3063/3063 - 29s - loss: 0.3394 - val_loss: 0.3463
Epoch 8/256
3063/3063 - 29s - loss: 0.3353 - val_loss: 0.3341
Epoch 9/256
3063/3063 - 29s - loss: 0.3300 - val_loss: 0.3456
Epoch 10/256
3063/3063 - 29s - loss: 0.3227 - val_loss: 0.3191
Epoch 11/256
3063/3063 - 29s - loss: 0.3157 - val_loss: 0.3197
Epoch 12/256
3063/3063 - 29s - loss: 0.3090 - val_loss: 0.3021
Epoch 13/256
3063/3063 - 29s - loss: 0.3041 - val_loss: 0.3004
Epoch 14/256
3063/3063 - 29s - loss: 0.2984 - val_loss: 0.3055
Epoch 15/256
3063/3063 - 29s - loss: 0.2946 - val_loss: 0.2995
Epoch 16/256
3063/3063 - 29s - loss: 0.2895 - val_loss: 0.2885
Epoch 17/256
3063/3063 - 29s - loss: 0.2855 - val_loss: 0.2785
Epoch 18/256
3063/3063 - 29s - loss: 0.2809 - val_loss: 0.2778
Epoch 19/256
3063/3063 - 29s - loss: 0.2782 - val_loss: 0.3215
Epoch 20/256
3063/3063 - 29s - loss: 0.2757 - val_loss: 0.2699
Epoch 21/256
3063/3063 - 29s - loss: 0.2737 - val_loss: 0.2726
Epoch 22/256
3063/3063 - 29s - loss: 0.2709 - val_loss: 0.2827
Epoch 23/256
3063/3063 - 29s - loss: 0.2682 - val_loss: 0.2702
Epoch 24/256
3063/3063 - 29s - loss: 0.2665 - val_loss: 0.2813
Epoch 25/256
3063/3063 - 29s - loss: 0.2644 - val_loss: 0.2691
Epoch 26/256
3063/3063 - 29s - loss: 0.2628 - val_loss: 0.2757
Epoch 27/256
3063/3063 - 29s - loss: 0.2615 - val_loss: 0.2756
Epoch 28/256
3063/3063 - 29s - loss: 0.2595 - val_loss: 0.2683
Epoch 29/256
3063/3063 - 29s - loss: 0.2569 - val_loss: 0.2630
Epoch 30/256
3063/3063 - 29s - loss: 0.2559 - val_loss: 0.2651
Epoch 31/256
3063/3063 - 29s - loss: 0.2550 - val_loss: 0.2600
Epoch 32/256
3063/3063 - 29s - loss: 0.2542 - val_loss: 0.2620
Epoch 33/256
3063/3063 - 29s - loss: 0.2525 - val_loss: 0.2623
Epoch 34/256
3063/3063 - 29s - loss: 0.2519 - val_loss: 0.2619
Epoch 35/256
3063/3063 - 29s - loss: 0.2502 - val_loss: 0.2567
Epoch 36/256
3063/3063 - 29s - loss: 0.2491 - val_loss: 0.2612
Epoch 37/256
3063/3063 - 29s - loss: 0.2479 - val_loss: 0.2601
Epoch 38/256
3063/3063 - 29s - loss: 0.2469 - val_loss: 0.2595
Epoch 39/256
3063/3063 - 29s - loss: 0.2455 - val_loss: 0.2744
Epoch 40/256
3063/3063 - 29s - loss: 0.2450 - val_loss: 0.2603
Epoch 41/256
3063/3063 - 29s - loss: 0.2442 - val_loss: 0.2583
Epoch 42/256
3063/3063 - 29s - loss: 0.2429 - val_loss: 0.2640
Epoch 43/256
3063/3063 - 29s - loss: 0.2411 - val_loss: 0.2571
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 16)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 30s - loss: 0.3955 - val_loss: 0.3623
Epoch 2/256
3063/3063 - 29s - loss: 0.3647 - val_loss: 0.3559
Epoch 3/256
3063/3063 - 29s - loss: 0.3544 - val_loss: 0.3556
Epoch 4/256
3063/3063 - 29s - loss: 0.3494 - val_loss: 0.3464
Epoch 5/256
3063/3063 - 29s - loss: 0.3446 - val_loss: 0.3446
Epoch 6/256
3063/3063 - 29s - loss: 0.3423 - val_loss: 0.3426
Epoch 7/256
3063/3063 - 29s - loss: 0.3394 - val_loss: 0.3454
Epoch 8/256
3063/3063 - 29s - loss: 0.3360 - val_loss: 0.3444
Epoch 9/256
3063/3063 - 29s - loss: 0.3339 - val_loss: 0.3293
Epoch 10/256
3063/3063 - 29s - loss: 0.3304 - val_loss: 0.3354
Epoch 11/256
3063/3063 - 29s - loss: 0.3265 - val_loss: 0.3501
Epoch 12/256
3063/3063 - 29s - loss: 0.3217 - val_loss: 0.3221
Epoch 13/256
3063/3063 - 29s - loss: 0.3148 - val_loss: 0.3184
Epoch 14/256
3063/3063 - 29s - loss: 0.3100 - val_loss: 0.3175
Epoch 15/256
3063/3063 - 29s - loss: 0.3042 - val_loss: 0.3076
Epoch 16/256
3063/3063 - 29s - loss: 0.2984 - val_loss: 0.3116
Epoch 17/256
3063/3063 - 29s - loss: 0.2938 - val_loss: 0.2950
Epoch 18/256
3063/3063 - 29s - loss: 0.2910 - val_loss: 0.2837
Epoch 19/256
3063/3063 - 29s - loss: 0.2854 - val_loss: 0.2855
Epoch 20/256
3063/3063 - 29s - loss: 0.2820 - val_loss: 0.2851
Epoch 21/256
3063/3063 - 29s - loss: 0.2784 - val_loss: 0.2803
Epoch 22/256
3063/3063 - 29s - loss: 0.2760 - val_loss: 0.2948
Epoch 23/256
3063/3063 - 29s - loss: 0.2747 - val_loss: 0.2793
Epoch 24/256
3063/3063 - 29s - loss: 0.2726 - val_loss: 0.2784
Epoch 25/256
3063/3063 - 29s - loss: 0.2692 - val_loss: 0.2795
Epoch 26/256
3063/3063 - 29s - loss: 0.2688 - val_loss: 0.2730
Epoch 27/256
3063/3063 - 29s - loss: 0.2669 - val_loss: 0.2676
Epoch 28/256
3063/3063 - 29s - loss: 0.2659 - val_loss: 0.2716
Epoch 29/256
3063/3063 - 29s - loss: 0.2634 - val_loss: 0.2630
Epoch 30/256
3063/3063 - 29s - loss: 0.2616 - val_loss: 0.2703
Epoch 31/256
3063/3063 - 29s - loss: 0.2609 - val_loss: 0.2882
Epoch 32/256
3063/3063 - 29s - loss: 0.2591 - val_loss: 0.2646
Epoch 33/256
3063/3063 - 29s - loss: 0.2573 - val_loss: 0.2625
Epoch 34/256
3063/3063 - 29s - loss: 0.2574 - val_loss: 0.2609
Epoch 35/256
3063/3063 - 29s - loss: 0.2557 - val_loss: 0.2660
Epoch 36/256
3063/3063 - 29s - loss: 0.2551 - val_loss: 0.2900
Epoch 37/256
3063/3063 - 29s - loss: 0.2536 - val_loss: 0.2672
Epoch 38/256
3063/3063 - 29s - loss: 0.2529 - val_loss: 0.2638
Epoch 39/256
3063/3063 - 29s - loss: 0.2512 - val_loss: 0.2703
Epoch 40/256
3063/3063 - 29s - loss: 0.2509 - val_loss: 0.2661
Epoch 41/256
3063/3063 - 29s - loss: 0.2494 - val_loss: 0.2645
Epoch 42/256
3063/3063 - 29s - loss: 0.2490 - val_loss: 0.2558
Epoch 43/256
3063/3063 - 29s - loss: 0.2463 - val_loss: 0.2549
Epoch 44/256
3063/3063 - 29s - loss: 0.2467 - val_loss: 0.2690
Epoch 45/256
3063/3063 - 29s - loss: 0.2455 - val_loss: 0.2563
Epoch 46/256
3063/3063 - 29s - loss: 0.2453 - val_loss: 0.2793
Epoch 47/256
3063/3063 - 29s - loss: 0.2446 - val_loss: 0.2556
Epoch 48/256
3063/3063 - 29s - loss: 0.2432 - val_loss: 0.2587
Epoch 49/256
3063/3063 - 29s - loss: 0.2426 - val_loss: 0.2688
Epoch 50/256
3063/3063 - 29s - loss: 0.2413 - val_loss: 0.2528
Epoch 51/256
3063/3063 - 29s - loss: 0.2405 - val_loss: 0.2565
Epoch 52/256
3063/3063 - 29s - loss: 0.2394 - val_loss: 0.2645
Epoch 53/256
3063/3063 - 29s - loss: 0.2392 - val_loss: 0.2508
Epoch 54/256
3063/3063 - 29s - loss: 0.2370 - val_loss: 0.2499
Epoch 55/256
3063/3063 - 29s - loss: 0.2367 - val_loss: 0.2516
Epoch 56/256
3063/3063 - 29s - loss: 0.2361 - val_loss: 0.2680
Epoch 57/256
3063/3063 - 29s - loss: 0.2366 - val_loss: 0.2600
Epoch 58/256
3063/3063 - 29s - loss: 0.2346 - val_loss: 0.2830
Epoch 59/256
3063/3063 - 29s - loss: 0.2340 - val_loss: 0.2561
Epoch 60/256
3063/3063 - 29s - loss: 0.2330 - val_loss: 0.2560
Epoch 61/256
3063/3063 - 29s - loss: 0.2327 - val_loss: 0.2755
Epoch 62/256
3063/3063 - 29s - loss: 0.2306 - val_loss: 0.2563
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 16)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 30s - loss: 0.4005 - val_loss: 0.3642
Epoch 2/256
3063/3063 - 29s - loss: 0.3636 - val_loss: 0.3530
Epoch 3/256
3063/3063 - 29s - loss: 0.3522 - val_loss: 0.3466
Epoch 4/256
3063/3063 - 29s - loss: 0.3481 - val_loss: 0.3425
Epoch 5/256
3063/3063 - 29s - loss: 0.3439 - val_loss: 0.3451
Epoch 6/256
3063/3063 - 29s - loss: 0.3403 - val_loss: 0.3354
Epoch 7/256
3063/3063 - 29s - loss: 0.3379 - val_loss: 0.3350
Epoch 8/256
3063/3063 - 29s - loss: 0.3343 - val_loss: 0.3288
Epoch 9/256
3063/3063 - 29s - loss: 0.3305 - val_loss: 0.3494
Epoch 10/256
3063/3063 - 29s - loss: 0.3259 - val_loss: 0.3273
Epoch 11/256
3063/3063 - 29s - loss: 0.3213 - val_loss: 0.3183
Epoch 12/256
3063/3063 - 29s - loss: 0.3150 - val_loss: 0.3135
Epoch 13/256
3063/3063 - 29s - loss: 0.3098 - val_loss: 0.3109
Epoch 14/256
3063/3063 - 29s - loss: 0.3030 - val_loss: 0.3043
Epoch 15/256
3063/3063 - 29s - loss: 0.2980 - val_loss: 0.2984
Epoch 16/256
3063/3063 - 29s - loss: 0.2914 - val_loss: 0.3052
Epoch 17/256
3063/3063 - 29s - loss: 0.2862 - val_loss: 0.2880
Epoch 18/256
3063/3063 - 29s - loss: 0.2822 - val_loss: 0.3069
Epoch 19/256
3063/3063 - 29s - loss: 0.2793 - val_loss: 0.2831
Epoch 20/256
3063/3063 - 29s - loss: 0.2763 - val_loss: 0.2995
Epoch 21/256
3063/3063 - 29s - loss: 0.2727 - val_loss: 0.2837
Epoch 22/256
3063/3063 - 29s - loss: 0.2722 - val_loss: 0.2778
Epoch 23/256
3063/3063 - 29s - loss: 0.2685 - val_loss: 0.2784
Epoch 24/256
3063/3063 - 29s - loss: 0.2666 - val_loss: 0.2736
Epoch 25/256
3063/3063 - 29s - loss: 0.2639 - val_loss: 0.2796
Epoch 26/256
3063/3063 - 29s - loss: 0.2641 - val_loss: 0.2690
Epoch 27/256
3063/3063 - 29s - loss: 0.2599 - val_loss: 0.2641
Epoch 28/256
3063/3063 - 29s - loss: 0.2600 - val_loss: 0.2619
Epoch 29/256
3063/3063 - 29s - loss: 0.2575 - val_loss: 0.2702
Epoch 30/256
3063/3063 - 29s - loss: 0.2563 - val_loss: 0.2621
Epoch 31/256
3063/3063 - 29s - loss: 0.2551 - val_loss: 0.2720
Epoch 32/256
3063/3063 - 29s - loss: 0.2537 - val_loss: 0.2631
Epoch 33/256
3063/3063 - 29s - loss: 0.2535 - val_loss: 0.2638
Epoch 34/256
3063/3063 - 29s - loss: 0.2520 - val_loss: 0.2621
Epoch 35/256
3063/3063 - 29s - loss: 0.2505 - val_loss: 0.2586
Epoch 36/256
3063/3063 - 29s - loss: 0.2488 - val_loss: 0.2577
Epoch 37/256
3063/3063 - 29s - loss: 0.2461 - val_loss: 0.2638
Epoch 38/256
3063/3063 - 29s - loss: 0.2467 - val_loss: 0.2526
Epoch 39/256
3063/3063 - 29s - loss: 0.2456 - val_loss: 0.2616
Epoch 40/256
3063/3063 - 29s - loss: 0.2452 - val_loss: 0.2584
Epoch 41/256
3063/3063 - 29s - loss: 0.2438 - val_loss: 0.2540
Epoch 42/256
3063/3063 - 29s - loss: 0.2416 - val_loss: 0.2496
Epoch 43/256
3063/3063 - 29s - loss: 0.2404 - val_loss: 0.2557
Epoch 44/256
3063/3063 - 29s - loss: 0.2390 - val_loss: 0.2500
Epoch 45/256
3063/3063 - 29s - loss: 0.2389 - val_loss: 0.2543
Epoch 46/256
3063/3063 - 29s - loss: 0.2382 - val_loss: 0.2527
Epoch 47/256
3063/3063 - 29s - loss: 0.2372 - val_loss: 0.2589
Epoch 48/256
3063/3063 - 29s - loss: 0.2359 - val_loss: 0.2515
Epoch 49/256
3063/3063 - 29s - loss: 0.2345 - val_loss: 0.2570
Epoch 50/256
3063/3063 - 29s - loss: 0.2348 - val_loss: 0.2503
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 16)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 30s - loss: 0.3957 - val_loss: 0.3566
Epoch 2/256
3063/3063 - 29s - loss: 0.3635 - val_loss: 0.3490
Epoch 3/256
3063/3063 - 29s - loss: 0.3532 - val_loss: 0.3460
Epoch 4/256
3063/3063 - 29s - loss: 0.3469 - val_loss: 0.3426
Epoch 5/256
3063/3063 - 29s - loss: 0.3436 - val_loss: 0.3361
Epoch 6/256
3063/3063 - 29s - loss: 0.3403 - val_loss: 0.3354
Epoch 7/256
3063/3063 - 29s - loss: 0.3377 - val_loss: 0.3425
Epoch 8/256
3063/3063 - 29s - loss: 0.3334 - val_loss: 0.3309
Epoch 9/256
3063/3063 - 29s - loss: 0.3300 - val_loss: 0.3280
Epoch 10/256
3063/3063 - 29s - loss: 0.3258 - val_loss: 0.3217
Epoch 11/256
3063/3063 - 29s - loss: 0.3187 - val_loss: 0.3093
Epoch 12/256
3063/3063 - 29s - loss: 0.3125 - val_loss: 0.3127
Epoch 13/256
3063/3063 - 29s - loss: 0.3059 - val_loss: 0.2970
Epoch 14/256
3063/3063 - 29s - loss: 0.2998 - val_loss: 0.3103
Epoch 15/256
3063/3063 - 29s - loss: 0.2957 - val_loss: 0.3010
Epoch 16/256
3063/3063 - 29s - loss: 0.2916 - val_loss: 0.3006
Epoch 17/256
3063/3063 - 29s - loss: 0.2878 - val_loss: 0.3098
Epoch 18/256
3063/3063 - 29s - loss: 0.2846 - val_loss: 0.2799
Epoch 19/256
3063/3063 - 29s - loss: 0.2813 - val_loss: 0.2976
Epoch 20/256
3063/3063 - 29s - loss: 0.2789 - val_loss: 0.2973
Epoch 21/256
3063/3063 - 29s - loss: 0.2751 - val_loss: 0.2759
Epoch 22/256
3063/3063 - 29s - loss: 0.2734 - val_loss: 0.2832
Epoch 23/256
3063/3063 - 29s - loss: 0.2709 - val_loss: 0.2863
Epoch 24/256
3063/3063 - 29s - loss: 0.2691 - val_loss: 0.2808
Epoch 25/256
3063/3063 - 29s - loss: 0.2671 - val_loss: 0.2688
Epoch 26/256
3063/3063 - 29s - loss: 0.2652 - val_loss: 0.2680
Epoch 27/256
3063/3063 - 29s - loss: 0.2642 - val_loss: 0.2920
Epoch 28/256
3063/3063 - 29s - loss: 0.2625 - val_loss: 0.2711
Epoch 29/256
3063/3063 - 29s - loss: 0.2608 - val_loss: 0.2805
Epoch 30/256
3063/3063 - 29s - loss: 0.2589 - val_loss: 0.2676
Epoch 31/256
3063/3063 - 29s - loss: 0.2566 - val_loss: 0.2684
Epoch 32/256
3063/3063 - 29s - loss: 0.2561 - val_loss: 0.2588
Epoch 33/256
3063/3063 - 29s - loss: 0.2543 - val_loss: 0.2601
Epoch 34/256
3063/3063 - 29s - loss: 0.2525 - val_loss: 0.2630
Epoch 35/256
3063/3063 - 29s - loss: 0.2512 - val_loss: 0.2687
Epoch 36/256
3063/3063 - 29s - loss: 0.2502 - val_loss: 0.2589
Epoch 37/256
3063/3063 - 29s - loss: 0.2487 - val_loss: 0.2545
Epoch 38/256
3063/3063 - 29s - loss: 0.2473 - val_loss: 0.2565
Epoch 39/256
3063/3063 - 29s - loss: 0.2464 - val_loss: 0.2648
Epoch 40/256
3063/3063 - 29s - loss: 0.2450 - val_loss: 0.2633
Epoch 41/256
3063/3063 - 29s - loss: 0.2434 - val_loss: 0.2728
Epoch 42/256
3063/3063 - 29s - loss: 0.2428 - val_loss: 0.2544
Epoch 43/256
3063/3063 - 29s - loss: 0.2415 - val_loss: 0.2563
Epoch 44/256
3063/3063 - 29s - loss: 0.2400 - val_loss: 0.2642
Epoch 45/256
3063/3063 - 29s - loss: 0.2397 - val_loss: 0.2618
Epoch 46/256
3063/3063 - 29s - loss: 0.2373 - val_loss: 0.2598
Epoch 47/256
3063/3063 - 29s - loss: 0.2373 - val_loss: 0.2474
Epoch 48/256
3063/3063 - 29s - loss: 0.2362 - val_loss: 0.2567
Epoch 49/256
3063/3063 - 29s - loss: 0.2361 - val_loss: 0.2512
Epoch 50/256
3063/3063 - 29s - loss: 0.2351 - val_loss: 0.2651
Epoch 51/256
3063/3063 - 29s - loss: 0.2342 - val_loss: 0.2718
Epoch 52/256
3063/3063 - 29s - loss: 0.2327 - val_loss: 0.2472
Epoch 53/256
3063/3063 - 29s - loss: 0.2308 - val_loss: 0.2556
Epoch 54/256
3063/3063 - 29s - loss: 0.2307 - val_loss: 0.2517
Epoch 55/256
3063/3063 - 29s - loss: 0.2288 - val_loss: 0.2547
Epoch 56/256
3063/3063 - 29s - loss: 0.2304 - val_loss: 0.2451
Epoch 57/256
3063/3063 - 29s - loss: 0.2288 - val_loss: 0.2474
Epoch 58/256
3063/3063 - 29s - loss: 0.2285 - val_loss: 0.2526
Epoch 59/256
3063/3063 - 29s - loss: 0.2267 - val_loss: 0.2504
Epoch 60/256
3063/3063 - 29s - loss: 0.2260 - val_loss: 0.2420
Epoch 61/256
3063/3063 - 29s - loss: 0.2262 - val_loss: 0.2511
Epoch 62/256
3063/3063 - 29s - loss: 0.2249 - val_loss: 0.2711
Epoch 63/256
3063/3063 - 29s - loss: 0.2245 - val_loss: 0.2647
Epoch 64/256
3063/3063 - 29s - loss: 0.2235 - val_loss: 0.2454
Epoch 65/256
3063/3063 - 29s - loss: 0.2224 - val_loss: 0.2541
Epoch 66/256
3063/3063 - 29s - loss: 0.2218 - val_loss: 0.2515
Epoch 67/256
3063/3063 - 29s - loss: 0.2224 - val_loss: 0.2515
Epoch 68/256
3063/3063 - 29s - loss: 0.2218 - val_loss: 0.2469
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 16)_64
			 16 [0.9633055287977732, 0.9603896235975267, 0.9621716342452359, 0.9624646564453913, 0.960078016222588, 0.9614984379141983, 0.9621526690668789, 0.9641497206258172]
		LATENT DIM 32
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.4020 - val_loss: 0.3629
Epoch 2/256
3063/3063 - 29s - loss: 0.3614 - val_loss: 0.3636
Epoch 3/256
3063/3063 - 29s - loss: 0.3546 - val_loss: 0.3475
Epoch 4/256
3063/3063 - 29s - loss: 0.3485 - val_loss: 0.3558
Epoch 5/256
3063/3063 - 29s - loss: 0.3451 - val_loss: 0.3421
Epoch 6/256
3063/3063 - 29s - loss: 0.3425 - val_loss: 0.3398
Epoch 7/256
3063/3063 - 29s - loss: 0.3390 - val_loss: 0.3411
Epoch 8/256
3063/3063 - 29s - loss: 0.3363 - val_loss: 0.3343
Epoch 9/256
3063/3063 - 29s - loss: 0.3345 - val_loss: 0.3322
Epoch 10/256
3063/3063 - 29s - loss: 0.3307 - val_loss: 0.3334
Epoch 11/256
3063/3063 - 29s - loss: 0.3259 - val_loss: 0.3286
Epoch 12/256
3063/3063 - 29s - loss: 0.3201 - val_loss: 0.3184
Epoch 13/256
3063/3063 - 29s - loss: 0.3134 - val_loss: 0.3042
Epoch 14/256
3063/3063 - 29s - loss: 0.3048 - val_loss: 0.3043
Epoch 15/256
3063/3063 - 29s - loss: 0.2967 - val_loss: 0.2996
Epoch 16/256
3063/3063 - 29s - loss: 0.2891 - val_loss: 0.2876
Epoch 17/256
3063/3063 - 29s - loss: 0.2847 - val_loss: 0.2886
Epoch 18/256
3063/3063 - 29s - loss: 0.2806 - val_loss: 0.2792
Epoch 19/256
3063/3063 - 29s - loss: 0.2766 - val_loss: 0.2879
Epoch 20/256
3063/3063 - 29s - loss: 0.2737 - val_loss: 0.2721
Epoch 21/256
3063/3063 - 29s - loss: 0.2714 - val_loss: 0.2875
Epoch 22/256
3063/3063 - 29s - loss: 0.2695 - val_loss: 0.3205
Epoch 23/256
3063/3063 - 29s - loss: 0.2660 - val_loss: 0.2732
Epoch 24/256
3063/3063 - 29s - loss: 0.2643 - val_loss: 0.2657
Epoch 25/256
3063/3063 - 29s - loss: 0.2621 - val_loss: 0.2687
Epoch 26/256
3063/3063 - 29s - loss: 0.2602 - val_loss: 0.2684
Epoch 27/256
3063/3063 - 29s - loss: 0.2581 - val_loss: 0.2858
Epoch 28/256
3063/3063 - 29s - loss: 0.2567 - val_loss: 0.2612
Epoch 29/256
3063/3063 - 29s - loss: 0.2554 - val_loss: 0.2556
Epoch 30/256
3063/3063 - 29s - loss: 0.2540 - val_loss: 0.2697
Epoch 31/256
3063/3063 - 29s - loss: 0.2519 - val_loss: 0.2650
Epoch 32/256
3063/3063 - 29s - loss: 0.2510 - val_loss: 0.2652
Epoch 33/256
3063/3063 - 29s - loss: 0.2498 - val_loss: 0.2560
Epoch 34/256
3063/3063 - 29s - loss: 0.2478 - val_loss: 0.2563
Epoch 35/256
3063/3063 - 29s - loss: 0.2464 - val_loss: 0.2616
Epoch 36/256
3063/3063 - 29s - loss: 0.2456 - val_loss: 0.2532
Epoch 37/256
3063/3063 - 29s - loss: 0.2444 - val_loss: 0.2549
Epoch 38/256
3063/3063 - 29s - loss: 0.2428 - val_loss: 0.2601
Epoch 39/256
3063/3063 - 29s - loss: 0.2411 - val_loss: 0.2716
Epoch 40/256
3063/3063 - 29s - loss: 0.2406 - val_loss: 0.2556
Epoch 41/256
3063/3063 - 29s - loss: 0.2390 - val_loss: 0.2567
Epoch 42/256
3063/3063 - 29s - loss: 0.2385 - val_loss: 0.2536
Epoch 43/256
3063/3063 - 29s - loss: 0.2366 - val_loss: 0.2496
Epoch 44/256
3063/3063 - 29s - loss: 0.2361 - val_loss: 0.2499
Epoch 45/256
3063/3063 - 29s - loss: 0.2359 - val_loss: 0.2543
Epoch 46/256
3063/3063 - 29s - loss: 0.2340 - val_loss: 0.2547
Epoch 47/256
3063/3063 - 29s - loss: 0.2327 - val_loss: 0.3148
Epoch 48/256
3063/3063 - 29s - loss: 0.2319 - val_loss: 0.2505
Epoch 49/256
3063/3063 - 29s - loss: 0.2306 - val_loss: 0.2623
Epoch 50/256
3063/3063 - 29s - loss: 0.2299 - val_loss: 0.2496
Epoch 51/256
3063/3063 - 29s - loss: 0.2293 - val_loss: 0.2723
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 32)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.3967 - val_loss: 0.3639
Epoch 2/256
3063/3063 - 29s - loss: 0.3624 - val_loss: 0.3510
Epoch 3/256
3063/3063 - 29s - loss: 0.3541 - val_loss: 0.3609
Epoch 4/256
3063/3063 - 29s - loss: 0.3493 - val_loss: 0.3510
Epoch 5/256
3063/3063 - 29s - loss: 0.3459 - val_loss: 0.3414
Epoch 6/256
3063/3063 - 29s - loss: 0.3432 - val_loss: 0.3542
Epoch 7/256
3063/3063 - 29s - loss: 0.3408 - val_loss: 0.3423
Epoch 8/256
3063/3063 - 29s - loss: 0.3389 - val_loss: 0.3356
Epoch 9/256
3063/3063 - 29s - loss: 0.3363 - val_loss: 0.3344
Epoch 10/256
3063/3063 - 29s - loss: 0.3335 - val_loss: 0.3328
Epoch 11/256
3063/3063 - 29s - loss: 0.3326 - val_loss: 0.3282
Epoch 12/256
3063/3063 - 29s - loss: 0.3300 - val_loss: 0.3275
Epoch 13/256
3063/3063 - 29s - loss: 0.3265 - val_loss: 0.3229
Epoch 14/256
3063/3063 - 29s - loss: 0.3222 - val_loss: 0.3236
Epoch 15/256
3063/3063 - 29s - loss: 0.3146 - val_loss: 0.3125
Epoch 16/256
3063/3063 - 29s - loss: 0.3058 - val_loss: 0.3025
Epoch 17/256
3063/3063 - 34s - loss: 0.2982 - val_loss: 0.2949
Epoch 18/256
3063/3063 - 29s - loss: 0.2916 - val_loss: 0.3018
Epoch 19/256
3063/3063 - 29s - loss: 0.2865 - val_loss: 0.2870
Epoch 20/256
3063/3063 - 29s - loss: 0.2806 - val_loss: 0.2818
Epoch 21/256
3063/3063 - 29s - loss: 0.2778 - val_loss: 0.2901
Epoch 22/256
3063/3063 - 29s - loss: 0.2752 - val_loss: 0.2707
Epoch 23/256
3063/3063 - 29s - loss: 0.2712 - val_loss: 0.2734
Epoch 24/256
3063/3063 - 29s - loss: 0.2686 - val_loss: 0.2764
Epoch 25/256
3063/3063 - 29s - loss: 0.2665 - val_loss: 0.2657
Epoch 26/256
3063/3063 - 29s - loss: 0.2636 - val_loss: 0.2638
Epoch 27/256
3063/3063 - 29s - loss: 0.2618 - val_loss: 0.2621
Epoch 28/256
3063/3063 - 29s - loss: 0.2597 - val_loss: 0.2775
Epoch 29/256
3063/3063 - 29s - loss: 0.2574 - val_loss: 0.2630
Epoch 30/256
3063/3063 - 29s - loss: 0.2549 - val_loss: 0.2651
Epoch 31/256
3063/3063 - 29s - loss: 0.2537 - val_loss: 0.2683
Epoch 32/256
3063/3063 - 29s - loss: 0.2535 - val_loss: 0.2563
Epoch 33/256
3063/3063 - 29s - loss: 0.2521 - val_loss: 0.2680
Epoch 34/256
3063/3063 - 29s - loss: 0.2506 - val_loss: 0.2556
Epoch 35/256
3063/3063 - 29s - loss: 0.2492 - val_loss: 0.2582
Epoch 36/256
3063/3063 - 29s - loss: 0.2469 - val_loss: 0.2545
Epoch 37/256
3063/3063 - 29s - loss: 0.2455 - val_loss: 0.2631
Epoch 38/256
3063/3063 - 29s - loss: 0.2460 - val_loss: 0.2517
Epoch 39/256
3063/3063 - 29s - loss: 0.2433 - val_loss: 0.2598
Epoch 40/256
3063/3063 - 29s - loss: 0.2422 - val_loss: 0.2554
Epoch 41/256
3063/3063 - 29s - loss: 0.2415 - val_loss: 0.2591
Epoch 42/256
3063/3063 - 29s - loss: 0.2411 - val_loss: 0.2464
Epoch 43/256
3063/3063 - 29s - loss: 0.2385 - val_loss: 0.2527
Epoch 44/256
3063/3063 - 29s - loss: 0.2382 - val_loss: 0.2571
Epoch 45/256
3063/3063 - 29s - loss: 0.2371 - val_loss: 0.2549
Epoch 46/256
3063/3063 - 29s - loss: 0.2360 - val_loss: 0.2511
Epoch 47/256
3063/3063 - 29s - loss: 0.2361 - val_loss: 0.2545
Epoch 48/256
3063/3063 - 29s - loss: 0.2342 - val_loss: 0.2513
Epoch 49/256
3063/3063 - 29s - loss: 0.2332 - val_loss: 0.2599
Epoch 50/256
3063/3063 - 29s - loss: 0.2325 - val_loss: 0.2497
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 32)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.3941 - val_loss: 0.3645
Epoch 2/256
3063/3063 - 29s - loss: 0.3599 - val_loss: 0.3542
Epoch 3/256
3063/3063 - 29s - loss: 0.3514 - val_loss: 0.3467
Epoch 4/256
3063/3063 - 29s - loss: 0.3474 - val_loss: 0.3425
Epoch 5/256
3063/3063 - 29s - loss: 0.3432 - val_loss: 0.3373
Epoch 6/256
3063/3063 - 29s - loss: 0.3399 - val_loss: 0.3439
Epoch 7/256
3063/3063 - 29s - loss: 0.3377 - val_loss: 0.3329
Epoch 8/256
3063/3063 - 29s - loss: 0.3332 - val_loss: 0.3305
Epoch 9/256
3063/3063 - 29s - loss: 0.3306 - val_loss: 0.3361
Epoch 10/256
3063/3063 - 29s - loss: 0.3246 - val_loss: 0.3175
Epoch 11/256
3063/3063 - 29s - loss: 0.3168 - val_loss: 0.3275
Epoch 12/256
3063/3063 - 29s - loss: 0.3106 - val_loss: 0.3132
Epoch 13/256
3063/3063 - 29s - loss: 0.3031 - val_loss: 0.2995
Epoch 14/256
3063/3063 - 29s - loss: 0.2969 - val_loss: 0.2963
Epoch 15/256
3063/3063 - 29s - loss: 0.2926 - val_loss: 0.2856
Epoch 16/256
3063/3063 - 29s - loss: 0.2873 - val_loss: 0.2935
Epoch 17/256
3063/3063 - 29s - loss: 0.2837 - val_loss: 0.2817
Epoch 18/256
3063/3063 - 29s - loss: 0.2798 - val_loss: 0.2971
Epoch 19/256
3063/3063 - 29s - loss: 0.2783 - val_loss: 0.2837
Epoch 20/256
3063/3063 - 29s - loss: 0.2754 - val_loss: 0.2778
Epoch 21/256
3063/3063 - 29s - loss: 0.2724 - val_loss: 0.2880
Epoch 22/256
3063/3063 - 29s - loss: 0.2695 - val_loss: 0.3093
Epoch 23/256
3063/3063 - 29s - loss: 0.2680 - val_loss: 0.2733
Epoch 24/256
3063/3063 - 30s - loss: 0.2660 - val_loss: 0.2668
Epoch 25/256
3063/3063 - 29s - loss: 0.2639 - val_loss: 0.2679
Epoch 26/256
3063/3063 - 30s - loss: 0.2625 - val_loss: 0.2681
Epoch 27/256
3063/3063 - 29s - loss: 0.2610 - val_loss: 0.2755
Epoch 28/256
3063/3063 - 29s - loss: 0.2590 - val_loss: 0.2681
Epoch 29/256
3063/3063 - 29s - loss: 0.2576 - val_loss: 0.2790
Epoch 30/256
3063/3063 - 29s - loss: 0.2568 - val_loss: 0.2649
Epoch 31/256
3063/3063 - 29s - loss: 0.2558 - val_loss: 0.2679
Epoch 32/256
3063/3063 - 29s - loss: 0.2533 - val_loss: 0.2768
Epoch 33/256
3063/3063 - 29s - loss: 0.2524 - val_loss: 0.2725
Epoch 34/256
3063/3063 - 29s - loss: 0.2516 - val_loss: 0.2645
Epoch 35/256
3063/3063 - 29s - loss: 0.2500 - val_loss: 0.2617
Epoch 36/256
3063/3063 - 30s - loss: 0.2490 - val_loss: 0.2799
Epoch 37/256
3063/3063 - 29s - loss: 0.2478 - val_loss: 0.2754
Epoch 38/256
3063/3063 - 29s - loss: 0.2458 - val_loss: 0.2649
Epoch 39/256
3063/3063 - 29s - loss: 0.2447 - val_loss: 0.2643
Epoch 40/256
3063/3063 - 30s - loss: 0.2440 - val_loss: 0.2533
Epoch 41/256
3063/3063 - 29s - loss: 0.2439 - val_loss: 0.2695
Epoch 42/256
3063/3063 - 29s - loss: 0.2420 - val_loss: 0.2754
Epoch 43/256
3063/3063 - 30s - loss: 0.2416 - val_loss: 0.2627
Epoch 44/256
3063/3063 - 29s - loss: 0.2388 - val_loss: 0.2564
Epoch 45/256
3063/3063 - 29s - loss: 0.2385 - val_loss: 0.2520
Epoch 46/256
3063/3063 - 29s - loss: 0.2381 - val_loss: 0.2565
Epoch 47/256
3063/3063 - 29s - loss: 0.2361 - val_loss: 0.2514
Epoch 48/256
3063/3063 - 29s - loss: 0.2356 - val_loss: 0.2538
Epoch 49/256
3063/3063 - 29s - loss: 0.2349 - val_loss: 0.2606
Epoch 50/256
3063/3063 - 29s - loss: 0.2338 - val_loss: 0.2552
Epoch 51/256
3063/3063 - 29s - loss: 0.2333 - val_loss: 0.2625
Epoch 52/256
3063/3063 - 29s - loss: 0.2315 - val_loss: 0.2544
Epoch 53/256
3063/3063 - 29s - loss: 0.2316 - val_loss: 0.2484
Epoch 54/256
3063/3063 - 29s - loss: 0.2288 - val_loss: 0.2644
Epoch 55/256
3063/3063 - 29s - loss: 0.2286 - val_loss: 0.2481
Epoch 56/256
3063/3063 - 29s - loss: 0.2279 - val_loss: 0.2483
Epoch 57/256
3063/3063 - 29s - loss: 0.2270 - val_loss: 0.2714
Epoch 58/256
3063/3063 - 29s - loss: 0.2263 - val_loss: 0.2532
Epoch 59/256
3063/3063 - 29s - loss: 0.2252 - val_loss: 0.2531
Epoch 60/256
3063/3063 - 29s - loss: 0.2248 - val_loss: 0.2525
Epoch 61/256
3063/3063 - 29s - loss: 0.2236 - val_loss: 0.2512
Epoch 62/256
3063/3063 - 30s - loss: 0.2231 - val_loss: 0.2498
Epoch 63/256
3063/3063 - 30s - loss: 0.2232 - val_loss: 0.2504
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 32)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.3962 - val_loss: 0.3704
Epoch 2/256
3063/3063 - 30s - loss: 0.3632 - val_loss: 0.3515
Epoch 3/256
3063/3063 - 29s - loss: 0.3548 - val_loss: 0.3532
Epoch 4/256
3063/3063 - 30s - loss: 0.3487 - val_loss: 0.3433
Epoch 5/256
3063/3063 - 30s - loss: 0.3467 - val_loss: 0.3418
Epoch 6/256
3063/3063 - 30s - loss: 0.3429 - val_loss: 0.3380
Epoch 7/256
3063/3063 - 30s - loss: 0.3406 - val_loss: 0.3422
Epoch 8/256
3063/3063 - 30s - loss: 0.3389 - val_loss: 0.3379
Epoch 9/256
3063/3063 - 30s - loss: 0.3359 - val_loss: 0.3449
Epoch 10/256
3063/3063 - 30s - loss: 0.3337 - val_loss: 0.3364
Epoch 11/256
3063/3063 - 30s - loss: 0.3314 - val_loss: 0.3312
Epoch 12/256
3063/3063 - 30s - loss: 0.3262 - val_loss: 0.3215
Epoch 13/256
3063/3063 - 30s - loss: 0.3230 - val_loss: 0.3466
Epoch 14/256
3063/3063 - 30s - loss: 0.3179 - val_loss: 0.3147
Epoch 15/256
3063/3063 - 30s - loss: 0.3100 - val_loss: 0.3072
Epoch 16/256
3063/3063 - 30s - loss: 0.3003 - val_loss: 0.3076
Epoch 17/256
3063/3063 - 30s - loss: 0.2936 - val_loss: 0.2944
Epoch 18/256
3063/3063 - 30s - loss: 0.2869 - val_loss: 0.2833
Epoch 19/256
3063/3063 - 30s - loss: 0.2819 - val_loss: 0.2901
Epoch 20/256
3063/3063 - 30s - loss: 0.2785 - val_loss: 0.2899
Epoch 21/256
3063/3063 - 30s - loss: 0.2768 - val_loss: 0.2739
Epoch 22/256
3063/3063 - 30s - loss: 0.2724 - val_loss: 0.2776
Epoch 23/256
3063/3063 - 30s - loss: 0.2708 - val_loss: 0.2758
Epoch 24/256
3063/3063 - 30s - loss: 0.2689 - val_loss: 0.2908
Epoch 25/256
3063/3063 - 30s - loss: 0.2662 - val_loss: 0.2804
Epoch 26/256
3063/3063 - 30s - loss: 0.2642 - val_loss: 0.2686
Epoch 27/256
3063/3063 - 30s - loss: 0.2632 - val_loss: 0.3020
Epoch 28/256
3063/3063 - 30s - loss: 0.2606 - val_loss: 0.2683
Epoch 29/256
3063/3063 - 30s - loss: 0.2588 - val_loss: 0.2648
Epoch 30/256
3063/3063 - 30s - loss: 0.2576 - val_loss: 0.2622
Epoch 31/256
3063/3063 - 30s - loss: 0.2566 - val_loss: 0.2666
Epoch 32/256
3063/3063 - 30s - loss: 0.2548 - val_loss: 0.2625
Epoch 33/256
3063/3063 - 30s - loss: 0.2552 - val_loss: 0.2652
Epoch 34/256
3063/3063 - 30s - loss: 0.2524 - val_loss: 0.2589
Epoch 35/256
3063/3063 - 30s - loss: 0.2521 - val_loss: 0.2597
Epoch 36/256
3063/3063 - 30s - loss: 0.2498 - val_loss: 0.2622
Epoch 37/256
3063/3063 - 30s - loss: 0.2484 - val_loss: 0.2560
Epoch 38/256
3063/3063 - 30s - loss: 0.2477 - val_loss: 0.2638
Epoch 39/256
3063/3063 - 30s - loss: 0.2471 - val_loss: 0.2620
Epoch 40/256
3063/3063 - 31s - loss: 0.2450 - val_loss: 0.2558
Epoch 41/256
3063/3063 - 31s - loss: 0.2446 - val_loss: 0.2760
Epoch 42/256
3063/3063 - 30s - loss: 0.2424 - val_loss: 0.2564
Epoch 43/256
3063/3063 - 30s - loss: 0.2419 - val_loss: 0.2540
Epoch 44/256
3063/3063 - 30s - loss: 0.2408 - val_loss: 0.2574
Epoch 45/256
3063/3063 - 30s - loss: 0.2404 - val_loss: 0.2560
Epoch 46/256
3063/3063 - 30s - loss: 0.2389 - val_loss: 0.2507
Epoch 47/256
3063/3063 - 30s - loss: 0.2375 - val_loss: 0.2667
Epoch 48/256
3063/3063 - 30s - loss: 0.2365 - val_loss: 0.2688
Epoch 49/256
3063/3063 - 30s - loss: 0.2364 - val_loss: 0.2521
Epoch 50/256
3063/3063 - 30s - loss: 0.2355 - val_loss: 0.2541
Epoch 51/256
3063/3063 - 30s - loss: 0.2337 - val_loss: 0.2583
Epoch 52/256
3063/3063 - 30s - loss: 0.2337 - val_loss: 0.2500
Epoch 53/256
3063/3063 - 30s - loss: 0.2327 - val_loss: 0.2543
Epoch 54/256
3063/3063 - 30s - loss: 0.2305 - val_loss: 0.2495
Epoch 55/256
3063/3063 - 30s - loss: 0.2299 - val_loss: 0.2485
Epoch 56/256
3063/3063 - 30s - loss: 0.2298 - val_loss: 0.2495
Epoch 57/256
3063/3063 - 30s - loss: 0.2295 - val_loss: 0.2510
Epoch 58/256
3063/3063 - 30s - loss: 0.2278 - val_loss: 0.2455
Epoch 59/256
3063/3063 - 30s - loss: 0.2272 - val_loss: 0.2558
Epoch 60/256
3063/3063 - 30s - loss: 0.2258 - val_loss: 0.2475
Epoch 61/256
3063/3063 - 30s - loss: 0.2245 - val_loss: 0.2555
Epoch 62/256
3063/3063 - 30s - loss: 0.2242 - val_loss: 0.2472
Epoch 63/256
3063/3063 - 30s - loss: 0.2232 - val_loss: 0.2488
Epoch 64/256
3063/3063 - 30s - loss: 0.2230 - val_loss: 0.2625
Epoch 65/256
3063/3063 - 30s - loss: 0.2225 - val_loss: 0.2459
Epoch 66/256
3063/3063 - 30s - loss: 0.2207 - val_loss: 0.2500
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 32)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 32s - loss: 0.3954 - val_loss: 0.3607
Epoch 2/256
3063/3063 - 30s - loss: 0.3631 - val_loss: 0.3552
Epoch 3/256
3063/3063 - 30s - loss: 0.3542 - val_loss: 0.3489
Epoch 4/256
3063/3063 - 30s - loss: 0.3488 - val_loss: 0.3573
Epoch 5/256
3063/3063 - 30s - loss: 0.3459 - val_loss: 0.3418
Epoch 6/256
3063/3063 - 30s - loss: 0.3427 - val_loss: 0.3660
Epoch 7/256
3063/3063 - 30s - loss: 0.3421 - val_loss: 0.3539
Epoch 8/256
3063/3063 - 30s - loss: 0.3391 - val_loss: 0.3382
Epoch 9/256
3063/3063 - 30s - loss: 0.3369 - val_loss: 0.3387
Epoch 10/256
3063/3063 - 30s - loss: 0.3337 - val_loss: 0.3358
Epoch 11/256
3063/3063 - 31s - loss: 0.3302 - val_loss: 0.3330
Epoch 12/256
3063/3063 - 30s - loss: 0.3260 - val_loss: 0.3213
Epoch 13/256
3063/3063 - 30s - loss: 0.3204 - val_loss: 0.3273
Epoch 14/256
3063/3063 - 30s - loss: 0.3135 - val_loss: 0.3201
Epoch 15/256
3063/3063 - 30s - loss: 0.3074 - val_loss: 0.3147
Epoch 16/256
3063/3063 - 30s - loss: 0.3013 - val_loss: 0.3085
Epoch 17/256
3063/3063 - 30s - loss: 0.2957 - val_loss: 0.2931
Epoch 18/256
3063/3063 - 30s - loss: 0.2908 - val_loss: 0.2981
Epoch 19/256
3063/3063 - 31s - loss: 0.2870 - val_loss: 0.3523
Epoch 20/256
3063/3063 - 31s - loss: 0.2829 - val_loss: 0.2856
Epoch 21/256
3063/3063 - 31s - loss: 0.2793 - val_loss: 0.2771
Epoch 22/256
3063/3063 - 31s - loss: 0.2759 - val_loss: 0.2789
Epoch 23/256
3063/3063 - 31s - loss: 0.2735 - val_loss: 0.2766
Epoch 24/256
3063/3063 - 30s - loss: 0.2710 - val_loss: 0.2784
Epoch 25/256
3063/3063 - 31s - loss: 0.2693 - val_loss: 0.2687
Epoch 26/256
3063/3063 - 30s - loss: 0.2665 - val_loss: 0.2745
Epoch 27/256
3063/3063 - 30s - loss: 0.2650 - val_loss: 0.2732
Epoch 28/256
3063/3063 - 30s - loss: 0.2618 - val_loss: 0.2738
Epoch 29/256
3063/3063 - 31s - loss: 0.2599 - val_loss: 0.2652
Epoch 30/256
3063/3063 - 31s - loss: 0.2583 - val_loss: 0.2610
Epoch 31/256
3063/3063 - 30s - loss: 0.2568 - val_loss: 0.2612
Epoch 32/256
3063/3063 - 30s - loss: 0.2549 - val_loss: 0.2605
Epoch 33/256
3063/3063 - 31s - loss: 0.2531 - val_loss: 0.2681
Epoch 34/256
3063/3063 - 30s - loss: 0.2520 - val_loss: 0.2640
Epoch 35/256
3063/3063 - 30s - loss: 0.2502 - val_loss: 0.2695
Epoch 36/256
3063/3063 - 31s - loss: 0.2489 - val_loss: 0.2642
Epoch 37/256
3063/3063 - 31s - loss: 0.2479 - val_loss: 0.2671
Epoch 38/256
3063/3063 - 31s - loss: 0.2457 - val_loss: 0.2578
Epoch 39/256
3063/3063 - 31s - loss: 0.2455 - val_loss: 0.2714
Epoch 40/256
3063/3063 - 31s - loss: 0.2438 - val_loss: 0.2638
Epoch 41/256
3063/3063 - 31s - loss: 0.2417 - val_loss: 0.2530
Epoch 42/256
3063/3063 - 30s - loss: 0.2412 - val_loss: 0.2594
Epoch 43/256
3063/3063 - 31s - loss: 0.2400 - val_loss: 0.2620
Epoch 44/256
3063/3063 - 30s - loss: 0.2389 - val_loss: 0.2791
Epoch 45/256
3063/3063 - 31s - loss: 0.2384 - val_loss: 0.2705
Epoch 46/256
3063/3063 - 31s - loss: 0.2370 - val_loss: 0.2518
Epoch 47/256
3063/3063 - 31s - loss: 0.2364 - val_loss: 0.2608
Epoch 48/256
3063/3063 - 31s - loss: 0.2340 - val_loss: 0.2600
Epoch 49/256
3063/3063 - 31s - loss: 0.2335 - val_loss: 0.2584
Epoch 50/256
3063/3063 - 30s - loss: 0.2323 - val_loss: 0.2647
Epoch 51/256
3063/3063 - 31s - loss: 0.2328 - val_loss: 0.2506
Epoch 52/256
3063/3063 - 31s - loss: 0.2305 - val_loss: 0.2523
Epoch 53/256
3063/3063 - 31s - loss: 0.2299 - val_loss: 0.2809
Epoch 54/256
3063/3063 - 30s - loss: 0.2290 - val_loss: 0.2521
Epoch 55/256
3063/3063 - 30s - loss: 0.2283 - val_loss: 0.2555
Epoch 56/256
3063/3063 - 30s - loss: 0.2279 - val_loss: 0.2509
Epoch 57/256
3063/3063 - 30s - loss: 0.2267 - val_loss: 0.2636
Epoch 58/256
3063/3063 - 30s - loss: 0.2252 - val_loss: 0.2543
Epoch 59/256
3063/3063 - 30s - loss: 0.2251 - val_loss: 0.2543
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 32)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 32s - loss: 0.3966 - val_loss: 0.3579
Epoch 2/256
3063/3063 - 30s - loss: 0.3610 - val_loss: 0.3572
Epoch 3/256
3063/3063 - 30s - loss: 0.3520 - val_loss: 0.3552
Epoch 4/256
3063/3063 - 31s - loss: 0.3471 - val_loss: 0.3436
Epoch 5/256
3063/3063 - 31s - loss: 0.3428 - val_loss: 0.3423
Epoch 6/256
3063/3063 - 30s - loss: 0.3400 - val_loss: 0.3404
Epoch 7/256
3063/3063 - 30s - loss: 0.3359 - val_loss: 0.3470
Epoch 8/256
3063/3063 - 30s - loss: 0.3308 - val_loss: 0.3280
Epoch 9/256
3063/3063 - 30s - loss: 0.3249 - val_loss: 0.3232
Epoch 10/256
3063/3063 - 31s - loss: 0.3146 - val_loss: 0.3187
Epoch 11/256
3063/3063 - 31s - loss: 0.3057 - val_loss: 0.3343
Epoch 12/256
3063/3063 - 31s - loss: 0.2976 - val_loss: 0.2993
Epoch 13/256
3063/3063 - 30s - loss: 0.2914 - val_loss: 0.2950
Epoch 14/256
3063/3063 - 31s - loss: 0.2866 - val_loss: 0.3028
Epoch 15/256
3063/3063 - 30s - loss: 0.2820 - val_loss: 0.2920
Epoch 16/256
3063/3063 - 31s - loss: 0.2785 - val_loss: 0.2824
Epoch 17/256
3063/3063 - 30s - loss: 0.2759 - val_loss: 0.2768
Epoch 18/256
3063/3063 - 31s - loss: 0.2743 - val_loss: 0.2774
Epoch 19/256
3063/3063 - 30s - loss: 0.2716 - val_loss: 0.2731
Epoch 20/256
3063/3063 - 31s - loss: 0.2697 - val_loss: 0.2766
Epoch 21/256
3063/3063 - 31s - loss: 0.2675 - val_loss: 0.2684
Epoch 22/256
3063/3063 - 31s - loss: 0.2648 - val_loss: 0.2730
Epoch 23/256
3063/3063 - 31s - loss: 0.2639 - val_loss: 0.2754
Epoch 24/256
3063/3063 - 31s - loss: 0.2634 - val_loss: 0.2681
Epoch 25/256
3063/3063 - 31s - loss: 0.2604 - val_loss: 0.2693
Epoch 26/256
3063/3063 - 31s - loss: 0.2598 - val_loss: 0.2695
Epoch 27/256
3063/3063 - 31s - loss: 0.2579 - val_loss: 0.2583
Epoch 28/256
3063/3063 - 31s - loss: 0.2577 - val_loss: 0.2657
Epoch 29/256
3063/3063 - 30s - loss: 0.2553 - val_loss: 0.2586
Epoch 30/256
3063/3063 - 31s - loss: 0.2540 - val_loss: 0.2689
Epoch 31/256
3063/3063 - 31s - loss: 0.2528 - val_loss: 0.2647
Epoch 32/256
3063/3063 - 31s - loss: 0.2506 - val_loss: 0.2611
Epoch 33/256
3063/3063 - 31s - loss: 0.2500 - val_loss: 0.2603
Epoch 34/256
3063/3063 - 31s - loss: 0.2499 - val_loss: 0.2563
Epoch 35/256
3063/3063 - 30s - loss: 0.2472 - val_loss: 0.2713
Epoch 36/256
3063/3063 - 31s - loss: 0.2477 - val_loss: 0.2586
Epoch 37/256
3063/3063 - 31s - loss: 0.2453 - val_loss: 0.2671
Epoch 38/256
3063/3063 - 31s - loss: 0.2447 - val_loss: 0.2583
Epoch 39/256
3063/3063 - 31s - loss: 0.2433 - val_loss: 0.2612
Epoch 40/256
3063/3063 - 31s - loss: 0.2422 - val_loss: 0.2535
Epoch 41/256
3063/3063 - 30s - loss: 0.2417 - val_loss: 0.2662
Epoch 42/256
3063/3063 - 31s - loss: 0.2413 - val_loss: 0.2606
Epoch 43/256
3063/3063 - 30s - loss: 0.2393 - val_loss: 0.2575
Epoch 44/256
3063/3063 - 30s - loss: 0.2392 - val_loss: 0.2621
Epoch 45/256
3063/3063 - 30s - loss: 0.2372 - val_loss: 0.2496
Epoch 46/256
3063/3063 - 31s - loss: 0.2365 - val_loss: 0.2616
Epoch 47/256
3063/3063 - 30s - loss: 0.2366 - val_loss: 0.2558
Epoch 48/256
3063/3063 - 30s - loss: 0.2344 - val_loss: 0.2563
Epoch 49/256
3063/3063 - 30s - loss: 0.2340 - val_loss: 0.2733
Epoch 50/256
3063/3063 - 30s - loss: 0.2331 - val_loss: 0.2519
Epoch 51/256
3063/3063 - 30s - loss: 0.2319 - val_loss: 0.2464
Epoch 52/256
3063/3063 - 31s - loss: 0.2321 - val_loss: 0.2667
Epoch 53/256
3063/3063 - 30s - loss: 0.2307 - val_loss: 0.2479
Epoch 54/256
3063/3063 - 30s - loss: 0.2287 - val_loss: 0.2438
Epoch 55/256
3063/3063 - 30s - loss: 0.2280 - val_loss: 0.2473
Epoch 56/256
3063/3063 - 30s - loss: 0.2271 - val_loss: 0.2693
Epoch 57/256
3063/3063 - 35s - loss: 0.2263 - val_loss: 0.2526
Epoch 58/256
3063/3063 - 30s - loss: 0.2260 - val_loss: 0.2651
Epoch 59/256
3063/3063 - 29s - loss: 0.2249 - val_loss: 0.2601
Epoch 60/256
3063/3063 - 29s - loss: 0.2241 - val_loss: 0.2504
Epoch 61/256
3063/3063 - 29s - loss: 0.2236 - val_loss: 0.2596
Epoch 62/256
3063/3063 - 29s - loss: 0.2220 - val_loss: 0.2573
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 32)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 30s - loss: 0.3973 - val_loss: 0.3567
Epoch 2/256
3063/3063 - 29s - loss: 0.3620 - val_loss: 0.3551
Epoch 3/256
3063/3063 - 29s - loss: 0.3521 - val_loss: 0.3460
Epoch 4/256
3063/3063 - 29s - loss: 0.3482 - val_loss: 0.3451
Epoch 5/256
3063/3063 - 29s - loss: 0.3441 - val_loss: 0.3471
Epoch 6/256
3063/3063 - 29s - loss: 0.3406 - val_loss: 0.3353
Epoch 7/256
3063/3063 - 29s - loss: 0.3393 - val_loss: 0.3386
Epoch 8/256
3063/3063 - 29s - loss: 0.3365 - val_loss: 0.3308
Epoch 9/256
3063/3063 - 29s - loss: 0.3324 - val_loss: 0.3460
Epoch 10/256
3063/3063 - 29s - loss: 0.3281 - val_loss: 0.3286
Epoch 11/256
3063/3063 - 29s - loss: 0.3235 - val_loss: 0.3208
Epoch 12/256
3063/3063 - 29s - loss: 0.3172 - val_loss: 0.3145
Epoch 13/256
3063/3063 - 29s - loss: 0.3127 - val_loss: 0.3137
Epoch 14/256
3063/3063 - 29s - loss: 0.3068 - val_loss: 0.3026
Epoch 15/256
3063/3063 - 29s - loss: 0.3019 - val_loss: 0.2975
Epoch 16/256
3063/3063 - 29s - loss: 0.2955 - val_loss: 0.3151
Epoch 17/256
3063/3063 - 29s - loss: 0.2905 - val_loss: 0.2960
Epoch 18/256
3063/3063 - 29s - loss: 0.2866 - val_loss: 0.3093
Epoch 19/256
3063/3063 - 29s - loss: 0.2834 - val_loss: 0.2887
Epoch 20/256
3063/3063 - 29s - loss: 0.2803 - val_loss: 0.2968
Epoch 21/256
3063/3063 - 29s - loss: 0.2755 - val_loss: 0.2926
Epoch 22/256
3063/3063 - 29s - loss: 0.2750 - val_loss: 0.2788
Epoch 23/256
3063/3063 - 34s - loss: 0.2712 - val_loss: 0.2796
Epoch 24/256
3063/3063 - 29s - loss: 0.2690 - val_loss: 0.2726
Epoch 25/256
3063/3063 - 29s - loss: 0.2666 - val_loss: 0.2761
Epoch 26/256
3063/3063 - 29s - loss: 0.2647 - val_loss: 0.2681
Epoch 27/256
3063/3063 - 29s - loss: 0.2621 - val_loss: 0.2667
Epoch 28/256
3063/3063 - 29s - loss: 0.2619 - val_loss: 0.2664
Epoch 29/256
3063/3063 - 29s - loss: 0.2591 - val_loss: 0.2724
Epoch 30/256
3063/3063 - 29s - loss: 0.2583 - val_loss: 0.2608
Epoch 31/256
3063/3063 - 29s - loss: 0.2560 - val_loss: 0.2626
Epoch 32/256
3063/3063 - 29s - loss: 0.2546 - val_loss: 0.2684
Epoch 33/256
3063/3063 - 29s - loss: 0.2537 - val_loss: 0.2658
Epoch 34/256
3063/3063 - 29s - loss: 0.2521 - val_loss: 0.2610
Epoch 35/256
3063/3063 - 29s - loss: 0.2510 - val_loss: 0.2716
Epoch 36/256
3063/3063 - 29s - loss: 0.2501 - val_loss: 0.2622
Epoch 37/256
3063/3063 - 29s - loss: 0.2477 - val_loss: 0.2636
Epoch 38/256
3063/3063 - 29s - loss: 0.2465 - val_loss: 0.2547
Epoch 39/256
3063/3063 - 29s - loss: 0.2457 - val_loss: 0.2595
Epoch 40/256
3063/3063 - 29s - loss: 0.2453 - val_loss: 0.2633
Epoch 41/256
3063/3063 - 29s - loss: 0.2441 - val_loss: 0.2643
Epoch 42/256
3063/3063 - 29s - loss: 0.2417 - val_loss: 0.2557
Epoch 43/256
3063/3063 - 29s - loss: 0.2405 - val_loss: 0.2605
Epoch 44/256
3063/3063 - 29s - loss: 0.2400 - val_loss: 0.2580
Epoch 45/256
3063/3063 - 29s - loss: 0.2386 - val_loss: 0.2721
Epoch 46/256
3063/3063 - 29s - loss: 0.2373 - val_loss: 0.2560
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 32)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 30s - loss: 0.3927 - val_loss: 0.3508
Epoch 2/256
3063/3063 - 29s - loss: 0.3620 - val_loss: 0.3474
Epoch 3/256
3063/3063 - 29s - loss: 0.3531 - val_loss: 0.3444
Epoch 4/256
3063/3063 - 29s - loss: 0.3473 - val_loss: 0.3456
Epoch 5/256
3063/3063 - 29s - loss: 0.3436 - val_loss: 0.3385
Epoch 6/256
3063/3063 - 29s - loss: 0.3395 - val_loss: 0.3345
Epoch 7/256
3063/3063 - 29s - loss: 0.3363 - val_loss: 0.3329
Epoch 8/256
3063/3063 - 29s - loss: 0.3293 - val_loss: 0.3256
Epoch 9/256
3063/3063 - 29s - loss: 0.3234 - val_loss: 0.3308
Epoch 10/256
3063/3063 - 29s - loss: 0.3157 - val_loss: 0.3078
Epoch 11/256
3063/3063 - 29s - loss: 0.3065 - val_loss: 0.3020
Epoch 12/256
3063/3063 - 29s - loss: 0.2999 - val_loss: 0.2946
Epoch 13/256
3063/3063 - 29s - loss: 0.2918 - val_loss: 0.2991
Epoch 14/256
3063/3063 - 29s - loss: 0.2878 - val_loss: 0.3040
Epoch 15/256
3063/3063 - 29s - loss: 0.2839 - val_loss: 0.2873
Epoch 16/256
3063/3063 - 29s - loss: 0.2795 - val_loss: 0.2910
Epoch 17/256
3063/3063 - 29s - loss: 0.2766 - val_loss: 0.2892
Epoch 18/256
3063/3063 - 29s - loss: 0.2743 - val_loss: 0.2711
Epoch 19/256
3063/3063 - 29s - loss: 0.2713 - val_loss: 0.2839
Epoch 20/256
3063/3063 - 29s - loss: 0.2687 - val_loss: 0.2978
Epoch 21/256
3063/3063 - 29s - loss: 0.2664 - val_loss: 0.2751
Epoch 22/256
3063/3063 - 29s - loss: 0.2647 - val_loss: 0.2784
Epoch 23/256
3063/3063 - 29s - loss: 0.2624 - val_loss: 0.2801
Epoch 24/256
3063/3063 - 29s - loss: 0.2611 - val_loss: 0.2653
Epoch 25/256
3063/3063 - 29s - loss: 0.2588 - val_loss: 0.2644
Epoch 26/256
3063/3063 - 29s - loss: 0.2580 - val_loss: 0.2587
Epoch 27/256
3063/3063 - 29s - loss: 0.2563 - val_loss: 0.2911
Epoch 28/256
3063/3063 - 29s - loss: 0.2551 - val_loss: 0.2648
Epoch 29/256
3063/3063 - 29s - loss: 0.2538 - val_loss: 0.2706
Epoch 30/256
3063/3063 - 29s - loss: 0.2510 - val_loss: 0.2733
Epoch 31/256
3063/3063 - 29s - loss: 0.2502 - val_loss: 0.2758
Epoch 32/256
3063/3063 - 29s - loss: 0.2489 - val_loss: 0.2558
Epoch 33/256
3063/3063 - 29s - loss: 0.2479 - val_loss: 0.2626
Epoch 34/256
3063/3063 - 29s - loss: 0.2449 - val_loss: 0.2558
Epoch 35/256
3063/3063 - 29s - loss: 0.2438 - val_loss: 0.2645
Epoch 36/256
3063/3063 - 29s - loss: 0.2425 - val_loss: 0.2588
Epoch 37/256
3063/3063 - 29s - loss: 0.2425 - val_loss: 0.2675
Epoch 38/256
3063/3063 - 29s - loss: 0.2412 - val_loss: 0.2532
Epoch 39/256
3063/3063 - 29s - loss: 0.2396 - val_loss: 0.2674
Epoch 40/256
3063/3063 - 29s - loss: 0.2375 - val_loss: 0.2555
Epoch 41/256
3063/3063 - 29s - loss: 0.2366 - val_loss: 0.2765
Epoch 42/256
3063/3063 - 29s - loss: 0.2366 - val_loss: 0.2514
Epoch 43/256
3063/3063 - 29s - loss: 0.2357 - val_loss: 0.2728
Epoch 44/256
3063/3063 - 29s - loss: 0.2334 - val_loss: 0.2759
Epoch 45/256
3063/3063 - 29s - loss: 0.2336 - val_loss: 0.2570
Epoch 46/256
3063/3063 - 29s - loss: 0.2309 - val_loss: 0.2635
Epoch 47/256
3063/3063 - 29s - loss: 0.2309 - val_loss: 0.2509
Epoch 48/256
3063/3063 - 29s - loss: 0.2303 - val_loss: 0.2550
Epoch 49/256
3063/3063 - 29s - loss: 0.2285 - val_loss: 0.2487
Epoch 50/256
3063/3063 - 29s - loss: 0.2275 - val_loss: 0.2611
Epoch 51/256
3063/3063 - 29s - loss: 0.2265 - val_loss: 0.2655
Epoch 52/256
3063/3063 - 29s - loss: 0.2252 - val_loss: 0.2518
Epoch 53/256
3063/3063 - 29s - loss: 0.2246 - val_loss: 0.2615
Epoch 54/256
3063/3063 - 29s - loss: 0.2242 - val_loss: 0.2465
Epoch 55/256
3063/3063 - 29s - loss: 0.2228 - val_loss: 0.2515
Epoch 56/256
3063/3063 - 29s - loss: 0.2226 - val_loss: 0.2449
Epoch 57/256
3063/3063 - 29s - loss: 0.2215 - val_loss: 0.2602
Epoch 58/256
3063/3063 - 29s - loss: 0.2214 - val_loss: 0.2521
Epoch 59/256
3063/3063 - 29s - loss: 0.2193 - val_loss: 0.2585
Epoch 60/256
3063/3063 - 29s - loss: 0.2188 - val_loss: 0.2408
Epoch 61/256
3063/3063 - 29s - loss: 0.2179 - val_loss: 0.2504
Epoch 62/256
3063/3063 - 29s - loss: 0.2172 - val_loss: 0.2643
Epoch 63/256
3063/3063 - 29s - loss: 0.2160 - val_loss: 0.2502
Epoch 64/256
3063/3063 - 29s - loss: 0.2158 - val_loss: 0.2411
Epoch 65/256
3063/3063 - 29s - loss: 0.2133 - val_loss: 0.2502
Epoch 66/256
3063/3063 - 29s - loss: 0.2132 - val_loss: 0.2449
Epoch 67/256
3063/3063 - 29s - loss: 0.2131 - val_loss: 0.2520
Epoch 68/256
3063/3063 - 29s - loss: 0.2129 - val_loss: 0.2461
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 32)_64
			 32 [0.9626029871907597, 0.9630114921436327, 0.962065614359289, 0.9633758400145664, 0.962027218442641, 0.9639849579652047, 0.9609312492467708, 0.9643892417672693]
		LATENT DIM 64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.3996 - val_loss: 0.3614
Epoch 2/256
3063/3063 - 30s - loss: 0.3611 - val_loss: 0.3771
Epoch 3/256
3063/3063 - 30s - loss: 0.3541 - val_loss: 0.3474
Epoch 4/256
3063/3063 - 30s - loss: 0.3484 - val_loss: 0.3501
Epoch 5/256
3063/3063 - 30s - loss: 0.3453 - val_loss: 0.3445
Epoch 6/256
3063/3063 - 30s - loss: 0.3420 - val_loss: 0.3375
Epoch 7/256
3063/3063 - 30s - loss: 0.3379 - val_loss: 0.3380
Epoch 8/256
3063/3063 - 30s - loss: 0.3351 - val_loss: 0.3323
Epoch 9/256
3063/3063 - 30s - loss: 0.3337 - val_loss: 0.3314
Epoch 10/256
3063/3063 - 30s - loss: 0.3306 - val_loss: 0.3321
Epoch 11/256
3063/3063 - 30s - loss: 0.3261 - val_loss: 0.3274
Epoch 12/256
3063/3063 - 30s - loss: 0.3197 - val_loss: 0.3219
Epoch 13/256
3063/3063 - 30s - loss: 0.3103 - val_loss: 0.3014
Epoch 14/256
3063/3063 - 30s - loss: 0.3004 - val_loss: 0.3140
Epoch 15/256
3063/3063 - 30s - loss: 0.2936 - val_loss: 0.3159
Epoch 16/256
3063/3063 - 30s - loss: 0.2892 - val_loss: 0.2917
Epoch 17/256
3063/3063 - 30s - loss: 0.2848 - val_loss: 0.2895
Epoch 18/256
3063/3063 - 30s - loss: 0.2808 - val_loss: 0.2851
Epoch 19/256
3063/3063 - 30s - loss: 0.2775 - val_loss: 0.2824
Epoch 20/256
3063/3063 - 30s - loss: 0.2751 - val_loss: 0.2757
Epoch 21/256
3063/3063 - 30s - loss: 0.2731 - val_loss: 0.2931
Epoch 22/256
3063/3063 - 30s - loss: 0.2715 - val_loss: 0.3287
Epoch 23/256
3063/3063 - 30s - loss: 0.2681 - val_loss: 0.2709
Epoch 24/256
3063/3063 - 30s - loss: 0.2666 - val_loss: 0.2842
Epoch 25/256
3063/3063 - 30s - loss: 0.2654 - val_loss: 0.2708
Epoch 26/256
3063/3063 - 30s - loss: 0.2630 - val_loss: 0.2709
Epoch 27/256
3063/3063 - 30s - loss: 0.2608 - val_loss: 0.2834
Epoch 28/256
3063/3063 - 30s - loss: 0.2602 - val_loss: 0.2651
Epoch 29/256
3063/3063 - 30s - loss: 0.2584 - val_loss: 0.2591
Epoch 30/256
3063/3063 - 30s - loss: 0.2558 - val_loss: 0.2640
Epoch 31/256
3063/3063 - 30s - loss: 0.2547 - val_loss: 0.2683
Epoch 32/256
3063/3063 - 30s - loss: 0.2537 - val_loss: 0.2672
Epoch 33/256
3063/3063 - 30s - loss: 0.2525 - val_loss: 0.2615
Epoch 34/256
3063/3063 - 30s - loss: 0.2501 - val_loss: 0.2606
Epoch 35/256
3063/3063 - 30s - loss: 0.2489 - val_loss: 0.2608
Epoch 36/256
3063/3063 - 30s - loss: 0.2478 - val_loss: 0.2754
Epoch 37/256
3063/3063 - 30s - loss: 0.2459 - val_loss: 0.2587
Epoch 38/256
3063/3063 - 30s - loss: 0.2448 - val_loss: 0.2739
Epoch 39/256
3063/3063 - 30s - loss: 0.2426 - val_loss: 0.2694
Epoch 40/256
3063/3063 - 30s - loss: 0.2420 - val_loss: 0.2542
Epoch 41/256
3063/3063 - 30s - loss: 0.2405 - val_loss: 0.2515
Epoch 42/256
3063/3063 - 30s - loss: 0.2397 - val_loss: 0.2528
Epoch 43/256
3063/3063 - 30s - loss: 0.2380 - val_loss: 0.2515
Epoch 44/256
3063/3063 - 30s - loss: 0.2365 - val_loss: 0.2565
Epoch 45/256
3063/3063 - 30s - loss: 0.2353 - val_loss: 0.2518
Epoch 46/256
3063/3063 - 30s - loss: 0.2343 - val_loss: 0.2531
Epoch 47/256
3063/3063 - 30s - loss: 0.2330 - val_loss: 0.3201
Epoch 48/256
3063/3063 - 30s - loss: 0.2323 - val_loss: 0.2499
Epoch 49/256
3063/3063 - 30s - loss: 0.2309 - val_loss: 0.2619
Epoch 50/256
3063/3063 - 30s - loss: 0.2297 - val_loss: 0.2523
Epoch 51/256
3063/3063 - 30s - loss: 0.2283 - val_loss: 0.2618
Epoch 52/256
3063/3063 - 30s - loss: 0.2282 - val_loss: 0.2652
Epoch 53/256
3063/3063 - 30s - loss: 0.2267 - val_loss: 0.2695
Epoch 54/256
3063/3063 - 30s - loss: 0.2259 - val_loss: 0.2436
Epoch 55/256
3063/3063 - 30s - loss: 0.2249 - val_loss: 0.2568
Epoch 56/256
3063/3063 - 30s - loss: 0.2237 - val_loss: 0.2416
Epoch 57/256
3063/3063 - 30s - loss: 0.2223 - val_loss: 0.2455
Epoch 58/256
3063/3063 - 30s - loss: 0.2219 - val_loss: 0.2489
Epoch 59/256
3063/3063 - 30s - loss: 0.2209 - val_loss: 0.2479
Epoch 60/256
3063/3063 - 30s - loss: 0.2202 - val_loss: 0.2587
Epoch 61/256
3063/3063 - 30s - loss: 0.2199 - val_loss: 0.2517
Epoch 62/256
3063/3063 - 30s - loss: 0.2188 - val_loss: 0.2591
Epoch 63/256
3063/3063 - 30s - loss: 0.2170 - val_loss: 0.2750
Epoch 64/256
3063/3063 - 30s - loss: 0.2167 - val_loss: 0.2543
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 64)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 32s - loss: 0.3940 - val_loss: 0.3747
Epoch 2/256
3063/3063 - 30s - loss: 0.3612 - val_loss: 0.3553
Epoch 3/256
3063/3063 - 30s - loss: 0.3530 - val_loss: 0.3594
Epoch 4/256
3063/3063 - 30s - loss: 0.3486 - val_loss: 0.3523
Epoch 5/256
3063/3063 - 30s - loss: 0.3447 - val_loss: 0.3430
Epoch 6/256
3063/3063 - 30s - loss: 0.3417 - val_loss: 0.3508
Epoch 7/256
3063/3063 - 30s - loss: 0.3387 - val_loss: 0.3429
Epoch 8/256
3063/3063 - 30s - loss: 0.3364 - val_loss: 0.3349
Epoch 9/256
3063/3063 - 30s - loss: 0.3327 - val_loss: 0.3294
Epoch 10/256
3063/3063 - 30s - loss: 0.3290 - val_loss: 0.3292
Epoch 11/256
3063/3063 - 30s - loss: 0.3259 - val_loss: 0.3181
Epoch 12/256
3063/3063 - 30s - loss: 0.3190 - val_loss: 0.3115
Epoch 13/256
3063/3063 - 30s - loss: 0.3105 - val_loss: 0.3101
Epoch 14/256
3063/3063 - 30s - loss: 0.3050 - val_loss: 0.3009
Epoch 15/256
3063/3063 - 30s - loss: 0.2964 - val_loss: 0.3061
Epoch 16/256
3063/3063 - 30s - loss: 0.2915 - val_loss: 0.2906
Epoch 17/256
3063/3063 - 30s - loss: 0.2864 - val_loss: 0.2807
Epoch 18/256
3063/3063 - 30s - loss: 0.2819 - val_loss: 0.2822
Epoch 19/256
3063/3063 - 30s - loss: 0.2785 - val_loss: 0.2864
Epoch 20/256
3063/3063 - 30s - loss: 0.2744 - val_loss: 0.2804
Epoch 21/256
3063/3063 - 30s - loss: 0.2720 - val_loss: 0.2891
Epoch 22/256
3063/3063 - 30s - loss: 0.2704 - val_loss: 0.2689
Epoch 23/256
3063/3063 - 30s - loss: 0.2678 - val_loss: 0.2689
Epoch 24/256
3063/3063 - 30s - loss: 0.2652 - val_loss: 0.2724
Epoch 25/256
3063/3063 - 30s - loss: 0.2640 - val_loss: 0.2672
Epoch 26/256
3063/3063 - 30s - loss: 0.2613 - val_loss: 0.2667
Epoch 27/256
3063/3063 - 30s - loss: 0.2601 - val_loss: 0.2625
Epoch 28/256
3063/3063 - 30s - loss: 0.2569 - val_loss: 0.2838
Epoch 29/256
3063/3063 - 30s - loss: 0.2547 - val_loss: 0.2655
Epoch 30/256
3063/3063 - 30s - loss: 0.2527 - val_loss: 0.2603
Epoch 31/256
3063/3063 - 30s - loss: 0.2504 - val_loss: 0.2653
Epoch 32/256
3063/3063 - 30s - loss: 0.2497 - val_loss: 0.2568
Epoch 33/256
3063/3063 - 30s - loss: 0.2472 - val_loss: 0.2587
Epoch 34/256
3063/3063 - 30s - loss: 0.2457 - val_loss: 0.2555
Epoch 35/256
3063/3063 - 30s - loss: 0.2435 - val_loss: 0.2680
Epoch 36/256
3063/3063 - 30s - loss: 0.2423 - val_loss: 0.2581
Epoch 37/256
3063/3063 - 30s - loss: 0.2400 - val_loss: 0.2590
Epoch 38/256
3063/3063 - 30s - loss: 0.2398 - val_loss: 0.2456
Epoch 39/256
3063/3063 - 30s - loss: 0.2372 - val_loss: 0.2496
Epoch 40/256
3063/3063 - 30s - loss: 0.2357 - val_loss: 0.2516
Epoch 41/256
3063/3063 - 30s - loss: 0.2352 - val_loss: 0.2544
Epoch 42/256
3063/3063 - 30s - loss: 0.2340 - val_loss: 0.2464
Epoch 43/256
3063/3063 - 30s - loss: 0.2321 - val_loss: 0.2524
Epoch 44/256
3063/3063 - 30s - loss: 0.2320 - val_loss: 0.2465
Epoch 45/256
3063/3063 - 30s - loss: 0.2309 - val_loss: 0.2549
Epoch 46/256
3063/3063 - 30s - loss: 0.2300 - val_loss: 0.2474
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 64)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.3971 - val_loss: 0.3611
Epoch 2/256
3063/3063 - 30s - loss: 0.3620 - val_loss: 0.3495
Epoch 3/256
3063/3063 - 30s - loss: 0.3529 - val_loss: 0.3492
Epoch 4/256
3063/3063 - 30s - loss: 0.3474 - val_loss: 0.3485
Epoch 5/256
3063/3063 - 30s - loss: 0.3421 - val_loss: 0.3344
Epoch 6/256
3063/3063 - 30s - loss: 0.3378 - val_loss: 0.3491
Epoch 7/256
3063/3063 - 30s - loss: 0.3336 - val_loss: 0.3263
Epoch 8/256
3063/3063 - 30s - loss: 0.3272 - val_loss: 0.3206
Epoch 9/256
3063/3063 - 30s - loss: 0.3199 - val_loss: 0.3226
Epoch 10/256
3063/3063 - 30s - loss: 0.3082 - val_loss: 0.3025
Epoch 11/256
3063/3063 - 30s - loss: 0.3005 - val_loss: 0.3107
Epoch 12/256
3063/3063 - 30s - loss: 0.2927 - val_loss: 0.3016
Epoch 13/256
3063/3063 - 30s - loss: 0.2870 - val_loss: 0.2864
Epoch 14/256
3063/3063 - 30s - loss: 0.2826 - val_loss: 0.2951
Epoch 15/256
3063/3063 - 30s - loss: 0.2792 - val_loss: 0.2765
Epoch 16/256
3063/3063 - 30s - loss: 0.2758 - val_loss: 0.2764
Epoch 17/256
3063/3063 - 30s - loss: 0.2721 - val_loss: 0.2722
Epoch 18/256
3063/3063 - 30s - loss: 0.2703 - val_loss: 0.3030
Epoch 19/256
3063/3063 - 30s - loss: 0.2689 - val_loss: 0.2854
Epoch 20/256
3063/3063 - 30s - loss: 0.2663 - val_loss: 0.2776
Epoch 21/256
3063/3063 - 30s - loss: 0.2631 - val_loss: 0.2850
Epoch 22/256
3063/3063 - 30s - loss: 0.2609 - val_loss: 0.2881
Epoch 23/256
3063/3063 - 30s - loss: 0.2597 - val_loss: 0.2834
Epoch 24/256
3063/3063 - 30s - loss: 0.2582 - val_loss: 0.2637
Epoch 25/256
3063/3063 - 30s - loss: 0.2562 - val_loss: 0.2570
Epoch 26/256
3063/3063 - 30s - loss: 0.2543 - val_loss: 0.2617
Epoch 27/256
3063/3063 - 30s - loss: 0.2524 - val_loss: 0.2735
Epoch 28/256
3063/3063 - 30s - loss: 0.2508 - val_loss: 0.2654
Epoch 29/256
3063/3063 - 30s - loss: 0.2486 - val_loss: 0.2665
Epoch 30/256
3063/3063 - 30s - loss: 0.2482 - val_loss: 0.2559
Epoch 31/256
3063/3063 - 30s - loss: 0.2466 - val_loss: 0.2605
Epoch 32/256
3063/3063 - 30s - loss: 0.2442 - val_loss: 0.2808
Epoch 33/256
3063/3063 - 30s - loss: 0.2433 - val_loss: 0.2594
Epoch 34/256
3063/3063 - 30s - loss: 0.2416 - val_loss: 0.2538
Epoch 35/256
3063/3063 - 30s - loss: 0.2394 - val_loss: 0.2493
Epoch 36/256
3063/3063 - 30s - loss: 0.2386 - val_loss: 0.2597
Epoch 37/256
3063/3063 - 30s - loss: 0.2373 - val_loss: 0.2697
Epoch 38/256
3063/3063 - 30s - loss: 0.2355 - val_loss: 0.2480
Epoch 39/256
3063/3063 - 30s - loss: 0.2341 - val_loss: 0.2561
Epoch 40/256
3063/3063 - 30s - loss: 0.2337 - val_loss: 0.2472
Epoch 41/256
3063/3063 - 30s - loss: 0.2318 - val_loss: 0.2613
Epoch 42/256
3063/3063 - 30s - loss: 0.2310 - val_loss: 0.2849
Epoch 43/256
3063/3063 - 30s - loss: 0.2299 - val_loss: 0.2609
Epoch 44/256
3063/3063 - 30s - loss: 0.2289 - val_loss: 0.2455
Epoch 45/256
3063/3063 - 30s - loss: 0.2269 - val_loss: 0.2474
Epoch 46/256
3063/3063 - 30s - loss: 0.2262 - val_loss: 0.2497
Epoch 47/256
3063/3063 - 30s - loss: 0.2256 - val_loss: 0.2454
Epoch 48/256
3063/3063 - 30s - loss: 0.2239 - val_loss: 0.2443
Epoch 49/256
3063/3063 - 30s - loss: 0.2233 - val_loss: 0.2664
Epoch 50/256
3063/3063 - 30s - loss: 0.2219 - val_loss: 0.2460
Epoch 51/256
3063/3063 - 30s - loss: 0.2219 - val_loss: 0.2568
Epoch 52/256
3063/3063 - 30s - loss: 0.2203 - val_loss: 0.2494
Epoch 53/256
3063/3063 - 30s - loss: 0.2193 - val_loss: 0.2400
Epoch 54/256
3063/3063 - 30s - loss: 0.2175 - val_loss: 0.2595
Epoch 55/256
3063/3063 - 30s - loss: 0.2168 - val_loss: 0.2424
Epoch 56/256
3063/3063 - 30s - loss: 0.2161 - val_loss: 0.2410
Epoch 57/256
3063/3063 - 30s - loss: 0.2155 - val_loss: 0.2757
Epoch 58/256
3063/3063 - 30s - loss: 0.2145 - val_loss: 0.2482
Epoch 59/256
3063/3063 - 30s - loss: 0.2138 - val_loss: 0.2486
Epoch 60/256
3063/3063 - 30s - loss: 0.2123 - val_loss: 0.2432
Epoch 61/256
3063/3063 - 30s - loss: 0.2122 - val_loss: 0.2454
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 64)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.3954 - val_loss: 0.3672
Epoch 2/256
3063/3063 - 30s - loss: 0.3626 - val_loss: 0.3537
Epoch 3/256
3063/3063 - 30s - loss: 0.3548 - val_loss: 0.3520
Epoch 4/256
3063/3063 - 30s - loss: 0.3489 - val_loss: 0.3458
Epoch 5/256
3063/3063 - 30s - loss: 0.3464 - val_loss: 0.3400
Epoch 6/256
3063/3063 - 30s - loss: 0.3426 - val_loss: 0.3379
Epoch 7/256
3063/3063 - 30s - loss: 0.3398 - val_loss: 0.3369
Epoch 8/256
3063/3063 - 30s - loss: 0.3369 - val_loss: 0.3386
Epoch 9/256
3063/3063 - 30s - loss: 0.3343 - val_loss: 0.3328
Epoch 10/256
3063/3063 - 30s - loss: 0.3313 - val_loss: 0.3317
Epoch 11/256
3063/3063 - 30s - loss: 0.3283 - val_loss: 0.3249
Epoch 12/256
3063/3063 - 30s - loss: 0.3197 - val_loss: 0.3139
Epoch 13/256
3063/3063 - 30s - loss: 0.3134 - val_loss: 0.3494
Epoch 14/256
3063/3063 - 30s - loss: 0.3075 - val_loss: 0.3051
Epoch 15/256
3063/3063 - 30s - loss: 0.3007 - val_loss: 0.3179
Epoch 16/256
3063/3063 - 30s - loss: 0.2953 - val_loss: 0.3067
Epoch 17/256
3063/3063 - 30s - loss: 0.2898 - val_loss: 0.2977
Epoch 18/256
3063/3063 - 30s - loss: 0.2851 - val_loss: 0.2866
Epoch 19/256
3063/3063 - 30s - loss: 0.2808 - val_loss: 0.2788
Epoch 20/256
3063/3063 - 30s - loss: 0.2773 - val_loss: 0.2810
Epoch 21/256
3063/3063 - 30s - loss: 0.2767 - val_loss: 0.2757
Epoch 22/256
3063/3063 - 30s - loss: 0.2719 - val_loss: 0.2776
Epoch 23/256
3063/3063 - 30s - loss: 0.2698 - val_loss: 0.2839
Epoch 24/256
3063/3063 - 30s - loss: 0.2679 - val_loss: 0.2877
Epoch 25/256
3063/3063 - 30s - loss: 0.2656 - val_loss: 0.2803
Epoch 26/256
3063/3063 - 30s - loss: 0.2631 - val_loss: 0.2681
Epoch 27/256
3063/3063 - 30s - loss: 0.2615 - val_loss: 0.2937
Epoch 28/256
3063/3063 - 30s - loss: 0.2602 - val_loss: 0.2757
Epoch 29/256
3063/3063 - 30s - loss: 0.2574 - val_loss: 0.2635
Epoch 30/256
3063/3063 - 30s - loss: 0.2562 - val_loss: 0.2691
Epoch 31/256
3063/3063 - 30s - loss: 0.2552 - val_loss: 0.2581
Epoch 32/256
3063/3063 - 30s - loss: 0.2527 - val_loss: 0.2586
Epoch 33/256
3063/3063 - 30s - loss: 0.2517 - val_loss: 0.2606
Epoch 34/256
3063/3063 - 30s - loss: 0.2491 - val_loss: 0.2560
Epoch 35/256
3063/3063 - 30s - loss: 0.2479 - val_loss: 0.2552
Epoch 36/256
3063/3063 - 30s - loss: 0.2454 - val_loss: 0.2625
Epoch 37/256
3063/3063 - 30s - loss: 0.2445 - val_loss: 0.2550
Epoch 38/256
3063/3063 - 30s - loss: 0.2435 - val_loss: 0.2604
Epoch 39/256
3063/3063 - 30s - loss: 0.2422 - val_loss: 0.2551
Epoch 40/256
3063/3063 - 30s - loss: 0.2409 - val_loss: 0.2525
Epoch 41/256
3063/3063 - 30s - loss: 0.2393 - val_loss: 0.2662
Epoch 42/256
3063/3063 - 30s - loss: 0.2378 - val_loss: 0.2529
Epoch 43/256
3063/3063 - 30s - loss: 0.2364 - val_loss: 0.2490
Epoch 44/256
3063/3063 - 30s - loss: 0.2358 - val_loss: 0.2534
Epoch 45/256
3063/3063 - 30s - loss: 0.2348 - val_loss: 0.2541
Epoch 46/256
3063/3063 - 30s - loss: 0.2329 - val_loss: 0.2461
Epoch 47/256
3063/3063 - 30s - loss: 0.2318 - val_loss: 0.2582
Epoch 48/256
3063/3063 - 30s - loss: 0.2308 - val_loss: 0.2537
Epoch 49/256
3063/3063 - 30s - loss: 0.2296 - val_loss: 0.2459
Epoch 50/256
3063/3063 - 30s - loss: 0.2290 - val_loss: 0.2578
Epoch 51/256
3063/3063 - 30s - loss: 0.2277 - val_loss: 0.2547
Epoch 52/256
3063/3063 - 30s - loss: 0.2267 - val_loss: 0.2512
Epoch 53/256
3063/3063 - 30s - loss: 0.2256 - val_loss: 0.2499
Epoch 54/256
3063/3063 - 30s - loss: 0.2237 - val_loss: 0.2456
Epoch 55/256
3063/3063 - 30s - loss: 0.2244 - val_loss: 0.2432
Epoch 56/256
3063/3063 - 30s - loss: 0.2235 - val_loss: 0.2421
Epoch 57/256
3063/3063 - 30s - loss: 0.2228 - val_loss: 0.2485
Epoch 58/256
3063/3063 - 30s - loss: 0.2211 - val_loss: 0.2499
Epoch 59/256
3063/3063 - 30s - loss: 0.2203 - val_loss: 0.2556
Epoch 60/256
3063/3063 - 30s - loss: 0.2191 - val_loss: 0.2539
Epoch 61/256
3063/3063 - 30s - loss: 0.2180 - val_loss: 0.2504
Epoch 62/256
3063/3063 - 30s - loss: 0.2168 - val_loss: 0.2505
Epoch 63/256
3063/3063 - 30s - loss: 0.2162 - val_loss: 0.2487
Epoch 64/256
3063/3063 - 30s - loss: 0.2164 - val_loss: 0.2730
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 64)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.4006 - val_loss: 0.3590
Epoch 2/256
3063/3063 - 30s - loss: 0.3639 - val_loss: 0.3604
Epoch 3/256
3063/3063 - 30s - loss: 0.3551 - val_loss: 0.3487
Epoch 4/256
3063/3063 - 30s - loss: 0.3490 - val_loss: 0.3474
Epoch 5/256
3063/3063 - 30s - loss: 0.3460 - val_loss: 0.3442
Epoch 6/256
3063/3063 - 30s - loss: 0.3427 - val_loss: 0.3645
Epoch 7/256
3063/3063 - 30s - loss: 0.3415 - val_loss: 0.3489
Epoch 8/256
3063/3063 - 30s - loss: 0.3378 - val_loss: 0.3339
Epoch 9/256
3063/3063 - 30s - loss: 0.3338 - val_loss: 0.3360
Epoch 10/256
3063/3063 - 30s - loss: 0.3281 - val_loss: 0.3289
Epoch 11/256
3063/3063 - 30s - loss: 0.3193 - val_loss: 0.3188
Epoch 12/256
3063/3063 - 30s - loss: 0.3105 - val_loss: 0.3037
Epoch 13/256
3063/3063 - 30s - loss: 0.3033 - val_loss: 0.2960
Epoch 14/256
3063/3063 - 30s - loss: 0.2969 - val_loss: 0.3002
Epoch 15/256
3063/3063 - 30s - loss: 0.2923 - val_loss: 0.3030
Epoch 16/256
3063/3063 - 30s - loss: 0.2866 - val_loss: 0.2969
Epoch 17/256
3063/3063 - 30s - loss: 0.2837 - val_loss: 0.2758
Epoch 18/256
3063/3063 - 30s - loss: 0.2797 - val_loss: 0.2795
Epoch 19/256
3063/3063 - 30s - loss: 0.2773 - val_loss: 0.3172
Epoch 20/256
3063/3063 - 30s - loss: 0.2754 - val_loss: 0.2809
Epoch 21/256
3063/3063 - 30s - loss: 0.2726 - val_loss: 0.2779
Epoch 22/256
3063/3063 - 30s - loss: 0.2703 - val_loss: 0.2759
Epoch 23/256
3063/3063 - 30s - loss: 0.2675 - val_loss: 0.2776
Epoch 24/256
3063/3063 - 30s - loss: 0.2655 - val_loss: 0.2754
Epoch 25/256
3063/3063 - 30s - loss: 0.2636 - val_loss: 0.2635
Epoch 26/256
3063/3063 - 30s - loss: 0.2621 - val_loss: 0.2687
Epoch 27/256
3063/3063 - 30s - loss: 0.2596 - val_loss: 0.2748
Epoch 28/256
3063/3063 - 30s - loss: 0.2582 - val_loss: 0.2622
Epoch 29/256
3063/3063 - 30s - loss: 0.2560 - val_loss: 0.2665
Epoch 30/256
3063/3063 - 30s - loss: 0.2551 - val_loss: 0.2633
Epoch 31/256
3063/3063 - 30s - loss: 0.2531 - val_loss: 0.2575
Epoch 32/256
3063/3063 - 30s - loss: 0.2518 - val_loss: 0.2628
Epoch 33/256
3063/3063 - 30s - loss: 0.2496 - val_loss: 0.2680
Epoch 34/256
3063/3063 - 30s - loss: 0.2486 - val_loss: 0.2596
Epoch 35/256
3063/3063 - 30s - loss: 0.2467 - val_loss: 0.2585
Epoch 36/256
3063/3063 - 30s - loss: 0.2455 - val_loss: 0.2566
Epoch 37/256
3063/3063 - 30s - loss: 0.2444 - val_loss: 0.2681
Epoch 38/256
3063/3063 - 30s - loss: 0.2424 - val_loss: 0.2520
Epoch 39/256
3063/3063 - 30s - loss: 0.2419 - val_loss: 0.2714
Epoch 40/256
3063/3063 - 30s - loss: 0.2411 - val_loss: 0.2510
Epoch 41/256
3063/3063 - 30s - loss: 0.2379 - val_loss: 0.2524
Epoch 42/256
3063/3063 - 30s - loss: 0.2373 - val_loss: 0.2506
Epoch 43/256
3063/3063 - 30s - loss: 0.2355 - val_loss: 0.2551
Epoch 44/256
3063/3063 - 30s - loss: 0.2340 - val_loss: 0.2636
Epoch 45/256
3063/3063 - 30s - loss: 0.2347 - val_loss: 0.2618
Epoch 46/256
3063/3063 - 30s - loss: 0.2318 - val_loss: 0.2567
Epoch 47/256
3063/3063 - 30s - loss: 0.2316 - val_loss: 0.2514
Epoch 48/256
3063/3063 - 30s - loss: 0.2298 - val_loss: 0.2564
Epoch 49/256
3063/3063 - 30s - loss: 0.2285 - val_loss: 0.2615
Epoch 50/256
3063/3063 - 30s - loss: 0.2288 - val_loss: 0.2546
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 64)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.3990 - val_loss: 0.3688
Epoch 2/256
3063/3063 - 30s - loss: 0.3641 - val_loss: 0.3569
Epoch 3/256
3063/3063 - 30s - loss: 0.3522 - val_loss: 0.3529
Epoch 4/256
3063/3063 - 30s - loss: 0.3477 - val_loss: 0.3444
Epoch 5/256
3063/3063 - 30s - loss: 0.3435 - val_loss: 0.3411
Epoch 6/256
3063/3063 - 29s - loss: 0.3411 - val_loss: 0.3391
Epoch 7/256
3063/3063 - 30s - loss: 0.3371 - val_loss: 0.3537
Epoch 8/256
3063/3063 - 30s - loss: 0.3327 - val_loss: 0.3320
Epoch 9/256
3063/3063 - 30s - loss: 0.3290 - val_loss: 0.3237
Epoch 10/256
3063/3063 - 30s - loss: 0.3224 - val_loss: 0.3238
Epoch 11/256
3063/3063 - 30s - loss: 0.3155 - val_loss: 0.3190
Epoch 12/256
3063/3063 - 30s - loss: 0.3083 - val_loss: 0.3075
Epoch 13/256
3063/3063 - 30s - loss: 0.2998 - val_loss: 0.2988
Epoch 14/256
3063/3063 - 30s - loss: 0.2934 - val_loss: 0.3035
Epoch 15/256
3063/3063 - 30s - loss: 0.2877 - val_loss: 0.2960
Epoch 16/256
3063/3063 - 30s - loss: 0.2836 - val_loss: 0.2932
Epoch 17/256
3063/3063 - 30s - loss: 0.2800 - val_loss: 0.2807
Epoch 18/256
3063/3063 - 30s - loss: 0.2783 - val_loss: 0.2720
Epoch 19/256
3063/3063 - 30s - loss: 0.2751 - val_loss: 0.2698
Epoch 20/256
3063/3063 - 30s - loss: 0.2720 - val_loss: 0.2713
Epoch 21/256
3063/3063 - 30s - loss: 0.2686 - val_loss: 0.2680
Epoch 22/256
3063/3063 - 30s - loss: 0.2670 - val_loss: 0.2838
Epoch 23/256
3063/3063 - 30s - loss: 0.2655 - val_loss: 0.2709
Epoch 24/256
3063/3063 - 30s - loss: 0.2638 - val_loss: 0.2718
Epoch 25/256
3063/3063 - 30s - loss: 0.2604 - val_loss: 0.2673
Epoch 26/256
3063/3063 - 30s - loss: 0.2606 - val_loss: 0.2663
Epoch 27/256
3063/3063 - 30s - loss: 0.2574 - val_loss: 0.2633
Epoch 28/256
3063/3063 - 30s - loss: 0.2568 - val_loss: 0.2666
Epoch 29/256
3063/3063 - 30s - loss: 0.2543 - val_loss: 0.2568
Epoch 30/256
3063/3063 - 30s - loss: 0.2525 - val_loss: 0.2650
Epoch 31/256
3063/3063 - 30s - loss: 0.2510 - val_loss: 0.2666
Epoch 32/256
3063/3063 - 30s - loss: 0.2491 - val_loss: 0.2693
Epoch 33/256
3063/3063 - 30s - loss: 0.2473 - val_loss: 0.2607
Epoch 34/256
3063/3063 - 30s - loss: 0.2467 - val_loss: 0.2567
Epoch 35/256
3063/3063 - 30s - loss: 0.2440 - val_loss: 0.2570
Epoch 36/256
3063/3063 - 30s - loss: 0.2439 - val_loss: 0.2544
Epoch 37/256
3063/3063 - 30s - loss: 0.2405 - val_loss: 0.2682
Epoch 38/256
3063/3063 - 30s - loss: 0.2403 - val_loss: 0.2515
Epoch 39/256
3063/3063 - 30s - loss: 0.2386 - val_loss: 0.2635
Epoch 40/256
3063/3063 - 30s - loss: 0.2374 - val_loss: 0.2569
Epoch 41/256
3063/3063 - 30s - loss: 0.2356 - val_loss: 0.2667
Epoch 42/256
3063/3063 - 30s - loss: 0.2354 - val_loss: 0.2523
Epoch 43/256
3063/3063 - 30s - loss: 0.2333 - val_loss: 0.2498
Epoch 44/256
3063/3063 - 30s - loss: 0.2330 - val_loss: 0.2578
Epoch 45/256
3063/3063 - 30s - loss: 0.2318 - val_loss: 0.2450
Epoch 46/256
3063/3063 - 30s - loss: 0.2304 - val_loss: 0.2625
Epoch 47/256
3063/3063 - 30s - loss: 0.2290 - val_loss: 0.2548
Epoch 48/256
3063/3063 - 30s - loss: 0.2271 - val_loss: 0.2523
Epoch 49/256
3063/3063 - 30s - loss: 0.2267 - val_loss: 0.2572
Epoch 50/256
3063/3063 - 30s - loss: 0.2253 - val_loss: 0.2447
Epoch 51/256
3063/3063 - 30s - loss: 0.2233 - val_loss: 0.2427
Epoch 52/256
3063/3063 - 30s - loss: 0.2236 - val_loss: 0.2505
Epoch 53/256
3063/3063 - 30s - loss: 0.2232 - val_loss: 0.2509
Epoch 54/256
3063/3063 - 35s - loss: 0.2208 - val_loss: 0.2454
Epoch 55/256
3063/3063 - 30s - loss: 0.2196 - val_loss: 0.2418
Epoch 56/256
3063/3063 - 30s - loss: 0.2197 - val_loss: 0.2551
Epoch 57/256
3063/3063 - 30s - loss: 0.2194 - val_loss: 0.2547
Epoch 58/256
3063/3063 - 30s - loss: 0.2173 - val_loss: 0.2653
Epoch 59/256
3063/3063 - 30s - loss: 0.2168 - val_loss: 0.2476
Epoch 60/256
3063/3063 - 30s - loss: 0.2150 - val_loss: 0.2419
Epoch 61/256
3063/3063 - 30s - loss: 0.2145 - val_loss: 0.2545
Epoch 62/256
3063/3063 - 30s - loss: 0.2119 - val_loss: 0.2466
Epoch 63/256
3063/3063 - 30s - loss: 0.2128 - val_loss: 0.2510
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 64)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.4039 - val_loss: 0.3705
Epoch 2/256
3063/3063 - 29s - loss: 0.3686 - val_loss: 0.3541
Epoch 3/256
3063/3063 - 30s - loss: 0.3562 - val_loss: 0.3455
Epoch 4/256
3063/3063 - 30s - loss: 0.3506 - val_loss: 0.3493
Epoch 5/256
3063/3063 - 29s - loss: 0.3449 - val_loss: 0.3437
Epoch 6/256
3063/3063 - 30s - loss: 0.3404 - val_loss: 0.3385
Epoch 7/256
3063/3063 - 30s - loss: 0.3381 - val_loss: 0.3314
Epoch 8/256
3063/3063 - 29s - loss: 0.3347 - val_loss: 0.3267
Epoch 9/256
3063/3063 - 30s - loss: 0.3304 - val_loss: 0.3362
Epoch 10/256
3063/3063 - 30s - loss: 0.3246 - val_loss: 0.3266
Epoch 11/256
3063/3063 - 30s - loss: 0.3200 - val_loss: 0.3156
Epoch 12/256
3063/3063 - 30s - loss: 0.3123 - val_loss: 0.3076
Epoch 13/256
3063/3063 - 30s - loss: 0.3059 - val_loss: 0.3055
Epoch 14/256
3063/3063 - 30s - loss: 0.3002 - val_loss: 0.2973
Epoch 15/256
3063/3063 - 30s - loss: 0.2945 - val_loss: 0.2931
Epoch 16/256
3063/3063 - 30s - loss: 0.2895 - val_loss: 0.3190
Epoch 17/256
3063/3063 - 30s - loss: 0.2849 - val_loss: 0.2876
Epoch 18/256
3063/3063 - 30s - loss: 0.2819 - val_loss: 0.3009
Epoch 19/256
3063/3063 - 30s - loss: 0.2795 - val_loss: 0.2847
Epoch 20/256
3063/3063 - 30s - loss: 0.2758 - val_loss: 0.2961
Epoch 21/256
3063/3063 - 30s - loss: 0.2722 - val_loss: 0.2778
Epoch 22/256
3063/3063 - 30s - loss: 0.2714 - val_loss: 0.2703
Epoch 23/256
3063/3063 - 30s - loss: 0.2684 - val_loss: 0.2729
Epoch 24/256
3063/3063 - 30s - loss: 0.2659 - val_loss: 0.2723
Epoch 25/256
3063/3063 - 30s - loss: 0.2633 - val_loss: 0.2805
Epoch 26/256
3063/3063 - 30s - loss: 0.2612 - val_loss: 0.2610
Epoch 27/256
3063/3063 - 30s - loss: 0.2575 - val_loss: 0.2635
Epoch 28/256
3063/3063 - 30s - loss: 0.2581 - val_loss: 0.2607
Epoch 29/256
3063/3063 - 30s - loss: 0.2556 - val_loss: 0.2705
Epoch 30/256
3063/3063 - 30s - loss: 0.2539 - val_loss: 0.2620
Epoch 31/256
3063/3063 - 30s - loss: 0.2517 - val_loss: 0.2797
Epoch 32/256
3063/3063 - 30s - loss: 0.2514 - val_loss: 0.2626
Epoch 33/256
3063/3063 - 30s - loss: 0.2501 - val_loss: 0.2638
Epoch 34/256
3063/3063 - 30s - loss: 0.2478 - val_loss: 0.2562
Epoch 35/256
3063/3063 - 30s - loss: 0.2467 - val_loss: 0.2615
Epoch 36/256
3063/3063 - 30s - loss: 0.2448 - val_loss: 0.2645
Epoch 37/256
3063/3063 - 30s - loss: 0.2433 - val_loss: 0.2547
Epoch 38/256
3063/3063 - 30s - loss: 0.2425 - val_loss: 0.2561
Epoch 39/256
3063/3063 - 30s - loss: 0.2415 - val_loss: 0.2515
Epoch 40/256
3063/3063 - 30s - loss: 0.2403 - val_loss: 0.2585
Epoch 41/256
3063/3063 - 30s - loss: 0.2402 - val_loss: 0.2578
Epoch 42/256
3063/3063 - 30s - loss: 0.2379 - val_loss: 0.2507
Epoch 43/256
3063/3063 - 30s - loss: 0.2367 - val_loss: 0.2606
Epoch 44/256
3063/3063 - 30s - loss: 0.2350 - val_loss: 0.2544
Epoch 45/256
3063/3063 - 30s - loss: 0.2351 - val_loss: 0.2525
Epoch 46/256
3063/3063 - 30s - loss: 0.2337 - val_loss: 0.2486
Epoch 47/256
3063/3063 - 30s - loss: 0.2327 - val_loss: 0.2542
Epoch 48/256
3063/3063 - 30s - loss: 0.2313 - val_loss: 0.2542
Epoch 49/256
3063/3063 - 30s - loss: 0.2310 - val_loss: 0.2509
Epoch 50/256
3063/3063 - 30s - loss: 0.2307 - val_loss: 0.2533
Epoch 51/256
3063/3063 - 30s - loss: 0.2291 - val_loss: 0.2474
Epoch 52/256
3063/3063 - 30s - loss: 0.2289 - val_loss: 0.2543
Epoch 53/256
3063/3063 - 30s - loss: 0.2267 - val_loss: 0.2569
Epoch 54/256
3063/3063 - 30s - loss: 0.2267 - val_loss: 0.2583
Epoch 55/256
3063/3063 - 30s - loss: 0.2257 - val_loss: 0.2512
Epoch 56/256
3063/3063 - 30s - loss: 0.2245 - val_loss: 0.2548
Epoch 57/256
3063/3063 - 30s - loss: 0.2244 - val_loss: 0.2548
Epoch 58/256
3063/3063 - 30s - loss: 0.2225 - val_loss: 0.2498
Epoch 59/256
3063/3063 - 30s - loss: 0.2234 - val_loss: 0.2468
Epoch 60/256
3063/3063 - 30s - loss: 0.2219 - val_loss: 0.2523
Epoch 61/256
3063/3063 - 29s - loss: 0.2208 - val_loss: 0.2519
Epoch 62/256
3063/3063 - 30s - loss: 0.2187 - val_loss: 0.2563
Epoch 63/256
3063/3063 - 30s - loss: 0.2202 - val_loss: 0.2484
Epoch 64/256
3063/3063 - 30s - loss: 0.2186 - val_loss: 0.2544
Epoch 65/256
3063/3063 - 30s - loss: 0.2175 - val_loss: 0.2525
Epoch 66/256
3063/3063 - 30s - loss: 0.2172 - val_loss: 0.2581
Epoch 67/256
3063/3063 - 30s - loss: 0.2169 - val_loss: 0.2523
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 64)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 31s - loss: 0.3961 - val_loss: 0.3533
Epoch 2/256
3063/3063 - 30s - loss: 0.3628 - val_loss: 0.3482
Epoch 3/256
3063/3063 - 30s - loss: 0.3525 - val_loss: 0.3445
Epoch 4/256
3063/3063 - 34s - loss: 0.3469 - val_loss: 0.3449
Epoch 5/256
3063/3063 - 30s - loss: 0.3436 - val_loss: 0.3346
Epoch 6/256
3063/3063 - 30s - loss: 0.3397 - val_loss: 0.3372
Epoch 7/256
3063/3063 - 30s - loss: 0.3368 - val_loss: 0.3375
Epoch 8/256
3063/3063 - 30s - loss: 0.3313 - val_loss: 0.3281
Epoch 9/256
3063/3063 - 30s - loss: 0.3268 - val_loss: 0.3251
Epoch 10/256
3063/3063 - 30s - loss: 0.3191 - val_loss: 0.3124
Epoch 11/256
3063/3063 - 30s - loss: 0.3097 - val_loss: 0.3009
Epoch 12/256
3063/3063 - 30s - loss: 0.3013 - val_loss: 0.3095
Epoch 13/256
3063/3063 - 30s - loss: 0.2933 - val_loss: 0.2874
Epoch 14/256
3063/3063 - 30s - loss: 0.2883 - val_loss: 0.2975
Epoch 15/256
3063/3063 - 30s - loss: 0.2843 - val_loss: 0.2836
Epoch 16/256
3063/3063 - 30s - loss: 0.2802 - val_loss: 0.2903
Epoch 17/256
3063/3063 - 30s - loss: 0.2767 - val_loss: 0.2911
Epoch 18/256
3063/3063 - 30s - loss: 0.2744 - val_loss: 0.2756
Epoch 19/256
3063/3063 - 30s - loss: 0.2717 - val_loss: 0.3031
Epoch 20/256
3063/3063 - 30s - loss: 0.2699 - val_loss: 0.2815
Epoch 21/256
3063/3063 - 30s - loss: 0.2662 - val_loss: 0.2725
Epoch 22/256
3063/3063 - 30s - loss: 0.2648 - val_loss: 0.2751
Epoch 23/256
3063/3063 - 30s - loss: 0.2633 - val_loss: 0.2735
Epoch 24/256
3063/3063 - 30s - loss: 0.2618 - val_loss: 0.2641
Epoch 25/256
3063/3063 - 30s - loss: 0.2587 - val_loss: 0.2662
Epoch 26/256
3063/3063 - 30s - loss: 0.2578 - val_loss: 0.2585
Epoch 27/256
3063/3063 - 30s - loss: 0.2561 - val_loss: 0.2894
Epoch 28/256
3063/3063 - 30s - loss: 0.2545 - val_loss: 0.2606
Epoch 29/256
3063/3063 - 30s - loss: 0.2536 - val_loss: 0.2701
Epoch 30/256
3063/3063 - 30s - loss: 0.2511 - val_loss: 0.2721
Epoch 31/256
3063/3063 - 30s - loss: 0.2489 - val_loss: 0.2709
Epoch 32/256
3063/3063 - 30s - loss: 0.2474 - val_loss: 0.2545
Epoch 33/256
3063/3063 - 30s - loss: 0.2467 - val_loss: 0.2665
Epoch 34/256
3063/3063 - 30s - loss: 0.2431 - val_loss: 0.2603
Epoch 35/256
3063/3063 - 30s - loss: 0.2428 - val_loss: 0.2698
Epoch 36/256
3063/3063 - 30s - loss: 0.2408 - val_loss: 0.2566
Epoch 37/256
3063/3063 - 30s - loss: 0.2402 - val_loss: 0.2582
Epoch 38/256
3063/3063 - 30s - loss: 0.2392 - val_loss: 0.2503
Epoch 39/256
3063/3063 - 30s - loss: 0.2377 - val_loss: 0.2721
Epoch 40/256
3063/3063 - 30s - loss: 0.2356 - val_loss: 0.2638
Epoch 41/256
3063/3063 - 30s - loss: 0.2343 - val_loss: 0.2671
Epoch 42/256
3063/3063 - 30s - loss: 0.2342 - val_loss: 0.2436
Epoch 43/256
3063/3063 - 30s - loss: 0.2333 - val_loss: 0.2555
Epoch 44/256
3063/3063 - 30s - loss: 0.2311 - val_loss: 0.2785
Epoch 45/256
3063/3063 - 30s - loss: 0.2312 - val_loss: 0.2540
Epoch 46/256
3063/3063 - 30s - loss: 0.2287 - val_loss: 0.2551
Epoch 47/256
3063/3063 - 30s - loss: 0.2289 - val_loss: 0.2497
Epoch 48/256
3063/3063 - 30s - loss: 0.2274 - val_loss: 0.2582
Epoch 49/256
3063/3063 - 30s - loss: 0.2260 - val_loss: 0.2502
Epoch 50/256
3063/3063 - 30s - loss: 0.2252 - val_loss: 0.2491
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 64)_64
			 64 [0.9642316997301184, 0.9629695589714967, 0.9644470045327181, 0.9643047954175427, 0.961985103046569, 0.9641758125378637, 0.9635119796282172, 0.964065358721329]
getting ROC for dnn
currently on dnn_256_3_2
