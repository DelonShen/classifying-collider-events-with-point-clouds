nohup: ignoring input
2022-07-11 13:31:17.223573: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-07-11 13:31:46.013296: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1
2022-07-11 13:31:46.030224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:84:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2022-07-11 13:31:46.031527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: 
pciBusID: 0000:85:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2022-07-11 13:31:46.031562: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-07-11 13:31:46.035917: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2022-07-11 13:31:46.035977: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
2022-07-11 13:31:46.037609: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10
2022-07-11 13:31:46.037930: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10
2022-07-11 13:31:46.042283: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11
2022-07-11 13:31:46.043206: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11
2022-07-11 13:31:46.043452: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2022-07-11 13:31:46.047336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1
2022-07-11 13:31:46.047939: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-07-11 13:31:46.171307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:84:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2022-07-11 13:31:46.172445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: 
pciBusID: 0000:85:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2022-07-11 13:31:46.176558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1
2022-07-11 13:31:46.176651: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-07-11 13:31:47.178189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-07-11 13:31:47.178255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1 
2022-07-11 13:31:47.178273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N Y 
2022-07-11 13:31:47.178283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   Y N 
2022-07-11 13:31:47.183397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8599 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:84:00.0, compute capability: 3.7)
2022-07-11 13:31:47.185336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10792 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id: 0000:85:00.0, compute capability: 3.7)
2022-07-11 13:33:07.160896: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
2022-07-11 13:33:07.161654: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2099935000 Hz
2022-07-11 13:33:08.989119: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2022-07-11 13:33:09.262965: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
Initializing Experimenter
	Loading Data from ../data/data100k_raw_combined_atlas_cut.pkl
	Data Loaded
	Creating Splits
	Splits Created
Done initalizing
Loading Experimenter from Saved Experimenter at /data/delon/experimenter/data100k_raw_combined_atlas_cut
Experimenter Loaded
Getting split
Split Stored
Loading models
{'tripletwise_5_(64, 128, 256, 128, 64)_64': 'models/data100k_raw_combined_atlas_cut_tripletwise_5_(64, 128, 256, 128, 64)_64', 'pairwise_5_(64, 128, 256, 128, 64)_64': 'models/data100k_raw_combined_atlas_cut_pairwise_5_(64, 128, 256, 128, 64)_64', 'particlewise_128_4_64': 'models/data100k_raw_combined_atlas_cut_particlewise_128_4_64', 'pairwise_nl_5_(64, 128, 256, 128, 64)_32_64': 'models/data100k_raw_combined_atlas_cut_pairwise_nl_5_(64, 128, 256, 128, 64)_32_64', 'pairwise_nl_iter_5_((64, 64, 116, 64, 64), (64, 64, 116, 64, 64), (64, 64, 116, 64, 64))_32_64': 'models/data100k_raw_combined_atlas_cut_pairwise_nl_iter_5_((64, 64, 116, 64, 64), (64, 64, 116, 64, 64), (64, 64, 116, 64, 64))_32_64', 'nested_concat_70_4_64_3': 'models/data100k_raw_combined_atlas_cut_nested_concat_70_4_64_3', 'naivednn_256_3_2': 'models/data100k_raw_combined_atlas_cut_naivednn_256_3_2', 'dnn_256_3_2': 'models/data100k_raw_combined_atlas_cut_dnn_256_3_2', 'nested_concat_general_68_3_64_3': 'models/data100k_raw_combined_atlas_cut_nested_concat_general_68_3_64_3'}
Loaded tripletwise_5_(64, 128, 256, 128, 64)_64 from models/data100k_raw_combined_atlas_cut_tripletwise_5_(64, 128, 256, 128, 64)_64
Loaded pairwise_5_(64, 128, 256, 128, 64)_64 from models/data100k_raw_combined_atlas_cut_pairwise_5_(64, 128, 256, 128, 64)_64
Loaded particlewise_128_4_64 from models/data100k_raw_combined_atlas_cut_particlewise_128_4_64
Loaded pairwise_nl_5_(64, 128, 256, 128, 64)_32_64 from models/data100k_raw_combined_atlas_cut_pairwise_nl_5_(64, 128, 256, 128, 64)_32_64
Loaded pairwise_nl_iter_5_((64, 64, 116, 64, 64), (64, 64, 116, 64, 64), (64, 64, 116, 64, 64))_32_64 from models/data100k_raw_combined_atlas_cut_pairwise_nl_iter_5_((64, 64, 116, 64, 64), (64, 64, 116, 64, 64), (64, 64, 116, 64, 64))_32_64
Loaded nested_concat_70_4_64_3 from models/data100k_raw_combined_atlas_cut_nested_concat_70_4_64_3
Loaded naivednn_256_3_2 from models/data100k_raw_combined_atlas_cut_naivednn_256_3_2
Loaded dnn_256_3_2 from models/data100k_raw_combined_atlas_cut_dnn_256_3_2
Loaded nested_concat_general_68_3_64_3 from models/data100k_raw_combined_atlas_cut_nested_concat_general_68_3_64_3
RIGHT NOW: particlewise
tf.data.datset created for training data
Epoch 1/512
3063/3063 - 14s - loss: 0.2918 - val_loss: 0.3229
Epoch 2/512
3063/3063 - 11s - loss: 0.2867 - val_loss: 0.3207
Epoch 3/512
3063/3063 - 12s - loss: 0.2846 - val_loss: 0.3250
Epoch 4/512
3063/3063 - 11s - loss: 0.2829 - val_loss: 0.3242
Epoch 5/512
3063/3063 - 11s - loss: 0.2821 - val_loss: 0.3268
Epoch 6/512
3063/3063 - 12s - loss: 0.2811 - val_loss: 0.3351
Epoch 7/512
3063/3063 - 11s - loss: 0.2798 - val_loss: 0.3401
Epoch 8/512
3063/3063 - 11s - loss: 0.2790 - val_loss: 0.3302
Epoch 9/512
3063/3063 - 12s - loss: 0.2781 - val_loss: 0.3246
Epoch 10/512
3063/3063 - 11s - loss: 0.2771 - val_loss: 0.3329
Epoch 11/512
3063/3063 - 11s - loss: 0.2765 - val_loss: 0.3383
Epoch 12/512
3063/3063 - 11s - loss: 0.2758 - val_loss: 0.3325
Epoch 13/512
3063/3063 - 10s - loss: 0.2751 - val_loss: 0.3330
Epoch 14/512
3063/3063 - 10s - loss: 0.2742 - val_loss: 0.3360
Epoch 15/512
3063/3063 - 10s - loss: 0.2725 - val_loss: 0.3447
Epoch 16/512
3063/3063 - 10s - loss: 0.2726 - val_loss: 0.3379
Epoch 17/512
3063/3063 - 11s - loss: 0.2712 - val_loss: 0.3410
Epoch 18/512
3063/3063 - 10s - loss: 0.2703 - val_loss: 0.3368
Epoch 19/512
3063/3063 - 11s - loss: 0.2691 - val_loss: 0.3381
Epoch 20/512
3063/3063 - 10s - loss: 0.2691 - val_loss: 0.3302
Epoch 21/512
3063/3063 - 10s - loss: 0.2672 - val_loss: 0.3496
Epoch 22/512
3063/3063 - 11s - loss: 0.2673 - val_loss: 0.3437
Epoch 23/512
3063/3063 - 10s - loss: 0.2658 - val_loss: 0.3380
Epoch 24/512
3063/3063 - 10s - loss: 0.2653 - val_loss: 0.3679
Epoch 25/512
3063/3063 - 11s - loss: 0.2644 - val_loss: 0.3332
Epoch 26/512
3063/3063 - 10s - loss: 0.2639 - val_loss: 0.3321
Epoch 27/512
3063/3063 - 10s - loss: 0.2623 - val_loss: 0.3375
Epoch 28/512
3063/3063 - 10s - loss: 0.2613 - val_loss: 0.3516
Epoch 29/512
3063/3063 - 10s - loss: 0.2605 - val_loss: 0.3435
Epoch 30/512
3063/3063 - 10s - loss: 0.2600 - val_loss: 0.3527
Epoch 31/512
3063/3063 - 10s - loss: 0.2584 - val_loss: 0.3535
Epoch 32/512
3063/3063 - 10s - loss: 0.2574 - val_loss: 0.3619
Epoch 33/512
3063/3063 - 11s - loss: 0.2566 - val_loss: 0.3557
Epoch 34/512
3063/3063 - 10s - loss: 0.2557 - val_loss: 0.3575
###
first saving models
currently on tripletwise_5_(64, 128, 256, 128, 64)_64
	this one already saved, skipped
currently on pairwise_5_(64, 128, 256, 128, 64)_64
	this one already saved, skipped
currently on particlewise_128_4_64
	this one already saved, skipped
currently on pairwise_nl_5_(64, 128, 256, 128, 64)_32_64
	this one already saved, skipped
currently on pairwise_nl_iter_5_((64, 64, 116, 64, 64), (64, 64, 116, 64, 64), (64, 64, 116, 64, 64))_32_64
	this one already saved, skipped
currently on nested_concat_70_4_64_3
	this one already saved, skipped
currently on naivednn_256_3_2
	this one already saved, skipped
currently on dnn_256_3_2
	this one already saved, skipped
currently on nested_concat_general_68_3_64_3
	this one already saved, skipped
now saving paramters of experimenter
saved experimenter at /data/delon/experimenter/data100k_raw_combined_atlas_cut
RIGHT NOW: nested_concat
tf.data.datset created for training data
Epoch 1/512
3063/3063 - 26s - loss: 0.2708 - val_loss: 0.2746
Epoch 2/512
3063/3063 - 23s - loss: 0.2345 - val_loss: 0.2607
Epoch 3/512
3063/3063 - 24s - loss: 0.2262 - val_loss: 0.2605
Epoch 4/512
3063/3063 - 24s - loss: 0.2220 - val_loss: 0.2619
Epoch 5/512
3063/3063 - 23s - loss: 0.2183 - val_loss: 0.2678
Epoch 6/512
3063/3063 - 22s - loss: 0.2149 - val_loss: 0.2779
Epoch 7/512
3063/3063 - 22s - loss: 0.2129 - val_loss: 0.2793
Epoch 8/512
3063/3063 - 21s - loss: 0.2122 - val_loss: 0.2657
Epoch 9/512
3063/3063 - 21s - loss: 0.2112 - val_loss: 0.2672
Epoch 10/512
3063/3063 - 23s - loss: 0.2094 - val_loss: 0.2764
Epoch 11/512
3063/3063 - 23s - loss: 0.2065 - val_loss: 0.2863
Epoch 12/512
3063/3063 - 24s - loss: 0.2072 - val_loss: 0.2743
Epoch 13/512
3063/3063 - 23s - loss: 0.2049 - val_loss: 0.2720
Epoch 14/512
3063/3063 - 23s - loss: 0.2040 - val_loss: 0.2807
Epoch 15/512
3063/3063 - 24s - loss: 0.2029 - val_loss: 0.2756
Epoch 16/512
3063/3063 - 22s - loss: 0.2021 - val_loss: 0.2785
Epoch 17/512
3063/3063 - 23s - loss: 0.2020 - val_loss: 0.2808
Epoch 18/512
3063/3063 - 22s - loss: 0.1999 - val_loss: 0.2738
Epoch 19/512
3063/3063 - 22s - loss: 0.2000 - val_loss: 0.2876
Epoch 20/512
3063/3063 - 22s - loss: 0.1982 - val_loss: 0.2766
Epoch 21/512
3063/3063 - 22s - loss: 0.1986 - val_loss: 0.2805
Epoch 22/512
3063/3063 - 22s - loss: 0.1964 - val_loss: 0.3008
Epoch 23/512
3063/3063 - 21s - loss: 0.1958 - val_loss: 0.2808
Epoch 24/512
3063/3063 - 21s - loss: 0.1959 - val_loss: 0.2834
Epoch 25/512
3063/3063 - 21s - loss: 0.1955 - val_loss: 0.2904
Epoch 26/512
3063/3063 - 21s - loss: 0.1955 - val_loss: 0.2715
Epoch 27/512
3063/3063 - 23s - loss: 0.1938 - val_loss: 0.2859
Epoch 28/512
3063/3063 - 23s - loss: 0.1923 - val_loss: 0.2796
Epoch 29/512
3063/3063 - 22s - loss: 0.1935 - val_loss: 0.2777
Epoch 30/512
3063/3063 - 21s - loss: 0.1906 - val_loss: 0.2967
Epoch 31/512
3063/3063 - 21s - loss: 0.1907 - val_loss: 0.2846
Epoch 32/512
3063/3063 - 21s - loss: 0.1909 - val_loss: 0.3074
Epoch 33/512
3063/3063 - 21s - loss: 0.1909 - val_loss: 0.2776
Epoch 34/512
3063/3063 - 21s - loss: 0.1891 - val_loss: 0.2914
Epoch 35/512
3063/3063 - 21s - loss: 0.1880 - val_loss: 0.2905
###
first saving models
currently on tripletwise_5_(64, 128, 256, 128, 64)_64
	this one already saved, skipped
currently on pairwise_5_(64, 128, 256, 128, 64)_64
	this one already saved, skipped
currently on particlewise_128_4_64
	this one already saved, skipped
currently on pairwise_nl_5_(64, 128, 256, 128, 64)_32_64
	this one already saved, skipped
currently on pairwise_nl_iter_5_((64, 64, 116, 64, 64), (64, 64, 116, 64, 64), (64, 64, 116, 64, 64))_32_64
	this one already saved, skipped
currently on nested_concat_70_4_64_3
	this one already saved, skipped
currently on naivednn_256_3_2
	this one already saved, skipped
currently on dnn_256_3_2
	this one already saved, skipped
currently on nested_concat_general_68_3_64_3
	this one already saved, skipped
now saving paramters of experimenter
saved experimenter at /data/delon/experimenter/data100k_raw_combined_atlas_cut
RIGHT NOW: nested_concat_general
tf.data.datset created for training data
Epoch 1/512
3063/3063 - 28s - loss: 0.3318 - val_loss: 0.3114
Epoch 2/512
3063/3063 - 25s - loss: 0.2972 - val_loss: 0.3025
Epoch 3/512
3063/3063 - 22s - loss: 0.2895 - val_loss: 0.2997
Epoch 4/512
3063/3063 - 23s - loss: 0.2848 - val_loss: 0.3114
Epoch 5/512
3063/3063 - 22s - loss: 0.2817 - val_loss: 0.2993
Epoch 6/512
3063/3063 - 22s - loss: 0.2789 - val_loss: 0.3058
Epoch 7/512
3063/3063 - 22s - loss: 0.2762 - val_loss: 0.3167
Epoch 8/512
3063/3063 - 22s - loss: 0.2741 - val_loss: 0.3069
Epoch 9/512
3063/3063 - 24s - loss: 0.2727 - val_loss: 0.3031
Epoch 10/512
3063/3063 - 24s - loss: 0.2700 - val_loss: 0.3035
Epoch 11/512
3063/3063 - 22s - loss: 0.2688 - val_loss: 0.3166
Epoch 12/512
3063/3063 - 23s - loss: 0.2669 - val_loss: 0.2995
Epoch 13/512
3063/3063 - 23s - loss: 0.2662 - val_loss: 0.3087
Epoch 14/512
3063/3063 - 23s - loss: 0.2644 - val_loss: 0.3103
Epoch 15/512
3063/3063 - 24s - loss: 0.2617 - val_loss: 0.3090
Epoch 16/512
3063/3063 - 24s - loss: 0.2604 - val_loss: 0.3103
Epoch 17/512
3063/3063 - 24s - loss: 0.2596 - val_loss: 0.3031
Epoch 18/512
3063/3063 - 23s - loss: 0.2573 - val_loss: 0.3047
Epoch 19/512
3063/3063 - 21s - loss: 0.2558 - val_loss: 0.3048
Epoch 20/512
3063/3063 - 23s - loss: 0.2550 - val_loss: 0.3154
Epoch 21/512
3063/3063 - 22s - loss: 0.2537 - val_loss: 0.3137
Epoch 22/512
3063/3063 - 24s - loss: 0.2519 - val_loss: 0.3277
Epoch 23/512
3063/3063 - 23s - loss: 0.2504 - val_loss: 0.3053
Epoch 24/512
3063/3063 - 23s - loss: 0.2489 - val_loss: 0.3201
Epoch 25/512
3063/3063 - 22s - loss: 0.2476 - val_loss: 0.3082
Epoch 26/512
3063/3063 - 22s - loss: 0.2460 - val_loss: 0.3037
Epoch 27/512
3063/3063 - 22s - loss: 0.2442 - val_loss: 0.3056
Epoch 28/512
3063/3063 - 23s - loss: 0.2433 - val_loss: 0.3047
Epoch 29/512
3063/3063 - 23s - loss: 0.2417 - val_loss: 0.3117
Epoch 30/512
3063/3063 - 22s - loss: 0.2396 - val_loss: 0.3186
Epoch 31/512
3063/3063 - 21s - loss: 0.2384 - val_loss: 0.3054
Epoch 32/512
3063/3063 - 22s - loss: 0.2374 - val_loss: 0.3238
Epoch 33/512
3063/3063 - 22s - loss: 0.2353 - val_loss: 0.3146
Epoch 34/512
3063/3063 - 23s - loss: 0.2338 - val_loss: 0.3244
Epoch 35/512
3063/3063 - 23s - loss: 0.2319 - val_loss: 0.3212
Epoch 36/512
3063/3063 - 23s - loss: 0.2299 - val_loss: 0.3118
Epoch 37/512
3063/3063 - 23s - loss: 0.2287 - val_loss: 0.3302
2022-07-11 14:11:05.177492: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2022-07-11 14:11:05.529220: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8201
###
first saving models
currently on tripletwise_5_(64, 128, 256, 128, 64)_64
	this one already saved, skipped
currently on pairwise_5_(64, 128, 256, 128, 64)_64
	this one already saved, skipped
currently on particlewise_128_4_64
	this one already saved, skipped
currently on pairwise_nl_5_(64, 128, 256, 128, 64)_32_64
	this one already saved, skipped
currently on pairwise_nl_iter_5_((64, 64, 116, 64, 64), (64, 64, 116, 64, 64), (64, 64, 116, 64, 64))_32_64
	this one already saved, skipped
currently on nested_concat_70_4_64_3
	this one already saved, skipped
currently on naivednn_256_3_2
	this one already saved, skipped
currently on dnn_256_3_2
	this one already saved, skipped
currently on nested_concat_general_68_3_64_3
	this one already saved, skipped
now saving paramters of experimenter
saved experimenter at /data/delon/experimenter/data100k_raw_combined_atlas_cut
RIGHT NOW: tripletwise
tf.data.datset created for training data
Epoch 1/512
3063/3063 - 312s - loss: 0.2121 - val_loss: 0.2451
Epoch 2/512
3063/3063 - 307s - loss: 0.2069 - val_loss: 0.2427
Epoch 3/512
3063/3063 - 307s - loss: 0.2059 - val_loss: 0.2384
Epoch 4/512
3063/3063 - 307s - loss: 0.2051 - val_loss: 0.2769
Epoch 5/512
3063/3063 - 307s - loss: 0.2037 - val_loss: 0.2378
Epoch 6/512
3063/3063 - 307s - loss: 0.2040 - val_loss: 0.2595
Epoch 7/512
3063/3063 - 307s - loss: 0.2016 - val_loss: 0.2535
Epoch 8/512
3063/3063 - 307s - loss: 0.2010 - val_loss: 0.2418
Epoch 9/512
3063/3063 - 307s - loss: 0.2015 - val_loss: 0.2369
Epoch 10/512
3063/3063 - 307s - loss: 0.1991 - val_loss: 0.2478
Epoch 11/512
3063/3063 - 307s - loss: 0.1987 - val_loss: 0.2468
Epoch 12/512
3063/3063 - 308s - loss: 0.1981 - val_loss: 0.2418
Epoch 13/512
3063/3063 - 308s - loss: 0.1984 - val_loss: 0.2455
Epoch 14/512
3063/3063 - 308s - loss: 0.1960 - val_loss: 0.2573
Epoch 15/512
3063/3063 - 308s - loss: 0.1951 - val_loss: 0.2418
Epoch 16/512
3063/3063 - 308s - loss: 0.1943 - val_loss: 0.2472
Epoch 17/512
3063/3063 - 307s - loss: 0.1933 - val_loss: 0.2420
Epoch 18/512
3063/3063 - 308s - loss: 0.1929 - val_loss: 0.2482
Epoch 19/512
3063/3063 - 307s - loss: 0.1920 - val_loss: 0.2489
Epoch 20/512
3063/3063 - 308s - loss: 0.1911 - val_loss: 0.2449
Epoch 21/512
3063/3063 - 307s - loss: 0.1917 - val_loss: 0.2442
Epoch 22/512
3063/3063 - 308s - loss: 0.1908 - val_loss: 0.2793
Epoch 23/512
3063/3063 - 308s - loss: 0.1895 - val_loss: 0.2578
Epoch 24/512
3063/3063 - 308s - loss: 0.1890 - val_loss: 0.2548
Epoch 25/512
3063/3063 - 307s - loss: 0.1879 - val_loss: 0.2433
Epoch 26/512
3063/3063 - 307s - loss: 0.1870 - val_loss: 0.2468
Epoch 27/512
3063/3063 - 307s - loss: 0.1855 - val_loss: 0.2479
Epoch 28/512
3063/3063 - 307s - loss: 0.1870 - val_loss: 0.2561
Epoch 29/512
3063/3063 - 307s - loss: 0.1846 - val_loss: 0.2568
Epoch 30/512
3063/3063 - 307s - loss: 0.1847 - val_loss: 0.2472
Epoch 31/512
3063/3063 - 307s - loss: 0.1843 - val_loss: 0.2481
Epoch 32/512
3063/3063 - 307s - loss: 0.1831 - val_loss: 0.2673
Epoch 33/512
3063/3063 - 307s - loss: 0.1818 - val_loss: 0.2503
Epoch 34/512
3063/3063 - 308s - loss: 0.1811 - val_loss: 0.2551
Epoch 35/512
3063/3063 - 308s - loss: 0.1807 - val_loss: 0.2562
Epoch 36/512
3063/3063 - 308s - loss: 0.1799 - val_loss: 0.2566
Epoch 37/512
3063/3063 - 307s - loss: 0.1776 - val_loss: 0.2683
Epoch 38/512
3063/3063 - 307s - loss: 0.1793 - val_loss: 0.2543
Epoch 39/512
3063/3063 - 307s - loss: 0.1768 - val_loss: 0.2622
Epoch 40/512
3063/3063 - 307s - loss: 0.1758 - val_loss: 0.2532
Epoch 41/512
3063/3063 - 306s - loss: 0.1766 - val_loss: 0.2482
###
first saving models
currently on tripletwise_5_(64, 128, 256, 128, 64)_64
	this one already saved, skipped
currently on pairwise_5_(64, 128, 256, 128, 64)_64
	this one already saved, skipped
currently on particlewise_128_4_64
	this one already saved, skipped
currently on pairwise_nl_5_(64, 128, 256, 128, 64)_32_64
	this one already saved, skipped
currently on pairwise_nl_iter_5_((64, 64, 116, 64, 64), (64, 64, 116, 64, 64), (64, 64, 116, 64, 64))_32_64
	this one already saved, skipped
currently on nested_concat_70_4_64_3
	this one already saved, skipped
currently on naivednn_256_3_2
	this one already saved, skipped
currently on dnn_256_3_2
	this one already saved, skipped
currently on nested_concat_general_68_3_64_3
	this one already saved, skipped
now saving paramters of experimenter
saved experimenter at /data/delon/experimenter/data100k_raw_combined_atlas_cut
RIGHT NOW: pairwise
tf.data.datset created for training data
Epoch 1/512
3063/3063 - 44s - loss: 0.2091 - val_loss: 0.2435
Epoch 2/512
3063/3063 - 41s - loss: 0.2038 - val_loss: 0.2451
Epoch 3/512
3063/3063 - 42s - loss: 0.2038 - val_loss: 0.2381
Epoch 4/512
3063/3063 - 41s - loss: 0.2012 - val_loss: 0.2554
Epoch 5/512
3063/3063 - 41s - loss: 0.1994 - val_loss: 0.2468
Epoch 6/512
3063/3063 - 41s - loss: 0.2005 - val_loss: 0.2489
Epoch 7/512
3063/3063 - 41s - loss: 0.1983 - val_loss: 0.2699
Epoch 8/512
3063/3063 - 42s - loss: 0.1969 - val_loss: 0.2396
Epoch 9/512
3063/3063 - 41s - loss: 0.1975 - val_loss: 0.2394
Epoch 10/512
3063/3063 - 40s - loss: 0.1950 - val_loss: 0.2515
Epoch 11/512
3063/3063 - 41s - loss: 0.1947 - val_loss: 0.2481
Epoch 12/512
3063/3063 - 41s - loss: 0.1934 - val_loss: 0.2521
Epoch 13/512
3063/3063 - 42s - loss: 0.1937 - val_loss: 0.2448
Epoch 14/512
3063/3063 - 41s - loss: 0.1918 - val_loss: 0.2578
Epoch 15/512
3063/3063 - 41s - loss: 0.1905 - val_loss: 0.2557
Epoch 16/512
3063/3063 - 41s - loss: 0.1904 - val_loss: 0.2563
Epoch 17/512
3063/3063 - 41s - loss: 0.1899 - val_loss: 0.2449
Epoch 18/512
3063/3063 - 41s - loss: 0.1881 - val_loss: 0.2486
Epoch 19/512
3063/3063 - 41s - loss: 0.1869 - val_loss: 0.2531
Epoch 20/512
3063/3063 - 41s - loss: 0.1877 - val_loss: 0.2452
Epoch 21/512
3063/3063 - 41s - loss: 0.1858 - val_loss: 0.2565
Epoch 22/512
3063/3063 - 41s - loss: 0.1850 - val_loss: 0.2727
Epoch 23/512
3063/3063 - 41s - loss: 0.1836 - val_loss: 0.2616
Epoch 24/512
3063/3063 - 41s - loss: 0.1835 - val_loss: 0.2612
Epoch 25/512
3063/3063 - 41s - loss: 0.1821 - val_loss: 0.2512
Epoch 26/512
3063/3063 - 41s - loss: 0.1814 - val_loss: 0.2507
Epoch 27/512
3063/3063 - 42s - loss: 0.1800 - val_loss: 0.2600
Epoch 28/512
3063/3063 - 41s - loss: 0.1794 - val_loss: 0.2517
Epoch 29/512
3063/3063 - 41s - loss: 0.1783 - val_loss: 0.2696
Epoch 30/512
3063/3063 - 40s - loss: 0.1772 - val_loss: 0.2759
Epoch 31/512
3063/3063 - 40s - loss: 0.1780 - val_loss: 0.2663
Epoch 32/512
3063/3063 - 40s - loss: 0.1758 - val_loss: 0.2678
Epoch 33/512
3063/3063 - 40s - loss: 0.1755 - val_loss: 0.2552
Epoch 34/512
3063/3063 - 40s - loss: 0.1737 - val_loss: 0.2591
Epoch 35/512
3063/3063 - 41s - loss: 0.1736 - val_loss: 0.2580
###
first saving models
currently on tripletwise_5_(64, 128, 256, 128, 64)_64
	this one already saved, skipped
currently on pairwise_5_(64, 128, 256, 128, 64)_64
	this one already saved, skipped
currently on particlewise_128_4_64
	this one already saved, skipped
currently on pairwise_nl_5_(64, 128, 256, 128, 64)_32_64
	this one already saved, skipped
currently on pairwise_nl_iter_5_((64, 64, 116, 64, 64), (64, 64, 116, 64, 64), (64, 64, 116, 64, 64))_32_64
	this one already saved, skipped
currently on nested_concat_70_4_64_3
	this one already saved, skipped
currently on naivednn_256_3_2
	this one already saved, skipped
currently on dnn_256_3_2
	this one already saved, skipped
currently on nested_concat_general_68_3_64_3
	this one already saved, skipped
now saving paramters of experimenter
saved experimenter at /data/delon/experimenter/data100k_raw_combined_atlas_cut
RIGHT NOW: pairwise_nl
tf.data.datset created for training data
Epoch 1/512
3063/3063 - 49s - loss: 0.2095 - val_loss: 0.2440
Epoch 2/512
3063/3063 - 44s - loss: 0.2050 - val_loss: 0.2452
Epoch 3/512
3063/3063 - 44s - loss: 0.2049 - val_loss: 0.2345
Epoch 4/512
3063/3063 - 45s - loss: 0.2027 - val_loss: 0.2452
Epoch 5/512
3063/3063 - 44s - loss: 0.2014 - val_loss: 0.2418
Epoch 6/512
3063/3063 - 44s - loss: 0.2018 - val_loss: 0.2487
Epoch 7/512
3063/3063 - 44s - loss: 0.1993 - val_loss: 0.2565
Epoch 8/512
3063/3063 - 44s - loss: 0.1989 - val_loss: 0.2367
Epoch 9/512
3063/3063 - 44s - loss: 0.1986 - val_loss: 0.2361
Epoch 10/512
3063/3063 - 44s - loss: 0.1977 - val_loss: 0.2349
Epoch 11/512
3063/3063 - 42s - loss: 0.1965 - val_loss: 0.2525
Epoch 12/512
3063/3063 - 42s - loss: 0.1956 - val_loss: 0.2433
Epoch 13/512
3063/3063 - 44s - loss: 0.1952 - val_loss: 0.2491
Epoch 14/512
3063/3063 - 45s - loss: 0.1934 - val_loss: 0.2479
Epoch 15/512
3063/3063 - 44s - loss: 0.1920 - val_loss: 0.2412
Epoch 16/512
3063/3063 - 45s - loss: 0.1922 - val_loss: 0.2466
Epoch 17/512
3063/3063 - 46s - loss: 0.1912 - val_loss: 0.2366
Epoch 18/512
3063/3063 - 45s - loss: 0.1894 - val_loss: 0.2458
Epoch 19/512
3063/3063 - 45s - loss: 0.1895 - val_loss: 0.2398
Epoch 20/512
3063/3063 - 45s - loss: 0.1883 - val_loss: 0.2564
Epoch 21/512
3063/3063 - 45s - loss: 0.1876 - val_loss: 0.2441
Epoch 22/512
3063/3063 - 45s - loss: 0.1880 - val_loss: 0.2847
Epoch 23/512
3063/3063 - 44s - loss: 0.1861 - val_loss: 0.2487
Epoch 24/512
3063/3063 - 45s - loss: 0.1859 - val_loss: 0.2486
Epoch 25/512
3063/3063 - 45s - loss: 0.1844 - val_loss: 0.2405
Epoch 26/512
3063/3063 - 45s - loss: 0.1837 - val_loss: 0.2435
Epoch 27/512
3063/3063 - 45s - loss: 0.1829 - val_loss: 0.2471
Epoch 28/512
3063/3063 - 46s - loss: 0.1829 - val_loss: 0.2374
Epoch 29/512
3063/3063 - 45s - loss: 0.1810 - val_loss: 0.2576
Epoch 30/512
3063/3063 - 45s - loss: 0.1810 - val_loss: 0.2454
Epoch 31/512
3063/3063 - 44s - loss: 0.1792 - val_loss: 0.2425
Epoch 32/512
3063/3063 - 44s - loss: 0.1784 - val_loss: 0.2573
Epoch 33/512
3063/3063 - 44s - loss: 0.1790 - val_loss: 0.2459
Epoch 34/512
3063/3063 - 45s - loss: 0.1773 - val_loss: 0.2431
Epoch 35/512
3063/3063 - 44s - loss: 0.1773 - val_loss: 0.2566
###
first saving models
currently on tripletwise_5_(64, 128, 256, 128, 64)_64
	this one already saved, skipped
currently on pairwise_5_(64, 128, 256, 128, 64)_64
	this one already saved, skipped
currently on particlewise_128_4_64
	this one already saved, skipped
currently on pairwise_nl_5_(64, 128, 256, 128, 64)_32_64
	this one already saved, skipped
currently on pairwise_nl_iter_5_((64, 64, 116, 64, 64), (64, 64, 116, 64, 64), (64, 64, 116, 64, 64))_32_64
	this one already saved, skipped
currently on nested_concat_70_4_64_3
	this one already saved, skipped
currently on naivednn_256_3_2
	this one already saved, skipped
currently on dnn_256_3_2
	this one already saved, skipped
currently on nested_concat_general_68_3_64_3
	this one already saved, skipped
now saving paramters of experimenter
saved experimenter at /data/delon/experimenter/data100k_raw_combined_atlas_cut
RIGHT NOW: pairwise_nl_iter
tf.data.datset created for training data
Epoch 1/512
3063/3063 - 93s - loss: 0.2076 - val_loss: 0.2515
Epoch 2/512
3063/3063 - 85s - loss: 0.2007 - val_loss: 0.2417
Epoch 3/512
3063/3063 - 84s - loss: 0.1995 - val_loss: 0.2387
Epoch 4/512
3063/3063 - 85s - loss: 0.1982 - val_loss: 0.2587
Epoch 5/512
3063/3063 - 80s - loss: 0.1976 - val_loss: 0.2471
Epoch 6/512
3063/3063 - 83s - loss: 0.1971 - val_loss: 0.2592
Epoch 7/512
3063/3063 - 84s - loss: 0.1948 - val_loss: 0.2570
Epoch 8/512
3063/3063 - 80s - loss: 0.1950 - val_loss: 0.2455
Epoch 9/512
3063/3063 - 79s - loss: 0.1943 - val_loss: 0.2406
Epoch 10/512
3063/3063 - 84s - loss: 0.1937 - val_loss: 0.2431
Epoch 11/512
3063/3063 - 84s - loss: 0.1927 - val_loss: 0.2586
Epoch 12/512
3063/3063 - 86s - loss: 0.1932 - val_loss: 0.2447
Epoch 13/512
3063/3063 - 86s - loss: 0.1912 - val_loss: 0.2502
Epoch 14/512
3063/3063 - 84s - loss: 0.1912 - val_loss: 0.2528
Epoch 15/512
3063/3063 - 86s - loss: 0.1893 - val_loss: 0.2443
Epoch 16/512
3063/3063 - 84s - loss: 0.1898 - val_loss: 0.2469
Epoch 17/512
3063/3063 - 85s - loss: 0.1885 - val_loss: 0.2567
Epoch 18/512
3063/3063 - 86s - loss: 0.1884 - val_loss: 0.2560
Epoch 19/512
3063/3063 - 85s - loss: 0.1883 - val_loss: 0.2528
Epoch 20/512
3063/3063 - 85s - loss: 0.1865 - val_loss: 0.2660
Epoch 21/512
3063/3063 - 85s - loss: 0.1869 - val_loss: 0.2567
Epoch 22/512
3063/3063 - 86s - loss: 0.1871 - val_loss: 0.2861
Epoch 23/512
3063/3063 - 86s - loss: 0.1854 - val_loss: 0.2485
Epoch 24/512
3063/3063 - 86s - loss: 0.1853 - val_loss: 0.2708
Epoch 25/512
3063/3063 - 85s - loss: 0.1836 - val_loss: 0.2539
Epoch 26/512
3063/3063 - 87s - loss: 0.1832 - val_loss: 0.2480
Epoch 27/512
3063/3063 - 88s - loss: 0.1820 - val_loss: 0.2596
Epoch 28/512
3063/3063 - 85s - loss: 0.1834 - val_loss: 0.2517
Epoch 29/512
3063/3063 - 86s - loss: 0.1812 - val_loss: 0.2613
Epoch 30/512
3063/3063 - 86s - loss: 0.1800 - val_loss: 0.2557
Epoch 31/512
3063/3063 - 85s - loss: 0.1806 - val_loss: 0.2625
Epoch 32/512
3063/3063 - 83s - loss: 0.1811 - val_loss: 0.2609
Epoch 33/512
3063/3063 - 85s - loss: 0.1788 - val_loss: 0.2517
Epoch 34/512
3063/3063 - 86s - loss: 0.1787 - val_loss: 0.2613
Epoch 35/512
3063/3063 - 86s - loss: 0.1778 - val_loss: 0.2577
###
first saving models
currently on tripletwise_5_(64, 128, 256, 128, 64)_64
	this one already saved, skipped
currently on pairwise_5_(64, 128, 256, 128, 64)_64
	this one already saved, skipped
currently on particlewise_128_4_64
	this one already saved, skipped
currently on pairwise_nl_5_(64, 128, 256, 128, 64)_32_64
	this one already saved, skipped
currently on pairwise_nl_iter_5_((64, 64, 116, 64, 64), (64, 64, 116, 64, 64), (64, 64, 116, 64, 64))_32_64
	this one already saved, skipped
currently on nested_concat_70_4_64_3
	this one already saved, skipped
currently on naivednn_256_3_2
	this one already saved, skipped
currently on dnn_256_3_2
	this one already saved, skipped
currently on nested_concat_general_68_3_64_3
	this one already saved, skipped
now saving paramters of experimenter
saved experimenter at /data/delon/experimenter/data100k_raw_combined_atlas_cut
RIGHT NOW: naivednn
tf.data.datset created for training data
Epoch 1/512
3063/3063 - 11s - loss: 0.3422 - val_loss: 0.3699
Epoch 2/512
3063/3063 - 9s - loss: 0.3386 - val_loss: 0.3674
Epoch 3/512
3063/3063 - 9s - loss: 0.3357 - val_loss: 0.3666
Epoch 4/512
3063/3063 - 9s - loss: 0.3327 - val_loss: 0.3702
Epoch 5/512
3063/3063 - 9s - loss: 0.3323 - val_loss: 0.3758
Epoch 6/512
3063/3063 - 9s - loss: 0.3296 - val_loss: 0.3797
Epoch 7/512
3063/3063 - 9s - loss: 0.3262 - val_loss: 0.3751
Epoch 8/512
3063/3063 - 9s - loss: 0.3249 - val_loss: 0.3721
Epoch 9/512
3063/3063 - 9s - loss: 0.3220 - val_loss: 0.3697
Epoch 10/512
3063/3063 - 9s - loss: 0.3221 - val_loss: 0.3885
Epoch 11/512
3063/3063 - 9s - loss: 0.3211 - val_loss: 0.3936
Epoch 12/512
3063/3063 - 9s - loss: 0.3188 - val_loss: 0.3784
Epoch 13/512
3063/3063 - 9s - loss: 0.3163 - val_loss: 0.3838
Epoch 14/512
3063/3063 - 9s - loss: 0.3150 - val_loss: 0.3844
Epoch 15/512
3063/3063 - 9s - loss: 0.3113 - val_loss: 0.3813
Epoch 16/512
3063/3063 - 9s - loss: 0.3112 - val_loss: 0.3868
Epoch 17/512
3063/3063 - 9s - loss: 0.3090 - val_loss: 0.3821
Epoch 18/512
3063/3063 - 9s - loss: 0.3057 - val_loss: 0.3965
Epoch 19/512
3063/3063 - 9s - loss: 0.3045 - val_loss: 0.3964
Epoch 20/512
3063/3063 - 9s - loss: 0.3020 - val_loss: 0.3928
Epoch 21/512
3063/3063 - 9s - loss: 0.3012 - val_loss: 0.4021
Epoch 22/512
3063/3063 - 9s - loss: 0.2971 - val_loss: 0.4055
Epoch 23/512
3063/3063 - 9s - loss: 0.2957 - val_loss: 0.3925
Epoch 24/512
3063/3063 - 9s - loss: 0.2956 - val_loss: 0.4060
Epoch 25/512
3063/3063 - 9s - loss: 0.2934 - val_loss: 0.4027
Epoch 26/512
3063/3063 - 9s - loss: 0.2911 - val_loss: 0.4086
Epoch 27/512
3063/3063 - 9s - loss: 0.2888 - val_loss: 0.4080
Epoch 28/512
3063/3063 - 9s - loss: 0.2865 - val_loss: 0.4319
Epoch 29/512
3063/3063 - 9s - loss: 0.2856 - val_loss: 0.4120
Epoch 30/512
3063/3063 - 9s - loss: 0.2825 - val_loss: 0.4420
Epoch 31/512
3063/3063 - 8s - loss: 0.2827 - val_loss: 0.4300
Epoch 32/512
3063/3063 - 9s - loss: 0.2788 - val_loss: 0.4245
Epoch 33/512
3063/3063 - 9s - loss: 0.2773 - val_loss: 0.4205
Epoch 34/512
3063/3063 - 9s - loss: 0.2749 - val_loss: 0.4708
Epoch 35/512
3063/3063 - 8s - loss: 0.2738 - val_loss: 0.4291
###
first saving models
currently on tripletwise_5_(64, 128, 256, 128, 64)_64
	this one already saved, skipped
currently on pairwise_5_(64, 128, 256, 128, 64)_64
	this one already saved, skipped
currently on particlewise_128_4_64
	this one already saved, skipped
currently on pairwise_nl_5_(64, 128, 256, 128, 64)_32_64
	this one already saved, skipped
currently on pairwise_nl_iter_5_((64, 64, 116, 64, 64), (64, 64, 116, 64, 64), (64, 64, 116, 64, 64))_32_64
	this one already saved, skipped
currently on nested_concat_70_4_64_3
	this one already saved, skipped
currently on naivednn_256_3_2
	this one already saved, skipped
currently on dnn_256_3_2
	this one already saved, skipped
currently on nested_concat_general_68_3_64_3
	this one already saved, skipped
now saving paramters of experimenter
saved experimenter at /data/delon/experimenter/data100k_raw_combined_atlas_cut
DNN Classifier
tf.data.datset created for training data
Epoch 1/512
3063/3063 - 10s - loss: 0.3732 - val_loss: 0.3467
Epoch 2/512
3063/3063 - 10s - loss: 0.3724 - val_loss: 0.3517
Epoch 3/512
3063/3063 - 9s - loss: 0.3738 - val_loss: 0.3527
Epoch 4/512
3063/3063 - 9s - loss: 0.3739 - val_loss: 0.3489
Epoch 5/512
3063/3063 - 9s - loss: 0.3727 - val_loss: 0.3466
Epoch 6/512
3063/3063 - 10s - loss: 0.3740 - val_loss: 0.3457
Epoch 7/512
3063/3063 - 9s - loss: 0.3713 - val_loss: 0.3507
Epoch 8/512
3063/3063 - 9s - loss: 0.3731 - val_loss: 0.3495
Epoch 9/512
3063/3063 - 10s - loss: 0.3722 - val_loss: 0.3496
Epoch 10/512
3063/3063 - 9s - loss: 0.3725 - val_loss: 0.3459
Epoch 11/512
3063/3063 - 10s - loss: 0.3720 - val_loss: 0.3470
Epoch 12/512
3063/3063 - 10s - loss: 0.3716 - val_loss: 0.3452
Epoch 13/512
3063/3063 - 10s - loss: 0.3723 - val_loss: 0.3495
Epoch 14/512
3063/3063 - 9s - loss: 0.3734 - val_loss: 0.3473
Epoch 15/512
3063/3063 - 9s - loss: 0.3721 - val_loss: 0.3492
Epoch 16/512
3063/3063 - 9s - loss: 0.3725 - val_loss: 0.3473
Epoch 17/512
3063/3063 - 10s - loss: 0.3727 - val_loss: 0.3456
Epoch 18/512
3063/3063 - 10s - loss: 0.3713 - val_loss: 0.3485
Epoch 19/512
3063/3063 - 10s - loss: 0.3720 - val_loss: 0.3487
Epoch 20/512
3063/3063 - 10s - loss: 0.3721 - val_loss: 0.3470
Epoch 21/512
3063/3063 - 10s - loss: 0.3713 - val_loss: 0.3495
Epoch 22/512
3063/3063 - 10s - loss: 0.3724 - val_loss: 0.3482
Epoch 23/512
3063/3063 - 10s - loss: 0.3707 - val_loss: 0.3468
Epoch 24/512
3063/3063 - 10s - loss: 0.3713 - val_loss: 0.3465
Epoch 25/512
3063/3063 - 10s - loss: 0.3710 - val_loss: 0.3486
Epoch 26/512
3063/3063 - 10s - loss: 0.3721 - val_loss: 0.3499
Epoch 27/512
3063/3063 - 10s - loss: 0.3727 - val_loss: 0.3470
Epoch 28/512
3063/3063 - 10s - loss: 0.3721 - val_loss: 0.3460
Epoch 29/512
3063/3063 - 10s - loss: 0.3718 - val_loss: 0.3483
Epoch 30/512
3063/3063 - 10s - loss: 0.3713 - val_loss: 0.3447
Epoch 31/512
3063/3063 - 10s - loss: 0.3722 - val_loss: 0.3461
Epoch 32/512
3063/3063 - 10s - loss: 0.3709 - val_loss: 0.3469
Epoch 33/512
3063/3063 - 10s - loss: 0.3724 - val_loss: 0.3469
Epoch 34/512
3063/3063 - 10s - loss: 0.3719 - val_loss: 0.3509
Epoch 35/512
3063/3063 - 9s - loss: 0.3709 - val_loss: 0.3461
Epoch 36/512
3063/3063 - 10s - loss: 0.3720 - val_loss: 0.3474
Epoch 37/512
3063/3063 - 10s - loss: 0.3702 - val_loss: 0.3490
Epoch 38/512
3063/3063 - 10s - loss: 0.3712 - val_loss: 0.3478
Epoch 39/512
3063/3063 - 10s - loss: 0.3724 - val_loss: 0.3514
Epoch 40/512
3063/3063 - 10s - loss: 0.3717 - val_loss: 0.3493
Epoch 41/512
3063/3063 - 10s - loss: 0.3701 - val_loss: 0.3461
Epoch 42/512
3063/3063 - 10s - loss: 0.3716 - val_loss: 0.3497
Epoch 43/512
3063/3063 - 10s - loss: 0.3716 - val_loss: 0.3474
Epoch 44/512
3063/3063 - 10s - loss: 0.3729 - val_loss: 0.3455
Epoch 45/512
3063/3063 - 10s - loss: 0.3714 - val_loss: 0.3492
Epoch 46/512
3063/3063 - 10s - loss: 0.3717 - val_loss: 0.3469
Epoch 47/512
3063/3063 - 10s - loss: 0.3713 - val_loss: 0.3475
Epoch 48/512
3063/3063 - 10s - loss: 0.3716 - val_loss: 0.3485
Epoch 49/512
3063/3063 - 10s - loss: 0.3716 - val_loss: 0.3498
Epoch 50/512
3063/3063 - 10s - loss: 0.3708 - val_loss: 0.3451
Epoch 51/512
3063/3063 - 10s - loss: 0.3719 - val_loss: 0.3490
Epoch 52/512
3063/3063 - 10s - loss: 0.3719 - val_loss: 0.3457
Epoch 53/512
3063/3063 - 10s - loss: 0.3703 - val_loss: 0.3483
Epoch 54/512
3063/3063 - 10s - loss: 0.3723 - val_loss: 0.3460
Epoch 55/512
3063/3063 - 10s - loss: 0.3730 - val_loss: 0.3475
Epoch 56/512
3063/3063 - 10s - loss: 0.3709 - val_loss: 0.3515
Epoch 57/512
3063/3063 - 10s - loss: 0.3700 - val_loss: 0.3461
Epoch 58/512
3063/3063 - 10s - loss: 0.3712 - val_loss: 0.3466
Epoch 59/512
3063/3063 - 10s - loss: 0.3717 - val_loss: 0.3462
Epoch 60/512
3063/3063 - 10s - loss: 0.3712 - val_loss: 0.3495
Epoch 61/512
3063/3063 - 9s - loss: 0.3702 - val_loss: 0.3470
Epoch 62/512
3063/3063 - 10s - loss: 0.3710 - val_loss: 0.3520
###
first saving models
currently on tripletwise_5_(64, 128, 256, 128, 64)_64
	this one already saved, skipped
currently on pairwise_5_(64, 128, 256, 128, 64)_64
	this one already saved, skipped
currently on particlewise_128_4_64
	this one already saved, skipped
currently on pairwise_nl_5_(64, 128, 256, 128, 64)_32_64
	this one already saved, skipped
currently on pairwise_nl_iter_5_((64, 64, 116, 64, 64), (64, 64, 116, 64, 64), (64, 64, 116, 64, 64))_32_64
	this one already saved, skipped
currently on nested_concat_70_4_64_3
	this one already saved, skipped
currently on naivednn_256_3_2
	this one already saved, skipped
currently on dnn_256_3_2
	this one already saved, skipped
currently on nested_concat_general_68_3_64_3
	this one already saved, skipped
now saving paramters of experimenter
saved experimenter at /data/delon/experimenter/data100k_raw_combined_atlas_cut
2022-07-11 19:44:38.752283: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-07-11 19:45:08.632618: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1
2022-07-11 19:45:08.648920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:84:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2022-07-11 19:45:08.649956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: 
pciBusID: 0000:85:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2022-07-11 19:45:08.649990: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-07-11 19:45:08.654009: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2022-07-11 19:45:08.654086: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
2022-07-11 19:45:08.655696: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10
2022-07-11 19:45:08.656009: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10
2022-07-11 19:45:08.660350: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11
2022-07-11 19:45:08.661228: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11
2022-07-11 19:45:08.661455: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2022-07-11 19:45:08.665290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1
2022-07-11 19:45:08.665851: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-07-11 19:45:08.801818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:84:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2022-07-11 19:45:08.802977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: 
pciBusID: 0000:85:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2022-07-11 19:45:08.821564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1
2022-07-11 19:45:08.821744: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-07-11 19:45:10.684651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-07-11 19:45:10.684713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1 
2022-07-11 19:45:10.684729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N Y 
2022-07-11 19:45:10.684737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   Y N 
2022-07-11 19:45:10.689897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8599 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:84:00.0, compute capability: 3.7)
2022-07-11 19:45:10.692264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10792 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id: 0000:85:00.0, compute capability: 3.7)
2022-07-11 19:46:04.827568: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
2022-07-11 19:46:04.830257: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2099935000 Hz
2022-07-11 19:46:06.193334: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2022-07-11 19:46:06.898926: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8201
2022-07-11 19:46:08.107866: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2022-07-11 19:46:08.524773: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
Initializing Experimenter
	Loading Data from ../data/data100k_raw_combined_atlas_cut.pkl
	Data Loaded
	Creating Splits
	Splits Created
Done initalizing
Loading Experimenter from Saved Experimenter at /data/delon/experimenter/data100k_raw_combined_atlas_cut
Experimenter Loaded
Getting split
Split Stored
Loading models
{'tripletwise_5_(64, 128, 256, 128, 64)_64': 'models/data100k_raw_combined_atlas_cut_tripletwise_5_(64, 128, 256, 128, 64)_64', 'pairwise_5_(64, 128, 256, 128, 64)_64': 'models/data100k_raw_combined_atlas_cut_pairwise_5_(64, 128, 256, 128, 64)_64', 'particlewise_128_4_64': 'models/data100k_raw_combined_atlas_cut_particlewise_128_4_64', 'pairwise_nl_5_(64, 128, 256, 128, 64)_32_64': 'models/data100k_raw_combined_atlas_cut_pairwise_nl_5_(64, 128, 256, 128, 64)_32_64', 'pairwise_nl_iter_5_((64, 64, 116, 64, 64), (64, 64, 116, 64, 64), (64, 64, 116, 64, 64))_32_64': 'models/data100k_raw_combined_atlas_cut_pairwise_nl_iter_5_((64, 64, 116, 64, 64), (64, 64, 116, 64, 64), (64, 64, 116, 64, 64))_32_64', 'nested_concat_70_4_64_3': 'models/data100k_raw_combined_atlas_cut_nested_concat_70_4_64_3', 'naivednn_256_3_2': 'models/data100k_raw_combined_atlas_cut_naivednn_256_3_2', 'dnn_256_3_2': 'models/data100k_raw_combined_atlas_cut_dnn_256_3_2', 'nested_concat_general_68_3_64_3': 'models/data100k_raw_combined_atlas_cut_nested_concat_general_68_3_64_3'}
Loaded tripletwise_5_(64, 128, 256, 128, 64)_64 from models/data100k_raw_combined_atlas_cut_tripletwise_5_(64, 128, 256, 128, 64)_64
Loaded pairwise_5_(64, 128, 256, 128, 64)_64 from models/data100k_raw_combined_atlas_cut_pairwise_5_(64, 128, 256, 128, 64)_64
Loaded particlewise_128_4_64 from models/data100k_raw_combined_atlas_cut_particlewise_128_4_64
Loaded pairwise_nl_5_(64, 128, 256, 128, 64)_32_64 from models/data100k_raw_combined_atlas_cut_pairwise_nl_5_(64, 128, 256, 128, 64)_32_64
Loaded pairwise_nl_iter_5_((64, 64, 116, 64, 64), (64, 64, 116, 64, 64), (64, 64, 116, 64, 64))_32_64 from models/data100k_raw_combined_atlas_cut_pairwise_nl_iter_5_((64, 64, 116, 64, 64), (64, 64, 116, 64, 64), (64, 64, 116, 64, 64))_32_64
Loaded nested_concat_70_4_64_3 from models/data100k_raw_combined_atlas_cut_nested_concat_70_4_64_3
Loaded naivednn_256_3_2 from models/data100k_raw_combined_atlas_cut_naivednn_256_3_2
Loaded dnn_256_3_2 from models/data100k_raw_combined_atlas_cut_dnn_256_3_2
Loaded nested_concat_general_68_3_64_3 from models/data100k_raw_combined_atlas_cut_nested_concat_general_68_3_64_3
At 0.5 threshold we have BDT signal efficiency 0.860
alright we're gonna start look at ['tripletwise', 'pairwise', 'particlewise', 'pairwise_nl', 'pairwise_nl_iter', 'nested_concat', 'naivednn', 'dnn', 'nested_concat_general']
getting ROC for tripletwise
currently on tripletwise_5_(64, 128, 256, 128, 64)_64
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 64)_64
getting ROC for particlewise
currently on particlewise_128_4_64
getting ROC for pairwise_nl
currently on pairwise_nl_5_(64, 128, 256, 128, 64)_32_64
getting ROC for pairwise_nl_iter
currently on pairwise_nl_iter_5_((64, 64, 116, 64, 64), (64, 64, 116, 64, 64), (64, 64, 116, 64, 64))_32_64
getting ROC for nested_concat
currently on nested_concat_70_4_64_3
getting ROC for naivednn
currently on naivednn_256_3_2
getting ROC for dnn
currently on dnn_256_3_2
getting ROC for nested_concat_general
currently on nested_concat_general_68_3_64_3
getting ROC for nested_concat_general
pog
getting ROC for nested_concat
pog
getting ROC for naivednn
pog
getting ROC for dnn
pog
getting ROC for particlewise
pog
getting ROC for pairwise_nl_iter
pog
getting ROC for tripletwise
pog
getting ROC for pairwise
pog
getting ROC for pairwise_nl
pog
getting ROC for tripletwise
pog
getting ROC for pairwise
pog
getting ROC for particlewise
pog
getting ROC for pairwise_nl
pog
getting ROC for pairwise_nl_iter
pog
getting ROC for nested_concat
pog
getting ROC for naivednn
pog
getting ROC for dnn
pog
getting ROC for nested_concat_general
pog
