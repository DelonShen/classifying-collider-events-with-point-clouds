nohup: ignoring input
2022-07-12 18:03:07.787408: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/rh/httpd24/root/usr/lib64:/home/delon/madgraph/MG5_aMC_v3_1_1/HEPTools/lhapdf6_py3/lib
2022-07-12 18:03:07.787453: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2022-07-12 18:04:57.221855: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/rh/httpd24/root/usr/lib64:/home/delon/madgraph/MG5_aMC_v3_1_1/HEPTools/lhapdf6_py3/lib
2022-07-12 18:04:57.221926: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)
2022-07-12 18:04:57.221983: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (top1.hep.utexas.edu): /proc/driver/nvidia/version does not exist
2022-07-12 18:04:57.222761: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-07-12 18:04:57.502580: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
2022-07-12 18:04:57.503223: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2397305000 Hz
Initializing Experimenter
	Loading Data from ../data/data100k_raw_combined_atlas_cut.pkl
	Data Loaded
	Creating Splits
	Splits Created
Done initalizing
RIGHT NOW: nested_concat
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/512
3063/3063 - 27s - loss: 0.3893 - val_loss: 0.3576
Epoch 2/512
3063/3063 - 23s - loss: 0.3558 - val_loss: 0.3665
Epoch 3/512
3063/3063 - 30s - loss: 0.3500 - val_loss: 0.3486
Epoch 4/512
3063/3063 - 32s - loss: 0.3435 - val_loss: 0.3501
Epoch 5/512
3063/3063 - 23s - loss: 0.3390 - val_loss: 0.3312
Epoch 6/512
3063/3063 - 23s - loss: 0.3345 - val_loss: 0.3245
Epoch 7/512
3063/3063 - 28s - loss: 0.3295 - val_loss: 0.3381
Epoch 8/512
3063/3063 - 34s - loss: 0.3267 - val_loss: 0.3251
Epoch 9/512
3063/3063 - 24s - loss: 0.3253 - val_loss: 0.3258
Epoch 10/512
3063/3063 - 22s - loss: 0.3232 - val_loss: 0.3343
Epoch 11/512
3063/3063 - 26s - loss: 0.3209 - val_loss: 0.3355
Epoch 12/512
3063/3063 - 34s - loss: 0.3185 - val_loss: 0.3226
Epoch 13/512
3063/3063 - 24s - loss: 0.3179 - val_loss: 0.3189
Epoch 14/512
3063/3063 - 23s - loss: 0.3167 - val_loss: 0.3192
Epoch 15/512
3063/3063 - 24s - loss: 0.3136 - val_loss: 0.3330
Epoch 16/512
3063/3063 - 24s - loss: 0.3135 - val_loss: 0.3220
Epoch 17/512
3063/3063 - 25s - loss: 0.3117 - val_loss: 0.3156
Epoch 18/512
3063/3063 - 23s - loss: 0.3100 - val_loss: 0.3191
Epoch 19/512
3063/3063 - 22s - loss: 0.3086 - val_loss: 0.3153
Epoch 20/512
3063/3063 - 22s - loss: 0.3079 - val_loss: 0.3150
Epoch 21/512
3063/3063 - 21s - loss: 0.3058 - val_loss: 0.3164
Epoch 22/512
3063/3063 - 21s - loss: 0.3047 - val_loss: 0.3337
Epoch 23/512
3063/3063 - 22s - loss: 0.3022 - val_loss: 0.3124
Epoch 24/512
3063/3063 - 21s - loss: 0.3007 - val_loss: 0.3155
Epoch 25/512
3063/3063 - 22s - loss: 0.2974 - val_loss: 0.3061
Epoch 26/512
3063/3063 - 22s - loss: 0.2948 - val_loss: 0.3039
Epoch 27/512
3063/3063 - 22s - loss: 0.2916 - val_loss: 0.3110
Epoch 28/512
3063/3063 - 23s - loss: 0.2905 - val_loss: 0.2946
Epoch 29/512
3063/3063 - 22s - loss: 0.2885 - val_loss: 0.2973
Epoch 30/512
3063/3063 - 22s - loss: 0.2861 - val_loss: 0.2987
Epoch 31/512
3063/3063 - 23s - loss: 0.2827 - val_loss: 0.2927
Epoch 32/512
3063/3063 - 22s - loss: 0.2799 - val_loss: 0.2983
Epoch 33/512
3063/3063 - 22s - loss: 0.2769 - val_loss: 0.2842
Epoch 34/512
3063/3063 - 22s - loss: 0.2721 - val_loss: 0.2880
Epoch 35/512
3063/3063 - 21s - loss: 0.2683 - val_loss: 0.2806
Epoch 36/512
3063/3063 - 23s - loss: 0.2658 - val_loss: 0.2742
Epoch 37/512
3063/3063 - 22s - loss: 0.2618 - val_loss: 0.2756
Epoch 38/512
3063/3063 - 22s - loss: 0.2593 - val_loss: 0.2686
Epoch 39/512
3063/3063 - 22s - loss: 0.2571 - val_loss: 0.2697
Epoch 40/512
3063/3063 - 21s - loss: 0.2546 - val_loss: 0.2821
Epoch 41/512
3063/3063 - 22s - loss: 0.2526 - val_loss: 0.2732
Epoch 42/512
3063/3063 - 23s - loss: 0.2524 - val_loss: 0.2755
Epoch 43/512
3063/3063 - 21s - loss: 0.2494 - val_loss: 0.2763
Epoch 44/512
3063/3063 - 23s - loss: 0.2479 - val_loss: 0.2781
Epoch 45/512
3063/3063 - 22s - loss: 0.2460 - val_loss: 0.2705
Epoch 46/512
3063/3063 - 22s - loss: 0.2442 - val_loss: 0.2681
Epoch 47/512
3063/3063 - 21s - loss: 0.2438 - val_loss: 0.2721
Epoch 48/512
3063/3063 - 22s - loss: 0.2424 - val_loss: 0.2773
Epoch 49/512
3063/3063 - 22s - loss: 0.2406 - val_loss: 0.2798
Epoch 50/512
3063/3063 - 23s - loss: 0.2399 - val_loss: 0.2718
Epoch 51/512
3063/3063 - 23s - loss: 0.2379 - val_loss: 0.2779
Epoch 52/512
3063/3063 - 22s - loss: 0.2366 - val_loss: 0.2733
Epoch 53/512
3063/3063 - 23s - loss: 0.2364 - val_loss: 0.2710
Epoch 54/512
3063/3063 - 23s - loss: 0.2344 - val_loss: 0.2714
Epoch 55/512
3063/3063 - 23s - loss: 0.2333 - val_loss: 0.2659
Epoch 56/512
3063/3063 - 22s - loss: 0.2317 - val_loss: 0.2841
Epoch 57/512
3063/3063 - 22s - loss: 0.2296 - val_loss: 0.2662
Epoch 58/512
3063/3063 - 22s - loss: 0.2295 - val_loss: 0.2735
Epoch 59/512
3063/3063 - 23s - loss: 0.2284 - val_loss: 0.2645
Epoch 60/512
3063/3063 - 22s - loss: 0.2269 - val_loss: 0.2691
Epoch 61/512
3063/3063 - 22s - loss: 0.2256 - val_loss: 0.2674
Epoch 62/512
3063/3063 - 22s - loss: 0.2253 - val_loss: 0.2679
Epoch 63/512
3063/3063 - 23s - loss: 0.2239 - val_loss: 0.2780
Epoch 64/512
3063/3063 - 23s - loss: 0.2227 - val_loss: 0.2661
Epoch 65/512
3063/3063 - 23s - loss: 0.2220 - val_loss: 0.2810
Epoch 66/512
3063/3063 - 21s - loss: 0.2210 - val_loss: 0.2785
Epoch 67/512
3063/3063 - 22s - loss: 0.2182 - val_loss: 0.2661
Epoch 68/512
3063/3063 - 22s - loss: 0.2173 - val_loss: 0.2813
Epoch 69/512
3063/3063 - 22s - loss: 0.2165 - val_loss: 0.2734
Epoch 70/512
3063/3063 - 22s - loss: 0.2159 - val_loss: 0.2715
Epoch 71/512
3063/3063 - 22s - loss: 0.2148 - val_loss: 0.2707
Epoch 72/512
3063/3063 - 22s - loss: 0.2140 - val_loss: 0.2709
Epoch 73/512
3063/3063 - 23s - loss: 0.2120 - val_loss: 0.2676
Epoch 74/512
3063/3063 - 23s - loss: 0.2124 - val_loss: 0.2695
Epoch 75/512
3063/3063 - 22s - loss: 0.2109 - val_loss: 0.2934
Epoch 76/512
3063/3063 - 22s - loss: 0.2088 - val_loss: 0.2755
Epoch 77/512
3063/3063 - 24s - loss: 0.2076 - val_loss: 0.2715
Epoch 78/512
3063/3063 - 23s - loss: 0.2066 - val_loss: 0.2716
Epoch 79/512
3063/3063 - 21s - loss: 0.2060 - val_loss: 0.2810
Epoch 80/512
3063/3063 - 21s - loss: 0.2053 - val_loss: 0.2768
Epoch 81/512
3063/3063 - 22s - loss: 0.2047 - val_loss: 0.2671
Epoch 82/512
3063/3063 - 22s - loss: 0.2041 - val_loss: 0.2882
Epoch 83/512
3063/3063 - 22s - loss: 0.2020 - val_loss: 0.2799
Epoch 84/512
3063/3063 - 22s - loss: 0.2031 - val_loss: 0.2719
Epoch 85/512
3063/3063 - 22s - loss: 0.2007 - val_loss: 0.2815
Epoch 86/512
3063/3063 - 22s - loss: 0.2011 - val_loss: 0.2824
Epoch 87/512
3063/3063 - 22s - loss: 0.1969 - val_loss: 0.2795
Epoch 88/512
3063/3063 - 21s - loss: 0.1974 - val_loss: 0.2842
Epoch 89/512
3063/3063 - 21s - loss: 0.1964 - val_loss: 0.2776
Epoch 90/512
3063/3063 - 22s - loss: 0.1945 - val_loss: 0.2816
Epoch 91/512
3063/3063 - 21s - loss: 0.1947 - val_loss: 0.2825
2022-07-12 18:39:35.313277: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
###
first saving models
currently on nested_concat_70_4_64_3
nested_concat_70_4_64_3 is saved in models/data100k_raw_combined_atlas_cut_nested_concat_70_4_64_3
now saving paramters of experimenter
saved experimenter at /data/delon/experimenter/data100k_raw_combined_atlas_cut
RIGHT NOW: nested_concat_general
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/512
3063/3063 - 25s - loss: 0.3930 - val_loss: 0.3511
Epoch 2/512
3063/3063 - 23s - loss: 0.3552 - val_loss: 0.3684
Epoch 3/512
3063/3063 - 22s - loss: 0.3473 - val_loss: 0.3431
Epoch 4/512
3063/3063 - 22s - loss: 0.3405 - val_loss: 0.3382
Epoch 5/512
3063/3063 - 22s - loss: 0.3344 - val_loss: 0.3280
Epoch 6/512
3063/3063 - 22s - loss: 0.3303 - val_loss: 0.3261
Epoch 7/512
3063/3063 - 22s - loss: 0.3253 - val_loss: 0.3317
Epoch 8/512
3063/3063 - 22s - loss: 0.3225 - val_loss: 0.3227
Epoch 9/512
3063/3063 - 22s - loss: 0.3210 - val_loss: 0.3226
Epoch 10/512
3063/3063 - 22s - loss: 0.3184 - val_loss: 0.3248
Epoch 11/512
3063/3063 - 22s - loss: 0.3155 - val_loss: 0.3249
Epoch 12/512
3063/3063 - 22s - loss: 0.3137 - val_loss: 0.3207
Epoch 13/512
3063/3063 - 22s - loss: 0.3122 - val_loss: 0.3153
Epoch 14/512
3063/3063 - 23s - loss: 0.3105 - val_loss: 0.3219
Epoch 15/512
3063/3063 - 22s - loss: 0.3074 - val_loss: 0.3247
Epoch 16/512
3063/3063 - 22s - loss: 0.3075 - val_loss: 0.3256
Epoch 17/512
3063/3063 - 22s - loss: 0.3051 - val_loss: 0.3115
Epoch 18/512
3063/3063 - 22s - loss: 0.3033 - val_loss: 0.3193
Epoch 19/512
3063/3063 - 22s - loss: 0.3022 - val_loss: 0.3140
Epoch 20/512
3063/3063 - 22s - loss: 0.3019 - val_loss: 0.3141
Epoch 21/512
3063/3063 - 22s - loss: 0.3003 - val_loss: 0.3077
Epoch 22/512
3063/3063 - 22s - loss: 0.2993 - val_loss: 0.3540
Epoch 23/512
3063/3063 - 22s - loss: 0.2977 - val_loss: 0.3097
Epoch 24/512
3063/3063 - 22s - loss: 0.2960 - val_loss: 0.3248
Epoch 25/512
3063/3063 - 22s - loss: 0.2955 - val_loss: 0.3138
Epoch 26/512
3063/3063 - 22s - loss: 0.2944 - val_loss: 0.3037
Epoch 27/512
3063/3063 - 22s - loss: 0.2922 - val_loss: 0.3104
Epoch 28/512
3063/3063 - 22s - loss: 0.2908 - val_loss: 0.3023
Epoch 29/512
3063/3063 - 22s - loss: 0.2895 - val_loss: 0.3055
Epoch 30/512
3063/3063 - 22s - loss: 0.2862 - val_loss: 0.3125
Epoch 31/512
3063/3063 - 22s - loss: 0.2859 - val_loss: 0.3073
Epoch 32/512
3063/3063 - 22s - loss: 0.2840 - val_loss: 0.3094
Epoch 33/512
3063/3063 - 22s - loss: 0.2832 - val_loss: 0.3010
Epoch 34/512
3063/3063 - 22s - loss: 0.2812 - val_loss: 0.3162
Epoch 35/512
3063/3063 - 22s - loss: 0.2795 - val_loss: 0.3000
Epoch 36/512
3063/3063 - 22s - loss: 0.2784 - val_loss: 0.3022
Epoch 37/512
3063/3063 - 22s - loss: 0.2770 - val_loss: 0.2996
Epoch 38/512
3063/3063 - 22s - loss: 0.2757 - val_loss: 0.3024
Epoch 39/512
3063/3063 - 22s - loss: 0.2745 - val_loss: 0.3074
Epoch 40/512
3063/3063 - 22s - loss: 0.2729 - val_loss: 0.3032
Epoch 41/512
3063/3063 - 22s - loss: 0.2720 - val_loss: 0.3026
Epoch 42/512
3063/3063 - 22s - loss: 0.2707 - val_loss: 0.3047
Epoch 43/512
3063/3063 - 22s - loss: 0.2700 - val_loss: 0.2992
Epoch 44/512
3063/3063 - 22s - loss: 0.2683 - val_loss: 0.3047
Epoch 45/512
3063/3063 - 22s - loss: 0.2669 - val_loss: 0.3146
Epoch 46/512
3063/3063 - 22s - loss: 0.2651 - val_loss: 0.3005
Epoch 47/512
3063/3063 - 22s - loss: 0.2640 - val_loss: 0.3087
Epoch 48/512
3063/3063 - 22s - loss: 0.2634 - val_loss: 0.3203
Epoch 49/512
3063/3063 - 22s - loss: 0.2616 - val_loss: 0.3004
Epoch 50/512
3063/3063 - 22s - loss: 0.2589 - val_loss: 0.3116
Epoch 51/512
3063/3063 - 23s - loss: 0.2590 - val_loss: 0.3147
Epoch 52/512
3063/3063 - 22s - loss: 0.2573 - val_loss: 0.3063
Epoch 53/512
3063/3063 - 22s - loss: 0.2557 - val_loss: 0.3069
Epoch 54/512
3063/3063 - 22s - loss: 0.2537 - val_loss: 0.3059
Epoch 55/512
3063/3063 - 22s - loss: 0.2528 - val_loss: 0.3142
Epoch 56/512
3063/3063 - 22s - loss: 0.2520 - val_loss: 0.3052
Epoch 57/512
3063/3063 - 22s - loss: 0.2493 - val_loss: 0.3105
Epoch 58/512
3063/3063 - 22s - loss: 0.2501 - val_loss: 0.3113
Epoch 59/512
3063/3063 - 22s - loss: 0.2464 - val_loss: 0.3143
Epoch 60/512
3063/3063 - 22s - loss: 0.2455 - val_loss: 0.3142
Epoch 61/512
3063/3063 - 22s - loss: 0.2447 - val_loss: 0.3171
Epoch 62/512
3063/3063 - 22s - loss: 0.2426 - val_loss: 0.3214
Epoch 63/512
3063/3063 - 22s - loss: 0.2409 - val_loss: 0.3375
Epoch 64/512
3063/3063 - 22s - loss: 0.2400 - val_loss: 0.3018
Epoch 65/512
3063/3063 - 22s - loss: 0.2381 - val_loss: 0.3084
Epoch 66/512
3063/3063 - 22s - loss: 0.2374 - val_loss: 0.3246
Epoch 67/512
3063/3063 - 22s - loss: 0.2357 - val_loss: 0.3074
Epoch 68/512
3063/3063 - 22s - loss: 0.2346 - val_loss: 0.3063
Epoch 69/512
3063/3063 - 22s - loss: 0.2312 - val_loss: 0.3177
Epoch 70/512
3063/3063 - 22s - loss: 0.2291 - val_loss: 0.3157
Epoch 71/512
3063/3063 - 22s - loss: 0.2288 - val_loss: 0.3117
Epoch 72/512
3063/3063 - 22s - loss: 0.2265 - val_loss: 0.3148
Epoch 73/512
3063/3063 - 22s - loss: 0.2263 - val_loss: 0.3168
Epoch 74/512
3063/3063 - 22s - loss: 0.2242 - val_loss: 0.3098
Epoch 75/512
3063/3063 - 22s - loss: 0.2208 - val_loss: 0.3323
###
first saving models
currently on nested_concat_70_4_64_3
	this one already saved, skipped
currently on nested_concat_general_68_3_64_3
nested_concat_general_68_3_64_3 is saved in models/data100k_raw_combined_atlas_cut_nested_concat_general_68_3_64_3
now saving paramters of experimenter
saved experimenter at /data/delon/experimenter/data100k_raw_combined_atlas_cut
DNN Classifier
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/512
3063/3063 - 7s - loss: 0.3947 - val_loss: 0.3559
Epoch 2/512
3063/3063 - 6s - loss: 0.3830 - val_loss: 0.3595
Epoch 3/512
3063/3063 - 6s - loss: 0.3819 - val_loss: 0.3673
Epoch 4/512
3063/3063 - 6s - loss: 0.3805 - val_loss: 0.3542
Epoch 5/512
3063/3063 - 6s - loss: 0.3783 - val_loss: 0.3497
Epoch 6/512
3063/3063 - 6s - loss: 0.3793 - val_loss: 0.3491
Epoch 7/512
3063/3063 - 6s - loss: 0.3763 - val_loss: 0.3533
Epoch 8/512
3063/3063 - 6s - loss: 0.3777 - val_loss: 0.3533
Epoch 9/512
3063/3063 - 6s - loss: 0.3765 - val_loss: 0.3518
Epoch 10/512
3063/3063 - 6s - loss: 0.3761 - val_loss: 0.3485
Epoch 11/512
3063/3063 - 6s - loss: 0.3755 - val_loss: 0.3496
Epoch 12/512
3063/3063 - 6s - loss: 0.3752 - val_loss: 0.3473
Epoch 13/512
3063/3063 - 6s - loss: 0.3756 - val_loss: 0.3518
Epoch 14/512
3063/3063 - 6s - loss: 0.3763 - val_loss: 0.3496
Epoch 15/512
3063/3063 - 6s - loss: 0.3748 - val_loss: 0.3510
Epoch 16/512
3063/3063 - 6s - loss: 0.3752 - val_loss: 0.3490
Epoch 17/512
3063/3063 - 6s - loss: 0.3753 - val_loss: 0.3465
Epoch 18/512
3063/3063 - 6s - loss: 0.3739 - val_loss: 0.3493
Epoch 19/512
3063/3063 - 6s - loss: 0.3744 - val_loss: 0.3509
Epoch 20/512
3063/3063 - 6s - loss: 0.3743 - val_loss: 0.3473
Epoch 21/512
3063/3063 - 6s - loss: 0.3733 - val_loss: 0.3495
Epoch 22/512
3063/3063 - 6s - loss: 0.3742 - val_loss: 0.3503
Epoch 23/512
3063/3063 - 6s - loss: 0.3728 - val_loss: 0.3477
Epoch 24/512
3063/3063 - 6s - loss: 0.3731 - val_loss: 0.3475
Epoch 25/512
3063/3063 - 6s - loss: 0.3728 - val_loss: 0.3492
Epoch 26/512
3063/3063 - 6s - loss: 0.3737 - val_loss: 0.3504
Epoch 27/512
3063/3063 - 6s - loss: 0.3743 - val_loss: 0.3478
Epoch 28/512
3063/3063 - 6s - loss: 0.3738 - val_loss: 0.3464
Epoch 29/512
3063/3063 - 6s - loss: 0.3734 - val_loss: 0.3493
Epoch 30/512
3063/3063 - 6s - loss: 0.3728 - val_loss: 0.3456
Epoch 31/512
3063/3063 - 6s - loss: 0.3736 - val_loss: 0.3465
Epoch 32/512
3063/3063 - 6s - loss: 0.3722 - val_loss: 0.3476
Epoch 33/512
3063/3063 - 6s - loss: 0.3738 - val_loss: 0.3477
Epoch 34/512
3063/3063 - 6s - loss: 0.3731 - val_loss: 0.3532
Epoch 35/512
3063/3063 - 6s - loss: 0.3723 - val_loss: 0.3466
Epoch 36/512
3063/3063 - 6s - loss: 0.3732 - val_loss: 0.3480
Epoch 37/512
3063/3063 - 6s - loss: 0.3715 - val_loss: 0.3495
Epoch 38/512
3063/3063 - 6s - loss: 0.3724 - val_loss: 0.3481
Epoch 39/512
3063/3063 - 6s - loss: 0.3737 - val_loss: 0.3526
Epoch 40/512
3063/3063 - 6s - loss: 0.3726 - val_loss: 0.3507
Epoch 41/512
3063/3063 - 6s - loss: 0.3711 - val_loss: 0.3469
Epoch 42/512
3063/3063 - 6s - loss: 0.3725 - val_loss: 0.3500
Epoch 43/512
3063/3063 - 6s - loss: 0.3725 - val_loss: 0.3477
Epoch 44/512
3063/3063 - 6s - loss: 0.3741 - val_loss: 0.3460
Epoch 45/512
3063/3063 - 6s - loss: 0.3724 - val_loss: 0.3495
Epoch 46/512
3063/3063 - 6s - loss: 0.3726 - val_loss: 0.3475
Epoch 47/512
3063/3063 - 6s - loss: 0.3721 - val_loss: 0.3482
Epoch 48/512
3063/3063 - 6s - loss: 0.3726 - val_loss: 0.3497
Epoch 49/512
3063/3063 - 6s - loss: 0.3726 - val_loss: 0.3512
Epoch 50/512
3063/3063 - 6s - loss: 0.3718 - val_loss: 0.3455
Epoch 51/512
3063/3063 - 6s - loss: 0.3729 - val_loss: 0.3489
Epoch 52/512
3063/3063 - 6s - loss: 0.3727 - val_loss: 0.3458
Epoch 53/512
3063/3063 - 6s - loss: 0.3712 - val_loss: 0.3488
Epoch 54/512
3063/3063 - 6s - loss: 0.3730 - val_loss: 0.3462
Epoch 55/512
3063/3063 - 6s - loss: 0.3738 - val_loss: 0.3474
Epoch 56/512
3063/3063 - 6s - loss: 0.3717 - val_loss: 0.3513
Epoch 57/512
3063/3063 - 6s - loss: 0.3708 - val_loss: 0.3466
Epoch 58/512
3063/3063 - 6s - loss: 0.3719 - val_loss: 0.3468
Epoch 59/512
3063/3063 - 6s - loss: 0.3725 - val_loss: 0.3463
Epoch 60/512
3063/3063 - 6s - loss: 0.3719 - val_loss: 0.3495
Epoch 61/512
3063/3063 - 6s - loss: 0.3709 - val_loss: 0.3466
Epoch 62/512
3063/3063 - 6s - loss: 0.3716 - val_loss: 0.3515
Epoch 63/512
3063/3063 - 5s - loss: 0.3724 - val_loss: 0.3454
Epoch 64/512
3063/3063 - 6s - loss: 0.3714 - val_loss: 0.3457
Epoch 65/512
3063/3063 - 6s - loss: 0.3716 - val_loss: 0.3472
Epoch 66/512
3063/3063 - 6s - loss: 0.3720 - val_loss: 0.3487
Epoch 67/512
3063/3063 - 6s - loss: 0.3734 - val_loss: 0.3470
Epoch 68/512
3063/3063 - 6s - loss: 0.3713 - val_loss: 0.3503
Epoch 69/512
3063/3063 - 6s - loss: 0.3714 - val_loss: 0.3474
Epoch 70/512
3063/3063 - 6s - loss: 0.3712 - val_loss: 0.3475
Epoch 71/512
3063/3063 - 6s - loss: 0.3715 - val_loss: 0.3457
Epoch 72/512
3063/3063 - 6s - loss: 0.3719 - val_loss: 0.3501
Epoch 73/512
3063/3063 - 6s - loss: 0.3708 - val_loss: 0.3495
Epoch 74/512
3063/3063 - 6s - loss: 0.3717 - val_loss: 0.3462
Epoch 75/512
3063/3063 - 6s - loss: 0.3717 - val_loss: 0.3483
Epoch 76/512
3063/3063 - 6s - loss: 0.3707 - val_loss: 0.3464
Epoch 77/512
3063/3063 - 6s - loss: 0.3706 - val_loss: 0.3473
Epoch 78/512
3063/3063 - 6s - loss: 0.3711 - val_loss: 0.3474
Epoch 79/512
3063/3063 - 6s - loss: 0.3705 - val_loss: 0.3470
Epoch 80/512
3063/3063 - 6s - loss: 0.3711 - val_loss: 0.3454
Epoch 81/512
3063/3063 - 6s - loss: 0.3716 - val_loss: 0.3459
Epoch 82/512
3063/3063 - 6s - loss: 0.3703 - val_loss: 0.3493
Epoch 83/512
3063/3063 - 6s - loss: 0.3714 - val_loss: 0.3462
Epoch 84/512
3063/3063 - 5s - loss: 0.3717 - val_loss: 0.3472
Epoch 85/512
3063/3063 - 5s - loss: 0.3708 - val_loss: 0.3474
Epoch 86/512
3063/3063 - 6s - loss: 0.3716 - val_loss: 0.3464
Epoch 87/512
3063/3063 - 6s - loss: 0.3721 - val_loss: 0.3471
Epoch 88/512
3063/3063 - 6s - loss: 0.3717 - val_loss: 0.3494
Epoch 89/512
3063/3063 - 6s - loss: 0.3696 - val_loss: 0.3495
Epoch 90/512
3063/3063 - 6s - loss: 0.3711 - val_loss: 0.3469
Epoch 91/512
3063/3063 - 6s - loss: 0.3716 - val_loss: 0.3535
Epoch 92/512
3063/3063 - 6s - loss: 0.3707 - val_loss: 0.3521
Epoch 93/512
3063/3063 - 6s - loss: 0.3713 - val_loss: 0.3466
Epoch 94/512
3063/3063 - 6s - loss: 0.3711 - val_loss: 0.3469
Epoch 95/512
3063/3063 - 6s - loss: 0.3707 - val_loss: 0.3485
###
first saving models
currently on nested_concat_70_4_64_3
	this one already saved, skipped
currently on nested_concat_general_68_3_64_3
	this one already saved, skipped
currently on dnn_256_3_2
dnn_256_3_2 is saved in models/data100k_raw_combined_atlas_cut_dnn_256_3_2
now saving paramters of experimenter
saved experimenter at /data/delon/experimenter/data100k_raw_combined_atlas_cut
2022-07-12 19:19:34.103292: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/rh/httpd24/root/usr/lib64:/home/delon/madgraph/MG5_aMC_v3_1_1/HEPTools/lhapdf6_py3/lib
2022-07-12 19:19:34.103352: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2022-07-12 19:20:03.036687: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/rh/httpd24/root/usr/lib64:/home/delon/madgraph/MG5_aMC_v3_1_1/HEPTools/lhapdf6_py3/lib
2022-07-12 19:20:03.037354: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)
2022-07-12 19:20:03.037399: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (top1.hep.utexas.edu): /proc/driver/nvidia/version does not exist
2022-07-12 19:20:03.037918: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-07-12 19:20:43.829948: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
2022-07-12 19:20:43.830836: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2397305000 Hz
Initializing Experimenter
	Loading Data from ../data/data100k_raw_combined_atlas_cut.pkl
	Data Loaded
	Creating Splits
	Splits Created
Done initalizing
Loading Experimenter from Saved Experimenter at /data/delon/experimenter/data100k_raw_combined_atlas_cut
Experimenter Loaded
Getting split
Split Stored
Loading models
{'nested_concat_70_4_64_3': 'models/data100k_raw_combined_atlas_cut_nested_concat_70_4_64_3', 'nested_concat_general_68_3_64_3': 'models/data100k_raw_combined_atlas_cut_nested_concat_general_68_3_64_3', 'dnn_256_3_2': 'models/data100k_raw_combined_atlas_cut_dnn_256_3_2'}
Loaded nested_concat_70_4_64_3 from models/data100k_raw_combined_atlas_cut_nested_concat_70_4_64_3
Loaded nested_concat_general_68_3_64_3 from models/data100k_raw_combined_atlas_cut_nested_concat_general_68_3_64_3
Loaded dnn_256_3_2 from models/data100k_raw_combined_atlas_cut_dnn_256_3_2
At 0.5 threshold we have BDT signal efficiency 0.860
alright we're gonna start look at ['nested_concat', 'nested_concat_general', 'dnn']
getting ROC for nested_concat
currently on nested_concat_70_4_64_3
getting ROC for nested_concat_general
currently on nested_concat_general_68_3_64_3
getting ROC for dnn
currently on dnn_256_3_2
getting ROC for dnn
pog
getting ROC for nested_concat_general
pog
getting ROC for nested_concat
pog
getting ROC for nested_concat
pog
getting ROC for nested_concat_general
pog
getting ROC for dnn
pog
