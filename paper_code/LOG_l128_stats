nohup: ignoring input
2022-06-22 12:20:08.697380: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-06-22 12:21:40.099637: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1
2022-06-22 12:21:40.116360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:84:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2022-06-22 12:21:40.117390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: 
pciBusID: 0000:85:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2022-06-22 12:21:40.117425: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-06-22 12:21:40.121145: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2022-06-22 12:21:40.121206: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
2022-06-22 12:21:40.122819: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10
2022-06-22 12:21:40.123135: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10
2022-06-22 12:21:40.127766: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11
2022-06-22 12:21:40.128922: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11
2022-06-22 12:21:40.129162: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2022-06-22 12:21:40.133087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1
2022-06-22 12:21:40.133818: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-06-22 12:21:40.253186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:84:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2022-06-22 12:21:40.254212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: 
pciBusID: 0000:85:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2022-06-22 12:21:40.257845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1
2022-06-22 12:21:40.257908: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-06-22 12:21:41.591027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-22 12:21:41.591083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1 
2022-06-22 12:21:41.591099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N Y 
2022-06-22 12:21:41.591108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   Y N 
2022-06-22 12:21:41.596098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8599 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:84:00.0, compute capability: 3.7)
2022-06-22 12:21:41.598252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10792 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id: 0000:85:00.0, compute capability: 3.7)
2022-06-22 12:21:42.246376: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
2022-06-22 12:21:42.247116: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2099935000 Hz
2022-06-22 12:21:44.388678: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2022-06-22 12:21:44.771330: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8201
2022-06-22 12:21:45.507764: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2022-06-22 12:21:45.788601: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
Initializing Experimenter
	Loading Data from ../data/data100k_raw_combined_atlas_cut.pkl
	Data Loaded
	Creating Splits
	Splits Created
Done initalizing
		LATENT DIM 128
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 46s - loss: 0.3957 - val_loss: 0.3564
Epoch 2/256
3063/3063 - 41s - loss: 0.3537 - val_loss: 0.3611
Epoch 3/256
3063/3063 - 42s - loss: 0.3425 - val_loss: 0.3350
Epoch 4/256
3063/3063 - 42s - loss: 0.3317 - val_loss: 0.3476
Epoch 5/256
3063/3063 - 42s - loss: 0.3215 - val_loss: 0.3126
Epoch 6/256
3063/3063 - 41s - loss: 0.3100 - val_loss: 0.3083
Epoch 7/256
3063/3063 - 41s - loss: 0.3009 - val_loss: 0.3045
Epoch 8/256
3063/3063 - 41s - loss: 0.2905 - val_loss: 0.2855
Epoch 9/256
3063/3063 - 41s - loss: 0.2855 - val_loss: 0.2899
Epoch 10/256
3063/3063 - 41s - loss: 0.2782 - val_loss: 0.2938
Epoch 11/256
3063/3063 - 42s - loss: 0.2724 - val_loss: 0.2715
Epoch 12/256
3063/3063 - 41s - loss: 0.2681 - val_loss: 0.2773
Epoch 13/256
3063/3063 - 41s - loss: 0.2649 - val_loss: 0.2642
Epoch 14/256
3063/3063 - 41s - loss: 0.2594 - val_loss: 0.2801
Epoch 15/256
3063/3063 - 42s - loss: 0.2561 - val_loss: 0.2631
Epoch 16/256
3063/3063 - 41s - loss: 0.2535 - val_loss: 0.2568
Epoch 17/256
3063/3063 - 41s - loss: 0.2514 - val_loss: 0.2549
Epoch 18/256
3063/3063 - 41s - loss: 0.2484 - val_loss: 0.2520
Epoch 19/256
3063/3063 - 41s - loss: 0.2459 - val_loss: 0.2551
Epoch 20/256
3063/3063 - 41s - loss: 0.2449 - val_loss: 0.2505
Epoch 21/256
3063/3063 - 41s - loss: 0.2427 - val_loss: 0.2635
Epoch 22/256
3063/3063 - 41s - loss: 0.2408 - val_loss: 0.2995
Epoch 23/256
3063/3063 - 41s - loss: 0.2386 - val_loss: 0.2495
Epoch 24/256
3063/3063 - 42s - loss: 0.2376 - val_loss: 0.2633
Epoch 25/256
3063/3063 - 42s - loss: 0.2355 - val_loss: 0.2460
Epoch 26/256
3063/3063 - 41s - loss: 0.2330 - val_loss: 0.2485
Epoch 27/256
3063/3063 - 41s - loss: 0.2321 - val_loss: 0.2538
Epoch 28/256
3063/3063 - 42s - loss: 0.2309 - val_loss: 0.2422
Epoch 29/256
3063/3063 - 42s - loss: 0.2287 - val_loss: 0.2480
Epoch 30/256
3063/3063 - 42s - loss: 0.2275 - val_loss: 0.2537
Epoch 31/256
3063/3063 - 42s - loss: 0.2271 - val_loss: 0.2481
Epoch 32/256
3063/3063 - 41s - loss: 0.2251 - val_loss: 0.2821
Epoch 33/256
3063/3063 - 42s - loss: 0.2233 - val_loss: 0.2460
Epoch 34/256
3063/3063 - 41s - loss: 0.2228 - val_loss: 0.2441
Epoch 35/256
3063/3063 - 42s - loss: 0.2211 - val_loss: 0.2524
Epoch 36/256
3063/3063 - 42s - loss: 0.2197 - val_loss: 0.2456
Epoch 37/256
3063/3063 - 41s - loss: 0.2181 - val_loss: 0.2482
Epoch 38/256
3063/3063 - 41s - loss: 0.2176 - val_loss: 0.2474
Epoch 39/256
3063/3063 - 41s - loss: 0.2160 - val_loss: 0.2442
Epoch 40/256
3063/3063 - 41s - loss: 0.2146 - val_loss: 0.2454
Epoch 41/256
3063/3063 - 41s - loss: 0.2139 - val_loss: 0.2436
Epoch 42/256
3063/3063 - 41s - loss: 0.2132 - val_loss: 0.2458
Epoch 43/256
3063/3063 - 42s - loss: 0.2110 - val_loss: 0.2446
Epoch 44/256
3063/3063 - 42s - loss: 0.2097 - val_loss: 0.2405
Epoch 45/256
3063/3063 - 42s - loss: 0.2089 - val_loss: 0.2416
Epoch 46/256
3063/3063 - 42s - loss: 0.2074 - val_loss: 0.2443
Epoch 47/256
3063/3063 - 42s - loss: 0.2059 - val_loss: 0.2666
Epoch 48/256
3063/3063 - 42s - loss: 0.2055 - val_loss: 0.2377
Epoch 49/256
3063/3063 - 41s - loss: 0.2054 - val_loss: 0.2513
Epoch 50/256
3063/3063 - 42s - loss: 0.2031 - val_loss: 0.2485
Epoch 51/256
3063/3063 - 42s - loss: 0.2032 - val_loss: 0.2593
Epoch 52/256
3063/3063 - 41s - loss: 0.2011 - val_loss: 0.2501
Epoch 53/256
3063/3063 - 41s - loss: 0.1996 - val_loss: 0.2505
Epoch 54/256
3063/3063 - 41s - loss: 0.1998 - val_loss: 0.2402
Epoch 55/256
3063/3063 - 41s - loss: 0.1993 - val_loss: 0.2408
Epoch 56/256
3063/3063 - 41s - loss: 0.1976 - val_loss: 0.2401
Epoch 57/256
3063/3063 - 40s - loss: 0.1958 - val_loss: 0.2506
Epoch 58/256
3063/3063 - 41s - loss: 0.1947 - val_loss: 0.2578
Epoch 59/256
3063/3063 - 41s - loss: 0.1941 - val_loss: 0.2485
Epoch 60/256
3063/3063 - 40s - loss: 0.1929 - val_loss: 0.2513
Epoch 61/256
3063/3063 - 41s - loss: 0.1921 - val_loss: 0.2614
Epoch 62/256
3063/3063 - 41s - loss: 0.1896 - val_loss: 0.2749
Epoch 63/256
3063/3063 - 41s - loss: 0.1895 - val_loss: 0.2578
Epoch 64/256
3063/3063 - 41s - loss: 0.1893 - val_loss: 0.2560
Epoch 65/256
3063/3063 - 41s - loss: 0.1867 - val_loss: 0.2563
Epoch 66/256
3063/3063 - 41s - loss: 0.1861 - val_loss: 0.2576
Epoch 67/256
3063/3063 - 41s - loss: 0.1845 - val_loss: 0.2427
Epoch 68/256
3063/3063 - 42s - loss: 0.1824 - val_loss: 0.2555
Epoch 69/256
3063/3063 - 41s - loss: 0.1824 - val_loss: 0.2526
Epoch 70/256
3063/3063 - 42s - loss: 0.1816 - val_loss: 0.2618
Epoch 71/256
3063/3063 - 41s - loss: 0.1809 - val_loss: 0.2546
Epoch 72/256
3063/3063 - 42s - loss: 0.1792 - val_loss: 0.2801
Epoch 73/256
3063/3063 - 41s - loss: 0.1781 - val_loss: 0.2504
Epoch 74/256
3063/3063 - 42s - loss: 0.1769 - val_loss: 0.2555
Epoch 75/256
3063/3063 - 41s - loss: 0.1757 - val_loss: 0.2638
Epoch 76/256
3063/3063 - 41s - loss: 0.1742 - val_loss: 0.2669
Epoch 77/256
3063/3063 - 39s - loss: 0.1733 - val_loss: 0.2560
Epoch 78/256
3063/3063 - 40s - loss: 0.1714 - val_loss: 0.2483
Epoch 79/256
3063/3063 - 40s - loss: 0.1718 - val_loss: 0.2632
Epoch 80/256
3063/3063 - 41s - loss: 0.1687 - val_loss: 0.2644
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 128)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 44s - loss: 0.3903 - val_loss: 0.3785
Epoch 2/256
3063/3063 - 42s - loss: 0.3556 - val_loss: 0.3456
Epoch 3/256
3063/3063 - 41s - loss: 0.3475 - val_loss: 0.3656
Epoch 4/256
3063/3063 - 42s - loss: 0.3421 - val_loss: 0.3544
Epoch 5/256
3063/3063 - 41s - loss: 0.3357 - val_loss: 0.3547
Epoch 6/256
3063/3063 - 42s - loss: 0.3276 - val_loss: 0.3226
Epoch 7/256
3063/3063 - 42s - loss: 0.3159 - val_loss: 0.3082
Epoch 8/256
3063/3063 - 42s - loss: 0.3048 - val_loss: 0.3076
Epoch 9/256
3063/3063 - 41s - loss: 0.2955 - val_loss: 0.2912
Epoch 10/256
3063/3063 - 42s - loss: 0.2871 - val_loss: 0.2807
Epoch 11/256
3063/3063 - 42s - loss: 0.2815 - val_loss: 0.2729
Epoch 12/256
3063/3063 - 42s - loss: 0.2757 - val_loss: 0.2703
Epoch 13/256
3063/3063 - 41s - loss: 0.2702 - val_loss: 0.2842
Epoch 14/256
3063/3063 - 42s - loss: 0.2674 - val_loss: 0.2782
Epoch 15/256
3063/3063 - 42s - loss: 0.2633 - val_loss: 0.2671
Epoch 16/256
3063/3063 - 42s - loss: 0.2584 - val_loss: 0.2705
Epoch 17/256
3063/3063 - 42s - loss: 0.2568 - val_loss: 0.2591
Epoch 18/256
3063/3063 - 42s - loss: 0.2530 - val_loss: 0.2719
Epoch 19/256
3063/3063 - 42s - loss: 0.2495 - val_loss: 0.2614
Epoch 20/256
3063/3063 - 42s - loss: 0.2467 - val_loss: 0.2517
Epoch 21/256
3063/3063 - 42s - loss: 0.2461 - val_loss: 0.2554
Epoch 22/256
3063/3063 - 42s - loss: 0.2437 - val_loss: 0.2458
Epoch 23/256
3063/3063 - 41s - loss: 0.2402 - val_loss: 0.2459
Epoch 24/256
3063/3063 - 42s - loss: 0.2383 - val_loss: 0.2668
Epoch 25/256
3063/3063 - 42s - loss: 0.2370 - val_loss: 0.2506
Epoch 26/256
3063/3063 - 42s - loss: 0.2342 - val_loss: 0.2617
Epoch 27/256
3063/3063 - 42s - loss: 0.2326 - val_loss: 0.2431
Epoch 28/256
3063/3063 - 42s - loss: 0.2310 - val_loss: 0.2541
Epoch 29/256
3063/3063 - 42s - loss: 0.2290 - val_loss: 0.2425
Epoch 30/256
3063/3063 - 41s - loss: 0.2266 - val_loss: 0.2458
Epoch 31/256
3063/3063 - 41s - loss: 0.2242 - val_loss: 0.2539
Epoch 32/256
3063/3063 - 41s - loss: 0.2239 - val_loss: 0.2421
Epoch 33/256
3063/3063 - 42s - loss: 0.2224 - val_loss: 0.2485
Epoch 34/256
3063/3063 - 41s - loss: 0.2203 - val_loss: 0.2504
Epoch 35/256
3063/3063 - 42s - loss: 0.2192 - val_loss: 0.2500
Epoch 36/256
3063/3063 - 41s - loss: 0.2183 - val_loss: 0.2441
Epoch 37/256
3063/3063 - 42s - loss: 0.2158 - val_loss: 0.2419
Epoch 38/256
3063/3063 - 41s - loss: 0.2153 - val_loss: 0.2362
Epoch 39/256
3063/3063 - 42s - loss: 0.2144 - val_loss: 0.2544
Epoch 40/256
3063/3063 - 42s - loss: 0.2118 - val_loss: 0.2400
Epoch 41/256
3063/3063 - 42s - loss: 0.2110 - val_loss: 0.2448
Epoch 42/256
3063/3063 - 42s - loss: 0.2104 - val_loss: 0.2410
Epoch 43/256
3063/3063 - 42s - loss: 0.2089 - val_loss: 0.2470
Epoch 44/256
3063/3063 - 42s - loss: 0.2079 - val_loss: 0.2464
Epoch 45/256
3063/3063 - 42s - loss: 0.2056 - val_loss: 0.2412
Epoch 46/256
3063/3063 - 42s - loss: 0.2048 - val_loss: 0.2467
Epoch 47/256
3063/3063 - 42s - loss: 0.2041 - val_loss: 0.2369
Epoch 48/256
3063/3063 - 42s - loss: 0.2025 - val_loss: 0.2450
Epoch 49/256
3063/3063 - 42s - loss: 0.2009 - val_loss: 0.2424
Epoch 50/256
3063/3063 - 42s - loss: 0.1994 - val_loss: 0.2389
Epoch 51/256
3063/3063 - 43s - loss: 0.1997 - val_loss: 0.2611
Epoch 52/256
3063/3063 - 42s - loss: 0.1975 - val_loss: 0.2505
Epoch 53/256
3063/3063 - 42s - loss: 0.1963 - val_loss: 0.2411
Epoch 54/256
3063/3063 - 41s - loss: 0.1959 - val_loss: 0.2445
Epoch 55/256
3063/3063 - 41s - loss: 0.1942 - val_loss: 0.2583
Epoch 56/256
3063/3063 - 42s - loss: 0.1933 - val_loss: 0.2579
Epoch 57/256
3063/3063 - 41s - loss: 0.1921 - val_loss: 0.2403
Epoch 58/256
3063/3063 - 41s - loss: 0.1895 - val_loss: 0.2461
Epoch 59/256
3063/3063 - 42s - loss: 0.1897 - val_loss: 0.2449
Epoch 60/256
3063/3063 - 42s - loss: 0.1875 - val_loss: 0.2525
Epoch 61/256
3063/3063 - 42s - loss: 0.1874 - val_loss: 0.2471
Epoch 62/256
3063/3063 - 42s - loss: 0.1866 - val_loss: 0.2487
Epoch 63/256
3063/3063 - 42s - loss: 0.1851 - val_loss: 0.2572
Epoch 64/256
3063/3063 - 42s - loss: 0.1847 - val_loss: 0.2460
Epoch 65/256
3063/3063 - 42s - loss: 0.1822 - val_loss: 0.2510
Epoch 66/256
3063/3063 - 41s - loss: 0.1810 - val_loss: 0.2598
Epoch 67/256
3063/3063 - 41s - loss: 0.1799 - val_loss: 0.2568
Epoch 68/256
3063/3063 - 42s - loss: 0.1789 - val_loss: 0.2605
Epoch 69/256
3063/3063 - 42s - loss: 0.1777 - val_loss: 0.2672
Epoch 70/256
3063/3063 - 42s - loss: 0.1760 - val_loss: 0.2737
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 128)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 44s - loss: 0.3919 - val_loss: 0.3721
Epoch 2/256
3063/3063 - 41s - loss: 0.3567 - val_loss: 0.3600
Epoch 3/256
3063/3063 - 42s - loss: 0.3467 - val_loss: 0.3326
Epoch 4/256
3063/3063 - 42s - loss: 0.3388 - val_loss: 0.3300
Epoch 5/256
3063/3063 - 42s - loss: 0.3296 - val_loss: 0.3291
Epoch 6/256
3063/3063 - 43s - loss: 0.3202 - val_loss: 0.3177
Epoch 7/256
3063/3063 - 42s - loss: 0.3094 - val_loss: 0.2953
Epoch 8/256
3063/3063 - 42s - loss: 0.2996 - val_loss: 0.2921
Epoch 9/256
3063/3063 - 42s - loss: 0.2916 - val_loss: 0.2879
Epoch 10/256
3063/3063 - 41s - loss: 0.2837 - val_loss: 0.2903
Epoch 11/256
3063/3063 - 41s - loss: 0.2784 - val_loss: 0.2878
Epoch 12/256
3063/3063 - 41s - loss: 0.2744 - val_loss: 0.2920
Epoch 13/256
3063/3063 - 42s - loss: 0.2693 - val_loss: 0.2675
Epoch 14/256
3063/3063 - 41s - loss: 0.2663 - val_loss: 0.2767
Epoch 15/256
3063/3063 - 42s - loss: 0.2628 - val_loss: 0.2712
Epoch 16/256
3063/3063 - 42s - loss: 0.2588 - val_loss: 0.2735
Epoch 17/256
3063/3063 - 41s - loss: 0.2560 - val_loss: 0.2652
Epoch 18/256
3063/3063 - 42s - loss: 0.2534 - val_loss: 0.2855
Epoch 19/256
3063/3063 - 42s - loss: 0.2516 - val_loss: 0.2669
Epoch 20/256
3063/3063 - 42s - loss: 0.2490 - val_loss: 0.2756
Epoch 21/256
3063/3063 - 42s - loss: 0.2474 - val_loss: 0.2587
Epoch 22/256
3063/3063 - 42s - loss: 0.2444 - val_loss: 0.2925
Epoch 23/256
3063/3063 - 42s - loss: 0.2427 - val_loss: 0.2627
Epoch 24/256
3063/3063 - 42s - loss: 0.2415 - val_loss: 0.2643
Epoch 25/256
3063/3063 - 42s - loss: 0.2392 - val_loss: 0.2470
Epoch 26/256
3063/3063 - 41s - loss: 0.2380 - val_loss: 0.2509
Epoch 27/256
3063/3063 - 42s - loss: 0.2355 - val_loss: 0.2552
Epoch 28/256
3063/3063 - 42s - loss: 0.2337 - val_loss: 0.2466
Epoch 29/256
3063/3063 - 42s - loss: 0.2326 - val_loss: 0.2707
Epoch 30/256
3063/3063 - 41s - loss: 0.2321 - val_loss: 0.2471
Epoch 31/256
3063/3063 - 41s - loss: 0.2289 - val_loss: 0.2560
Epoch 32/256
3063/3063 - 41s - loss: 0.2275 - val_loss: 0.2816
Epoch 33/256
3063/3063 - 42s - loss: 0.2259 - val_loss: 0.2447
Epoch 34/256
3063/3063 - 41s - loss: 0.2247 - val_loss: 0.2557
Epoch 35/256
3063/3063 - 42s - loss: 0.2233 - val_loss: 0.2423
Epoch 36/256
3063/3063 - 42s - loss: 0.2223 - val_loss: 0.2628
Epoch 37/256
3063/3063 - 42s - loss: 0.2192 - val_loss: 0.2579
Epoch 38/256
3063/3063 - 41s - loss: 0.2189 - val_loss: 0.2428
Epoch 39/256
3063/3063 - 42s - loss: 0.2175 - val_loss: 0.2470
Epoch 40/256
3063/3063 - 41s - loss: 0.2166 - val_loss: 0.2385
Epoch 41/256
3063/3063 - 42s - loss: 0.2157 - val_loss: 0.2512
Epoch 42/256
3063/3063 - 42s - loss: 0.2133 - val_loss: 0.2603
Epoch 43/256
3063/3063 - 42s - loss: 0.2127 - val_loss: 0.2645
Epoch 44/256
3063/3063 - 42s - loss: 0.2120 - val_loss: 0.2413
Epoch 45/256
3063/3063 - 43s - loss: 0.2107 - val_loss: 0.2401
Epoch 46/256
3063/3063 - 42s - loss: 0.2093 - val_loss: 0.2373
Epoch 47/256
3063/3063 - 42s - loss: 0.2079 - val_loss: 0.2490
Epoch 48/256
3063/3063 - 42s - loss: 0.2073 - val_loss: 0.2559
Epoch 49/256
3063/3063 - 41s - loss: 0.2057 - val_loss: 0.2497
Epoch 50/256
3063/3063 - 42s - loss: 0.2049 - val_loss: 0.2430
Epoch 51/256
3063/3063 - 42s - loss: 0.2041 - val_loss: 0.2551
Epoch 52/256
3063/3063 - 42s - loss: 0.2019 - val_loss: 0.2471
Epoch 53/256
3063/3063 - 42s - loss: 0.2013 - val_loss: 0.2432
Epoch 54/256
3063/3063 - 42s - loss: 0.1995 - val_loss: 0.2492
Epoch 55/256
3063/3063 - 42s - loss: 0.1984 - val_loss: 0.2419
Epoch 56/256
3063/3063 - 42s - loss: 0.1972 - val_loss: 0.2403
Epoch 57/256
3063/3063 - 42s - loss: 0.1969 - val_loss: 0.2540
Epoch 58/256
3063/3063 - 42s - loss: 0.1958 - val_loss: 0.2378
Epoch 59/256
3063/3063 - 42s - loss: 0.1940 - val_loss: 0.2493
Epoch 60/256
3063/3063 - 42s - loss: 0.1936 - val_loss: 0.2444
Epoch 61/256
3063/3063 - 42s - loss: 0.1916 - val_loss: 0.2505
Epoch 62/256
3063/3063 - 42s - loss: 0.1907 - val_loss: 0.2486
Epoch 63/256
3063/3063 - 42s - loss: 0.1912 - val_loss: 0.2474
Epoch 64/256
3063/3063 - 42s - loss: 0.1867 - val_loss: 0.2714
Epoch 65/256
3063/3063 - 41s - loss: 0.1879 - val_loss: 0.2523
Epoch 66/256
3063/3063 - 42s - loss: 0.1860 - val_loss: 0.2561
Epoch 67/256
3063/3063 - 41s - loss: 0.1851 - val_loss: 0.2600
Epoch 68/256
3063/3063 - 41s - loss: 0.1837 - val_loss: 0.2655
Epoch 69/256
3063/3063 - 42s - loss: 0.1821 - val_loss: 0.2575
Epoch 70/256
3063/3063 - 42s - loss: 0.1811 - val_loss: 0.2716
Epoch 71/256
3063/3063 - 42s - loss: 0.1806 - val_loss: 0.2574
Epoch 72/256
3063/3063 - 41s - loss: 0.1787 - val_loss: 0.2452
Epoch 73/256
3063/3063 - 42s - loss: 0.1785 - val_loss: 0.2506
Epoch 74/256
3063/3063 - 41s - loss: 0.1771 - val_loss: 0.2633
Epoch 75/256
3063/3063 - 41s - loss: 0.1764 - val_loss: 0.2618
Epoch 76/256
3063/3063 - 41s - loss: 0.1753 - val_loss: 0.2538
Epoch 77/256
3063/3063 - 41s - loss: 0.1742 - val_loss: 0.2548
Epoch 78/256
3063/3063 - 41s - loss: 0.1727 - val_loss: 0.2759
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 128)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 43s - loss: 0.3907 - val_loss: 0.3575
Epoch 2/256
3063/3063 - 41s - loss: 0.3565 - val_loss: 0.3551
Epoch 3/256
3063/3063 - 42s - loss: 0.3457 - val_loss: 0.3516
Epoch 4/256
3063/3063 - 42s - loss: 0.3355 - val_loss: 0.3463
Epoch 5/256
3063/3063 - 41s - loss: 0.3281 - val_loss: 0.3261
Epoch 6/256
3063/3063 - 41s - loss: 0.3162 - val_loss: 0.3094
Epoch 7/256
3063/3063 - 42s - loss: 0.3053 - val_loss: 0.3095
Epoch 8/256
3063/3063 - 41s - loss: 0.2970 - val_loss: 0.3127
Epoch 9/256
3063/3063 - 42s - loss: 0.2912 - val_loss: 0.2910
Epoch 10/256
3063/3063 - 41s - loss: 0.2845 - val_loss: 0.2844
Epoch 11/256
3063/3063 - 42s - loss: 0.2793 - val_loss: 0.2821
Epoch 12/256
3063/3063 - 41s - loss: 0.2736 - val_loss: 0.2715
Epoch 13/256
3063/3063 - 41s - loss: 0.2683 - val_loss: 0.3182
Epoch 14/256
3063/3063 - 41s - loss: 0.2658 - val_loss: 0.2691
Epoch 15/256
3063/3063 - 42s - loss: 0.2620 - val_loss: 0.2756
Epoch 16/256
3063/3063 - 42s - loss: 0.2580 - val_loss: 0.2776
Epoch 17/256
3063/3063 - 43s - loss: 0.2562 - val_loss: 0.2667
Epoch 18/256
3063/3063 - 42s - loss: 0.2526 - val_loss: 0.2601
Epoch 19/256
3063/3063 - 42s - loss: 0.2497 - val_loss: 0.2575
Epoch 20/256
3063/3063 - 41s - loss: 0.2471 - val_loss: 0.2697
Epoch 21/256
3063/3063 - 42s - loss: 0.2474 - val_loss: 0.2521
Epoch 22/256
3063/3063 - 42s - loss: 0.2427 - val_loss: 0.2611
Epoch 23/256
3063/3063 - 42s - loss: 0.2425 - val_loss: 0.2573
Epoch 24/256
3063/3063 - 41s - loss: 0.2396 - val_loss: 0.2646
Epoch 25/256
3063/3063 - 41s - loss: 0.2375 - val_loss: 0.2574
Epoch 26/256
3063/3063 - 41s - loss: 0.2349 - val_loss: 0.2526
Epoch 27/256
3063/3063 - 41s - loss: 0.2339 - val_loss: 0.2911
Epoch 28/256
3063/3063 - 42s - loss: 0.2321 - val_loss: 0.2473
Epoch 29/256
3063/3063 - 42s - loss: 0.2297 - val_loss: 0.2412
Epoch 30/256
3063/3063 - 42s - loss: 0.2289 - val_loss: 0.2434
Epoch 31/256
3063/3063 - 42s - loss: 0.2267 - val_loss: 0.2479
Epoch 32/256
3063/3063 - 42s - loss: 0.2267 - val_loss: 0.2431
Epoch 33/256
3063/3063 - 40s - loss: 0.2250 - val_loss: 0.2547
Epoch 34/256
3063/3063 - 41s - loss: 0.2233 - val_loss: 0.2488
Epoch 35/256
3063/3063 - 41s - loss: 0.2226 - val_loss: 0.2392
Epoch 36/256
3063/3063 - 41s - loss: 0.2211 - val_loss: 0.2537
Epoch 37/256
3063/3063 - 41s - loss: 0.2200 - val_loss: 0.2416
Epoch 38/256
3063/3063 - 42s - loss: 0.2181 - val_loss: 0.2667
Epoch 39/256
3063/3063 - 41s - loss: 0.2175 - val_loss: 0.2429
Epoch 40/256
3063/3063 - 41s - loss: 0.2163 - val_loss: 0.2425
Epoch 41/256
3063/3063 - 41s - loss: 0.2159 - val_loss: 0.2800
Epoch 42/256
3063/3063 - 42s - loss: 0.2144 - val_loss: 0.2400
Epoch 43/256
3063/3063 - 41s - loss: 0.2128 - val_loss: 0.2355
Epoch 44/256
3063/3063 - 42s - loss: 0.2118 - val_loss: 0.2457
Epoch 45/256
3063/3063 - 41s - loss: 0.2113 - val_loss: 0.2491
Epoch 46/256
3063/3063 - 42s - loss: 0.2102 - val_loss: 0.2397
Epoch 47/256
3063/3063 - 41s - loss: 0.2092 - val_loss: 0.2494
Epoch 48/256
3063/3063 - 41s - loss: 0.2080 - val_loss: 0.2462
Epoch 49/256
3063/3063 - 41s - loss: 0.2061 - val_loss: 0.2449
Epoch 50/256
3063/3063 - 41s - loss: 0.2052 - val_loss: 0.2626
Epoch 51/256
3063/3063 - 42s - loss: 0.2039 - val_loss: 0.2497
Epoch 52/256
3063/3063 - 42s - loss: 0.2024 - val_loss: 0.2397
Epoch 53/256
3063/3063 - 42s - loss: 0.2021 - val_loss: 0.2390
Epoch 54/256
3063/3063 - 41s - loss: 0.1996 - val_loss: 0.2505
Epoch 55/256
3063/3063 - 42s - loss: 0.2002 - val_loss: 0.2434
Epoch 56/256
3063/3063 - 42s - loss: 0.1996 - val_loss: 0.2431
Epoch 57/256
3063/3063 - 41s - loss: 0.1976 - val_loss: 0.2406
Epoch 58/256
3063/3063 - 41s - loss: 0.1965 - val_loss: 0.2399
Epoch 59/256
3063/3063 - 41s - loss: 0.1951 - val_loss: 0.2483
Epoch 60/256
3063/3063 - 41s - loss: 0.1943 - val_loss: 0.2516
Epoch 61/256
3063/3063 - 42s - loss: 0.1932 - val_loss: 0.2484
Epoch 62/256
3063/3063 - 42s - loss: 0.1921 - val_loss: 0.2544
Epoch 63/256
3063/3063 - 42s - loss: 0.1930 - val_loss: 0.2502
Epoch 64/256
3063/3063 - 42s - loss: 0.1894 - val_loss: 0.2692
Epoch 65/256
3063/3063 - 42s - loss: 0.1897 - val_loss: 0.2402
Epoch 66/256
3063/3063 - 41s - loss: 0.1882 - val_loss: 0.2478
Epoch 67/256
3063/3063 - 42s - loss: 0.1866 - val_loss: 0.2527
Epoch 68/256
3063/3063 - 41s - loss: 0.1851 - val_loss: 0.2546
Epoch 69/256
3063/3063 - 42s - loss: 0.1839 - val_loss: 0.2598
Epoch 70/256
3063/3063 - 42s - loss: 0.1826 - val_loss: 0.2514
Epoch 71/256
3063/3063 - 42s - loss: 0.1817 - val_loss: 0.2699
Epoch 72/256
3063/3063 - 42s - loss: 0.1803 - val_loss: 0.2521
Epoch 73/256
3063/3063 - 42s - loss: 0.1801 - val_loss: 0.2545
Epoch 74/256
3063/3063 - 42s - loss: 0.1784 - val_loss: 0.2802
Epoch 75/256
3063/3063 - 41s - loss: 0.1770 - val_loss: 0.2579
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 128)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 42s - loss: 0.3891 - val_loss: 0.3589
Epoch 2/256
3063/3063 - 41s - loss: 0.3560 - val_loss: 0.3448
Epoch 3/256
3063/3063 - 41s - loss: 0.3468 - val_loss: 0.3415
Epoch 4/256
3063/3063 - 40s - loss: 0.3392 - val_loss: 0.3389
Epoch 5/256
3063/3063 - 41s - loss: 0.3329 - val_loss: 0.3243
Epoch 6/256
3063/3063 - 41s - loss: 0.3238 - val_loss: 0.3423
Epoch 7/256
3063/3063 - 41s - loss: 0.3172 - val_loss: 0.3192
Epoch 8/256
3063/3063 - 41s - loss: 0.3098 - val_loss: 0.3242
Epoch 9/256
3063/3063 - 41s - loss: 0.3016 - val_loss: 0.3055
Epoch 10/256
3063/3063 - 42s - loss: 0.2935 - val_loss: 0.3165
Epoch 11/256
3063/3063 - 41s - loss: 0.2869 - val_loss: 0.2825
Epoch 12/256
3063/3063 - 41s - loss: 0.2809 - val_loss: 0.2758
Epoch 13/256
3063/3063 - 41s - loss: 0.2756 - val_loss: 0.2767
Epoch 14/256
3063/3063 - 41s - loss: 0.2699 - val_loss: 0.2804
Epoch 15/256
3063/3063 - 41s - loss: 0.2668 - val_loss: 0.2721
Epoch 16/256
3063/3063 - 41s - loss: 0.2622 - val_loss: 0.2665
Epoch 17/256
3063/3063 - 41s - loss: 0.2597 - val_loss: 0.2573
Epoch 18/256
3063/3063 - 41s - loss: 0.2552 - val_loss: 0.2559
Epoch 19/256
3063/3063 - 40s - loss: 0.2535 - val_loss: 0.3121
Epoch 20/256
3063/3063 - 41s - loss: 0.2515 - val_loss: 0.2500
Epoch 21/256
3063/3063 - 40s - loss: 0.2486 - val_loss: 0.2539
Epoch 22/256
3063/3063 - 40s - loss: 0.2458 - val_loss: 0.2510
Epoch 23/256
3063/3063 - 40s - loss: 0.2435 - val_loss: 0.2540
Epoch 24/256
3063/3063 - 40s - loss: 0.2399 - val_loss: 0.2712
Epoch 25/256
3063/3063 - 41s - loss: 0.2393 - val_loss: 0.2534
Epoch 26/256
3063/3063 - 41s - loss: 0.2379 - val_loss: 0.2589
Epoch 27/256
3063/3063 - 41s - loss: 0.2354 - val_loss: 0.2517
Epoch 28/256
3063/3063 - 41s - loss: 0.2325 - val_loss: 0.2766
Epoch 29/256
3063/3063 - 41s - loss: 0.2309 - val_loss: 0.2448
Epoch 30/256
3063/3063 - 41s - loss: 0.2300 - val_loss: 0.2485
Epoch 31/256
3063/3063 - 42s - loss: 0.2287 - val_loss: 0.2446
Epoch 32/256
3063/3063 - 42s - loss: 0.2264 - val_loss: 0.2516
Epoch 33/256
3063/3063 - 42s - loss: 0.2247 - val_loss: 0.2565
Epoch 34/256
3063/3063 - 42s - loss: 0.2236 - val_loss: 0.2468
Epoch 35/256
3063/3063 - 42s - loss: 0.2220 - val_loss: 0.2437
Epoch 36/256
3063/3063 - 44s - loss: 0.2217 - val_loss: 0.2389
Epoch 37/256
3063/3063 - 41s - loss: 0.2207 - val_loss: 0.2596
Epoch 38/256
3063/3063 - 42s - loss: 0.2186 - val_loss: 0.2401
Epoch 39/256
3063/3063 - 41s - loss: 0.2183 - val_loss: 0.2542
Epoch 40/256
3063/3063 - 42s - loss: 0.2167 - val_loss: 0.2425
Epoch 41/256
3063/3063 - 42s - loss: 0.2148 - val_loss: 0.2374
Epoch 42/256
3063/3063 - 42s - loss: 0.2136 - val_loss: 0.2522
Epoch 43/256
3063/3063 - 41s - loss: 0.2123 - val_loss: 0.2421
Epoch 44/256
3063/3063 - 41s - loss: 0.2106 - val_loss: 0.2557
Epoch 45/256
3063/3063 - 41s - loss: 0.2113 - val_loss: 0.2605
Epoch 46/256
3063/3063 - 42s - loss: 0.2094 - val_loss: 0.2386
Epoch 47/256
3063/3063 - 41s - loss: 0.2083 - val_loss: 0.2391
Epoch 48/256
3063/3063 - 42s - loss: 0.2074 - val_loss: 0.2577
Epoch 49/256
3063/3063 - 41s - loss: 0.2044 - val_loss: 0.2406
Epoch 50/256
3063/3063 - 42s - loss: 0.2044 - val_loss: 0.2577
Epoch 51/256
3063/3063 - 41s - loss: 0.2042 - val_loss: 0.2366
Epoch 52/256
3063/3063 - 41s - loss: 0.2017 - val_loss: 0.2516
Epoch 53/256
3063/3063 - 41s - loss: 0.2027 - val_loss: 0.2584
Epoch 54/256
3063/3063 - 41s - loss: 0.2002 - val_loss: 0.2404
Epoch 55/256
3063/3063 - 41s - loss: 0.1997 - val_loss: 0.2410
Epoch 56/256
3063/3063 - 41s - loss: 0.1982 - val_loss: 0.2432
Epoch 57/256
3063/3063 - 41s - loss: 0.1959 - val_loss: 0.2615
Epoch 58/256
3063/3063 - 41s - loss: 0.1955 - val_loss: 0.2570
Epoch 59/256
3063/3063 - 41s - loss: 0.1944 - val_loss: 0.2449
Epoch 60/256
3063/3063 - 41s - loss: 0.1930 - val_loss: 0.2493
Epoch 61/256
3063/3063 - 41s - loss: 0.1912 - val_loss: 0.2486
Epoch 62/256
3063/3063 - 41s - loss: 0.1898 - val_loss: 0.2459
Epoch 63/256
3063/3063 - 42s - loss: 0.1904 - val_loss: 0.2415
Epoch 64/256
3063/3063 - 41s - loss: 0.1888 - val_loss: 0.2799
Epoch 65/256
3063/3063 - 41s - loss: 0.1889 - val_loss: 0.2618
Epoch 66/256
3063/3063 - 41s - loss: 0.1876 - val_loss: 0.2551
Epoch 67/256
3063/3063 - 41s - loss: 0.1854 - val_loss: 0.2559
Epoch 68/256
3063/3063 - 41s - loss: 0.1839 - val_loss: 0.2618
Epoch 69/256
3063/3063 - 41s - loss: 0.1832 - val_loss: 0.2689
Epoch 70/256
3063/3063 - 40s - loss: 0.1814 - val_loss: 0.2542
Epoch 71/256
3063/3063 - 41s - loss: 0.1808 - val_loss: 0.2541
Epoch 72/256
3063/3063 - 40s - loss: 0.1787 - val_loss: 0.2475
Epoch 73/256
3063/3063 - 41s - loss: 0.1785 - val_loss: 0.2556
Epoch 74/256
3063/3063 - 41s - loss: 0.1775 - val_loss: 0.2593
Epoch 75/256
3063/3063 - 41s - loss: 0.1769 - val_loss: 0.2530
Epoch 76/256
3063/3063 - 40s - loss: 0.1755 - val_loss: 0.2522
Epoch 77/256
3063/3063 - 41s - loss: 0.1746 - val_loss: 0.2533
Epoch 78/256
3063/3063 - 41s - loss: 0.1734 - val_loss: 0.2486
Epoch 79/256
3063/3063 - 41s - loss: 0.1731 - val_loss: 0.2492
Epoch 80/256
3063/3063 - 41s - loss: 0.1707 - val_loss: 0.2832
Epoch 81/256
3063/3063 - 41s - loss: 0.1696 - val_loss: 0.2530
Epoch 82/256
3063/3063 - 41s - loss: 0.1690 - val_loss: 0.2683
Epoch 83/256
3063/3063 - 42s - loss: 0.1676 - val_loss: 0.2575
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 128)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 42s - loss: 0.3935 - val_loss: 0.3534
Epoch 2/256
3063/3063 - 41s - loss: 0.3570 - val_loss: 0.3585
Epoch 3/256
3063/3063 - 41s - loss: 0.3464 - val_loss: 0.3456
Epoch 4/256
3063/3063 - 41s - loss: 0.3385 - val_loss: 0.3526
Epoch 5/256
3063/3063 - 41s - loss: 0.3296 - val_loss: 0.3195
Epoch 6/256
3063/3063 - 40s - loss: 0.3160 - val_loss: 0.2999
Epoch 7/256
3063/3063 - 41s - loss: 0.3028 - val_loss: 0.3119
Epoch 8/256
3063/3063 - 41s - loss: 0.2925 - val_loss: 0.2952
Epoch 9/256
3063/3063 - 41s - loss: 0.2866 - val_loss: 0.2836
Epoch 10/256
3063/3063 - 41s - loss: 0.2795 - val_loss: 0.2898
Epoch 11/256
3063/3063 - 40s - loss: 0.2751 - val_loss: 0.2903
Epoch 12/256
3063/3063 - 40s - loss: 0.2696 - val_loss: 0.2722
Epoch 13/256
3063/3063 - 41s - loss: 0.2658 - val_loss: 0.2741
Epoch 14/256
3063/3063 - 41s - loss: 0.2612 - val_loss: 0.2748
Epoch 15/256
3063/3063 - 41s - loss: 0.2587 - val_loss: 0.2731
Epoch 16/256
3063/3063 - 40s - loss: 0.2550 - val_loss: 0.2807
Epoch 17/256
3063/3063 - 42s - loss: 0.2524 - val_loss: 0.2529
Epoch 18/256
3063/3063 - 41s - loss: 0.2504 - val_loss: 0.2551
Epoch 19/256
3063/3063 - 41s - loss: 0.2470 - val_loss: 0.2495
Epoch 20/256
3063/3063 - 41s - loss: 0.2450 - val_loss: 0.2511
Epoch 21/256
3063/3063 - 41s - loss: 0.2428 - val_loss: 0.2532
Epoch 22/256
3063/3063 - 41s - loss: 0.2398 - val_loss: 0.2610
Epoch 23/256
3063/3063 - 41s - loss: 0.2380 - val_loss: 0.2496
Epoch 24/256
3063/3063 - 42s - loss: 0.2375 - val_loss: 0.2458
Epoch 25/256
3063/3063 - 41s - loss: 0.2351 - val_loss: 0.2527
Epoch 26/256
3063/3063 - 41s - loss: 0.2336 - val_loss: 0.2528
Epoch 27/256
3063/3063 - 41s - loss: 0.2309 - val_loss: 0.2419
Epoch 28/256
3063/3063 - 41s - loss: 0.2313 - val_loss: 0.2426
Epoch 29/256
3063/3063 - 41s - loss: 0.2280 - val_loss: 0.2373
Epoch 30/256
3063/3063 - 41s - loss: 0.2268 - val_loss: 0.2424
Epoch 31/256
3063/3063 - 42s - loss: 0.2248 - val_loss: 0.2471
Epoch 32/256
3063/3063 - 41s - loss: 0.2239 - val_loss: 0.2541
Epoch 33/256
3063/3063 - 41s - loss: 0.2225 - val_loss: 0.2476
Epoch 34/256
3063/3063 - 42s - loss: 0.2218 - val_loss: 0.2444
Epoch 35/256
3063/3063 - 42s - loss: 0.2189 - val_loss: 0.2639
Epoch 36/256
3063/3063 - 42s - loss: 0.2195 - val_loss: 0.2365
Epoch 37/256
3063/3063 - 41s - loss: 0.2172 - val_loss: 0.2496
Epoch 38/256
3063/3063 - 42s - loss: 0.2155 - val_loss: 0.2450
Epoch 39/256
3063/3063 - 42s - loss: 0.2149 - val_loss: 0.2571
Epoch 40/256
3063/3063 - 42s - loss: 0.2136 - val_loss: 0.2559
Epoch 41/256
3063/3063 - 41s - loss: 0.2123 - val_loss: 0.2575
Epoch 42/256
3063/3063 - 41s - loss: 0.2111 - val_loss: 0.2404
Epoch 43/256
3063/3063 - 41s - loss: 0.2106 - val_loss: 0.2358
Epoch 44/256
3063/3063 - 42s - loss: 0.2089 - val_loss: 0.2511
Epoch 45/256
3063/3063 - 41s - loss: 0.2076 - val_loss: 0.2533
Epoch 46/256
3063/3063 - 41s - loss: 0.2077 - val_loss: 0.2575
Epoch 47/256
3063/3063 - 40s - loss: 0.2064 - val_loss: 0.2488
Epoch 48/256
3063/3063 - 41s - loss: 0.2027 - val_loss: 0.2455
Epoch 49/256
3063/3063 - 41s - loss: 0.2039 - val_loss: 0.2451
Epoch 50/256
3063/3063 - 41s - loss: 0.2025 - val_loss: 0.2391
Epoch 51/256
3063/3063 - 41s - loss: 0.2012 - val_loss: 0.2438
Epoch 52/256
3063/3063 - 41s - loss: 0.2004 - val_loss: 0.2600
Epoch 53/256
3063/3063 - 41s - loss: 0.1985 - val_loss: 0.2399
Epoch 54/256
3063/3063 - 41s - loss: 0.1970 - val_loss: 0.2409
Epoch 55/256
3063/3063 - 41s - loss: 0.1969 - val_loss: 0.2352
Epoch 56/256
3063/3063 - 41s - loss: 0.1952 - val_loss: 0.2467
Epoch 57/256
3063/3063 - 41s - loss: 0.1946 - val_loss: 0.2423
Epoch 58/256
3063/3063 - 42s - loss: 0.1930 - val_loss: 0.2596
Epoch 59/256
3063/3063 - 41s - loss: 0.1928 - val_loss: 0.2661
Epoch 60/256
3063/3063 - 42s - loss: 0.1903 - val_loss: 0.2528
Epoch 61/256
3063/3063 - 42s - loss: 0.1906 - val_loss: 0.2534
Epoch 62/256
3063/3063 - 42s - loss: 0.1882 - val_loss: 0.2416
Epoch 63/256
3063/3063 - 42s - loss: 0.1886 - val_loss: 0.2474
Epoch 64/256
3063/3063 - 41s - loss: 0.1860 - val_loss: 0.2695
Epoch 65/256
3063/3063 - 42s - loss: 0.1857 - val_loss: 0.2546
Epoch 66/256
3063/3063 - 41s - loss: 0.1848 - val_loss: 0.2629
Epoch 67/256
3063/3063 - 41s - loss: 0.1820 - val_loss: 0.2524
Epoch 68/256
3063/3063 - 41s - loss: 0.1817 - val_loss: 0.2460
Epoch 69/256
3063/3063 - 42s - loss: 0.1808 - val_loss: 0.2501
Epoch 70/256
3063/3063 - 41s - loss: 0.1785 - val_loss: 0.2494
Epoch 71/256
3063/3063 - 41s - loss: 0.1781 - val_loss: 0.2562
Epoch 72/256
3063/3063 - 42s - loss: 0.1754 - val_loss: 0.2613
Epoch 73/256
3063/3063 - 42s - loss: 0.1766 - val_loss: 0.2487
Epoch 74/256
3063/3063 - 42s - loss: 0.1772 - val_loss: 0.2642
Epoch 75/256
3063/3063 - 42s - loss: 0.1745 - val_loss: 0.2691
Epoch 76/256
3063/3063 - 41s - loss: 0.1723 - val_loss: 0.2651
Epoch 77/256
3063/3063 - 41s - loss: 0.1712 - val_loss: 0.2595
Epoch 78/256
3063/3063 - 42s - loss: 0.1708 - val_loss: 0.2731
Epoch 79/256
3063/3063 - 42s - loss: 0.1690 - val_loss: 0.2741
Epoch 80/256
3063/3063 - 42s - loss: 0.1678 - val_loss: 0.2624
Epoch 81/256
3063/3063 - 42s - loss: 0.1669 - val_loss: 0.2728
Epoch 82/256
3063/3063 - 42s - loss: 0.1652 - val_loss: 0.2586
Epoch 83/256
3063/3063 - 42s - loss: 0.1645 - val_loss: 0.2944
Epoch 84/256
3063/3063 - 42s - loss: 0.1637 - val_loss: 0.2656
Epoch 85/256
3063/3063 - 41s - loss: 0.1636 - val_loss: 0.2796
Epoch 86/256
3063/3063 - 42s - loss: 0.1616 - val_loss: 0.2807
Epoch 87/256
3063/3063 - 41s - loss: 0.1611 - val_loss: 0.2678
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 128)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 45s - loss: 0.3928 - val_loss: 0.3522
Epoch 2/256
3063/3063 - 42s - loss: 0.3572 - val_loss: 0.3450
Epoch 3/256
3063/3063 - 42s - loss: 0.3474 - val_loss: 0.3401
Epoch 4/256
3063/3063 - 42s - loss: 0.3418 - val_loss: 0.3322
Epoch 5/256
3063/3063 - 42s - loss: 0.3325 - val_loss: 0.3256
Epoch 6/256
3063/3063 - 42s - loss: 0.3206 - val_loss: 0.3143
Epoch 7/256
3063/3063 - 43s - loss: 0.3099 - val_loss: 0.3024
Epoch 8/256
3063/3063 - 42s - loss: 0.2994 - val_loss: 0.2878
Epoch 9/256
3063/3063 - 42s - loss: 0.2900 - val_loss: 0.2985
Epoch 10/256
3063/3063 - 42s - loss: 0.2816 - val_loss: 0.2804
Epoch 11/256
3063/3063 - 43s - loss: 0.2769 - val_loss: 0.2755
Epoch 12/256
3063/3063 - 42s - loss: 0.2720 - val_loss: 0.2713
Epoch 13/256
3063/3063 - 42s - loss: 0.2681 - val_loss: 0.2832
Epoch 14/256
3063/3063 - 42s - loss: 0.2643 - val_loss: 0.2654
Epoch 15/256
3063/3063 - 42s - loss: 0.2605 - val_loss: 0.2630
Epoch 16/256
3063/3063 - 43s - loss: 0.2574 - val_loss: 0.2817
Epoch 17/256
3063/3063 - 42s - loss: 0.2546 - val_loss: 0.2600
Epoch 18/256
3063/3063 - 42s - loss: 0.2526 - val_loss: 0.2836
Epoch 19/256
3063/3063 - 42s - loss: 0.2499 - val_loss: 0.2578
Epoch 20/256
3063/3063 - 41s - loss: 0.2482 - val_loss: 0.2825
Epoch 21/256
3063/3063 - 42s - loss: 0.2443 - val_loss: 0.2688
Epoch 22/256
3063/3063 - 42s - loss: 0.2450 - val_loss: 0.2546
Epoch 23/256
3063/3063 - 42s - loss: 0.2416 - val_loss: 0.2597
Epoch 24/256
3063/3063 - 42s - loss: 0.2400 - val_loss: 0.2506
Epoch 25/256
3063/3063 - 41s - loss: 0.2378 - val_loss: 0.2645
Epoch 26/256
3063/3063 - 42s - loss: 0.2358 - val_loss: 0.2555
Epoch 27/256
3063/3063 - 41s - loss: 0.2341 - val_loss: 0.2529
Epoch 28/256
3063/3063 - 42s - loss: 0.2336 - val_loss: 0.2472
Epoch 29/256
3063/3063 - 42s - loss: 0.2316 - val_loss: 0.2495
Epoch 30/256
3063/3063 - 42s - loss: 0.2295 - val_loss: 0.2455
Epoch 31/256
3063/3063 - 41s - loss: 0.2288 - val_loss: 0.2560
Epoch 32/256
3063/3063 - 41s - loss: 0.2263 - val_loss: 0.2591
Epoch 33/256
3063/3063 - 41s - loss: 0.2267 - val_loss: 0.2416
Epoch 34/256
3063/3063 - 41s - loss: 0.2245 - val_loss: 0.2483
Epoch 35/256
3063/3063 - 41s - loss: 0.2231 - val_loss: 0.2566
Epoch 36/256
3063/3063 - 42s - loss: 0.2206 - val_loss: 0.2470
Epoch 37/256
3063/3063 - 41s - loss: 0.2181 - val_loss: 0.2451
Epoch 38/256
3063/3063 - 42s - loss: 0.2184 - val_loss: 0.2438
Epoch 39/256
3063/3063 - 41s - loss: 0.2173 - val_loss: 0.2372
Epoch 40/256
3063/3063 - 42s - loss: 0.2172 - val_loss: 0.2414
Epoch 41/256
3063/3063 - 41s - loss: 0.2156 - val_loss: 0.2438
Epoch 42/256
3063/3063 - 42s - loss: 0.2137 - val_loss: 0.2388
Epoch 43/256
3063/3063 - 42s - loss: 0.2124 - val_loss: 0.2612
Epoch 44/256
3063/3063 - 42s - loss: 0.2119 - val_loss: 0.2473
Epoch 45/256
3063/3063 - 41s - loss: 0.2094 - val_loss: 0.2403
Epoch 46/256
3063/3063 - 42s - loss: 0.2093 - val_loss: 0.2492
Epoch 47/256
3063/3063 - 42s - loss: 0.2076 - val_loss: 0.2356
Epoch 48/256
3063/3063 - 42s - loss: 0.2069 - val_loss: 0.2599
Epoch 49/256
3063/3063 - 42s - loss: 0.2050 - val_loss: 0.2446
Epoch 50/256
3063/3063 - 42s - loss: 0.2043 - val_loss: 0.2363
Epoch 51/256
3063/3063 - 42s - loss: 0.2034 - val_loss: 0.2436
Epoch 52/256
3063/3063 - 41s - loss: 0.2027 - val_loss: 0.2668
Epoch 53/256
3063/3063 - 42s - loss: 0.2010 - val_loss: 0.2533
Epoch 54/256
3063/3063 - 41s - loss: 0.2000 - val_loss: 0.2489
Epoch 55/256
3063/3063 - 42s - loss: 0.1984 - val_loss: 0.2421
Epoch 56/256
3063/3063 - 42s - loss: 0.1976 - val_loss: 0.2475
Epoch 57/256
3063/3063 - 41s - loss: 0.1959 - val_loss: 0.2611
Epoch 58/256
3063/3063 - 42s - loss: 0.1956 - val_loss: 0.2477
Epoch 59/256
3063/3063 - 41s - loss: 0.1948 - val_loss: 0.2621
Epoch 60/256
3063/3063 - 42s - loss: 0.1930 - val_loss: 0.2566
Epoch 61/256
3063/3063 - 42s - loss: 0.1925 - val_loss: 0.2402
Epoch 62/256
3063/3063 - 42s - loss: 0.1911 - val_loss: 0.2423
Epoch 63/256
3063/3063 - 42s - loss: 0.1910 - val_loss: 0.2607
Epoch 64/256
3063/3063 - 41s - loss: 0.1887 - val_loss: 0.2620
Epoch 65/256
3063/3063 - 42s - loss: 0.1876 - val_loss: 0.2510
Epoch 66/256
3063/3063 - 42s - loss: 0.1872 - val_loss: 0.2567
Epoch 67/256
3063/3063 - 42s - loss: 0.1869 - val_loss: 0.2658
Epoch 68/256
3063/3063 - 42s - loss: 0.1853 - val_loss: 0.2636
Epoch 69/256
3063/3063 - 42s - loss: 0.1846 - val_loss: 0.2685
Epoch 70/256
3063/3063 - 41s - loss: 0.1820 - val_loss: 0.2558
Epoch 71/256
3063/3063 - 42s - loss: 0.1815 - val_loss: 0.2612
Epoch 72/256
3063/3063 - 42s - loss: 0.1805 - val_loss: 0.2752
Epoch 73/256
3063/3063 - 42s - loss: 0.1791 - val_loss: 0.2647
Epoch 74/256
3063/3063 - 41s - loss: 0.1782 - val_loss: 0.2625
Epoch 75/256
3063/3063 - 42s - loss: 0.1779 - val_loss: 0.2701
Epoch 76/256
3063/3063 - 42s - loss: 0.1771 - val_loss: 0.2490
Epoch 77/256
3063/3063 - 42s - loss: 0.1750 - val_loss: 0.2692
Epoch 78/256
3063/3063 - 42s - loss: 0.1737 - val_loss: 0.2544
Epoch 79/256
3063/3063 - 42s - loss: 0.1730 - val_loss: 0.2694
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 128)_64
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 43s - loss: 0.3915 - val_loss: 0.3529
Epoch 2/256
3063/3063 - 41s - loss: 0.3582 - val_loss: 0.3441
Epoch 3/256
3063/3063 - 41s - loss: 0.3461 - val_loss: 0.3356
Epoch 4/256
3063/3063 - 41s - loss: 0.3361 - val_loss: 0.3478
Epoch 5/256
3063/3063 - 41s - loss: 0.3277 - val_loss: 0.3192
Epoch 6/256
3063/3063 - 41s - loss: 0.3169 - val_loss: 0.3125
Epoch 7/256
3063/3063 - 41s - loss: 0.3080 - val_loss: 0.2992
Epoch 8/256
3063/3063 - 41s - loss: 0.2964 - val_loss: 0.2890
Epoch 9/256
3063/3063 - 40s - loss: 0.2894 - val_loss: 0.2938
Epoch 10/256
3063/3063 - 41s - loss: 0.2832 - val_loss: 0.2871
Epoch 11/256
3063/3063 - 41s - loss: 0.2781 - val_loss: 0.2798
Epoch 12/256
3063/3063 - 41s - loss: 0.2733 - val_loss: 0.2739
Epoch 13/256
3063/3063 - 41s - loss: 0.2672 - val_loss: 0.2629
Epoch 14/256
3063/3063 - 41s - loss: 0.2645 - val_loss: 0.2742
Epoch 15/256
3063/3063 - 42s - loss: 0.2598 - val_loss: 0.2638
Epoch 16/256
3063/3063 - 42s - loss: 0.2564 - val_loss: 0.2641
Epoch 17/256
3063/3063 - 41s - loss: 0.2528 - val_loss: 0.2580
Epoch 18/256
3063/3063 - 41s - loss: 0.2516 - val_loss: 0.2523
Epoch 19/256
3063/3063 - 41s - loss: 0.2478 - val_loss: 0.2756
Epoch 20/256
3063/3063 - 42s - loss: 0.2463 - val_loss: 0.2738
Epoch 21/256
3063/3063 - 41s - loss: 0.2445 - val_loss: 0.2489
Epoch 22/256
3063/3063 - 42s - loss: 0.2413 - val_loss: 0.2535
Epoch 23/256
3063/3063 - 42s - loss: 0.2401 - val_loss: 0.2538
Epoch 24/256
3063/3063 - 41s - loss: 0.2389 - val_loss: 0.2526
Epoch 25/256
3063/3063 - 42s - loss: 0.2355 - val_loss: 0.2510
Epoch 26/256
3063/3063 - 41s - loss: 0.2345 - val_loss: 0.2426
Epoch 27/256
3063/3063 - 41s - loss: 0.2331 - val_loss: 0.2728
Epoch 28/256
3063/3063 - 42s - loss: 0.2309 - val_loss: 0.2528
Epoch 29/256
3063/3063 - 42s - loss: 0.2298 - val_loss: 0.2521
Epoch 30/256
3063/3063 - 41s - loss: 0.2282 - val_loss: 0.2504
Epoch 31/256
3063/3063 - 41s - loss: 0.2255 - val_loss: 0.2478
Epoch 32/256
3063/3063 - 41s - loss: 0.2252 - val_loss: 0.2411
Epoch 33/256
3063/3063 - 41s - loss: 0.2239 - val_loss: 0.2430
Epoch 34/256
3063/3063 - 40s - loss: 0.2216 - val_loss: 0.2483
Epoch 35/256
3063/3063 - 41s - loss: 0.2200 - val_loss: 0.2583
Epoch 36/256
3063/3063 - 41s - loss: 0.2184 - val_loss: 0.2421
Epoch 37/256
3063/3063 - 41s - loss: 0.2180 - val_loss: 0.2481
Epoch 38/256
3063/3063 - 42s - loss: 0.2168 - val_loss: 0.2402
Epoch 39/256
3063/3063 - 41s - loss: 0.2154 - val_loss: 0.2508
Epoch 40/256
3063/3063 - 42s - loss: 0.2137 - val_loss: 0.2568
Epoch 41/256
3063/3063 - 42s - loss: 0.2132 - val_loss: 0.2761
Epoch 42/256
3063/3063 - 42s - loss: 0.2119 - val_loss: 0.2473
Epoch 43/256
3063/3063 - 41s - loss: 0.2098 - val_loss: 0.2502
Epoch 44/256
3063/3063 - 42s - loss: 0.2085 - val_loss: 0.2529
Epoch 45/256
3063/3063 - 41s - loss: 0.2086 - val_loss: 0.2410
Epoch 46/256
3063/3063 - 42s - loss: 0.2058 - val_loss: 0.2444
Epoch 47/256
3063/3063 - 41s - loss: 0.2064 - val_loss: 0.2435
Epoch 48/256
3063/3063 - 42s - loss: 0.2044 - val_loss: 0.2429
Epoch 49/256
3063/3063 - 41s - loss: 0.2042 - val_loss: 0.2450
Epoch 50/256
3063/3063 - 41s - loss: 0.2023 - val_loss: 0.2501
Epoch 51/256
3063/3063 - 41s - loss: 0.2007 - val_loss: 0.2590
Epoch 52/256
3063/3063 - 42s - loss: 0.2009 - val_loss: 0.2438
Epoch 53/256
3063/3063 - 42s - loss: 0.1983 - val_loss: 0.2568
Epoch 54/256
3063/3063 - 42s - loss: 0.1973 - val_loss: 0.2526
Epoch 55/256
3063/3063 - 41s - loss: 0.1952 - val_loss: 0.2486
Epoch 56/256
3063/3063 - 41s - loss: 0.1952 - val_loss: 0.2541
Epoch 57/256
3063/3063 - 41s - loss: 0.1948 - val_loss: 0.2465
Epoch 58/256
3063/3063 - 41s - loss: 0.1934 - val_loss: 0.2437
Epoch 59/256
3063/3063 - 40s - loss: 0.1918 - val_loss: 0.2487
Epoch 60/256
3063/3063 - 41s - loss: 0.1912 - val_loss: 0.2390
Epoch 61/256
3063/3063 - 41s - loss: 0.1894 - val_loss: 0.2485
Epoch 62/256
3063/3063 - 41s - loss: 0.1880 - val_loss: 0.2785
Epoch 63/256
3063/3063 - 41s - loss: 0.1878 - val_loss: 0.2474
Epoch 64/256
3063/3063 - 41s - loss: 0.1857 - val_loss: 0.2429
Epoch 65/256
3063/3063 - 41s - loss: 0.1852 - val_loss: 0.2592
Epoch 66/256
3063/3063 - 41s - loss: 0.1853 - val_loss: 0.2618
Epoch 67/256
3063/3063 - 41s - loss: 0.1847 - val_loss: 0.2563
Epoch 68/256
3063/3063 - 41s - loss: 0.1821 - val_loss: 0.2564
Epoch 69/256
3063/3063 - 41s - loss: 0.1817 - val_loss: 0.2727
Epoch 70/256
3063/3063 - 41s - loss: 0.1806 - val_loss: 0.2506
Epoch 71/256
3063/3063 - 42s - loss: 0.1794 - val_loss: 0.2591
Epoch 72/256
3063/3063 - 41s - loss: 0.1777 - val_loss: 0.2585
Epoch 73/256
3063/3063 - 42s - loss: 0.1771 - val_loss: 0.2539
Epoch 74/256
3063/3063 - 41s - loss: 0.1760 - val_loss: 0.2684
Epoch 75/256
3063/3063 - 42s - loss: 0.1732 - val_loss: 0.2562
Epoch 76/256
3063/3063 - 41s - loss: 0.1723 - val_loss: 0.2615
Epoch 77/256
3063/3063 - 41s - loss: 0.1722 - val_loss: 0.2600
Epoch 78/256
3063/3063 - 41s - loss: 0.1713 - val_loss: 0.2597
Epoch 79/256
3063/3063 - 42s - loss: 0.1709 - val_loss: 0.2649
Epoch 80/256
3063/3063 - 41s - loss: 0.1691 - val_loss: 0.2711
Epoch 81/256
3063/3063 - 41s - loss: 0.1677 - val_loss: 0.2569
Epoch 82/256
3063/3063 - 42s - loss: 0.1669 - val_loss: 0.2663
Epoch 83/256
3063/3063 - 41s - loss: 0.1650 - val_loss: 0.2518
Epoch 84/256
3063/3063 - 42s - loss: 0.1651 - val_loss: 0.2600
Epoch 85/256
3063/3063 - 41s - loss: 0.1625 - val_loss: 0.2677
Epoch 86/256
3063/3063 - 42s - loss: 0.1623 - val_loss: 0.2693
Epoch 87/256
3063/3063 - 42s - loss: 0.1607 - val_loss: 0.2655
Epoch 88/256
3063/3063 - 42s - loss: 0.1585 - val_loss: 0.2872
Epoch 89/256
3063/3063 - 42s - loss: 0.1592 - val_loss: 0.2724
Epoch 90/256
3063/3063 - 41s - loss: 0.1569 - val_loss: 0.3141
Epoch 91/256
3063/3063 - 41s - loss: 0.1558 - val_loss: 0.2728
Epoch 92/256
3063/3063 - 42s - loss: 0.1553 - val_loss: 0.2864
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 128)_64
$2^7$ & $0.966 \pm 0.000$ & 117K & $502.1\pm 28.9$ & $150.4\pm 8.5$ & $43.3\pm 1.3$ & $30.3\pm 0.9$\\

