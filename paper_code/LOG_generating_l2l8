nohup: ignoring input
2022-06-02 16:54:00.082854: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/rh/httpd24/root/usr/lib64:/home/delon/madgraph/MG5_aMC_v3_1_1/HEPTools/lhapdf6_py3/lib
2022-06-02 16:54:00.082897: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2022-06-02 16:55:27.941290: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/rh/httpd24/root/usr/lib64:/home/delon/madgraph/MG5_aMC_v3_1_1/HEPTools/lhapdf6_py3/lib
2022-06-02 16:55:27.941342: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)
2022-06-02 16:55:27.941384: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (top1.hep.utexas.edu): /proc/driver/nvidia/version does not exist
2022-06-02 16:55:27.942115: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-06-02 16:55:28.244089: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
2022-06-02 16:55:28.244718: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2397305000 Hz
Initializing Experimenter
	Loading Data from ../data/data100k_raw_combined_atlas_cut.pkl
	Data Loaded
	Creating Splits
	Splits Created
Done initalizing
RIGHT NOW: latent_two
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 57s - loss: 0.4006 - val_loss: 0.3573
Epoch 2/256
3063/3063 - 55s - loss: 0.3573 - val_loss: 0.3809
Epoch 3/256
3063/3063 - 54s - loss: 0.3482 - val_loss: 0.3379
Epoch 4/256
3063/3063 - 54s - loss: 0.3398 - val_loss: 0.3326
Epoch 5/256
3063/3063 - 54s - loss: 0.3345 - val_loss: 0.3247
Epoch 6/256
3063/3063 - 54s - loss: 0.3297 - val_loss: 0.3289
Epoch 7/256
3063/3063 - 54s - loss: 0.3243 - val_loss: 0.3236
Epoch 8/256
3063/3063 - 54s - loss: 0.3192 - val_loss: 0.3163
Epoch 9/256
3063/3063 - 54s - loss: 0.3156 - val_loss: 0.3416
Epoch 10/256
3063/3063 - 54s - loss: 0.3104 - val_loss: 0.3257
Epoch 11/256
3063/3063 - 54s - loss: 0.3069 - val_loss: 0.3155
Epoch 12/256
3063/3063 - 54s - loss: 0.3037 - val_loss: 0.3116
Epoch 13/256
3063/3063 - 54s - loss: 0.3012 - val_loss: 0.3011
Epoch 14/256
3063/3063 - 54s - loss: 0.2986 - val_loss: 0.3203
Epoch 15/256
3063/3063 - 54s - loss: 0.2956 - val_loss: 0.3187
Epoch 16/256
3063/3063 - 54s - loss: 0.2940 - val_loss: 0.2948
Epoch 17/256
3063/3063 - 54s - loss: 0.2919 - val_loss: 0.2903
Epoch 18/256
3063/3063 - 54s - loss: 0.2894 - val_loss: 0.3030
Epoch 19/256
3063/3063 - 54s - loss: 0.2880 - val_loss: 0.3001
Epoch 20/256
3063/3063 - 54s - loss: 0.2858 - val_loss: 0.2884
Epoch 21/256
3063/3063 - 54s - loss: 0.2838 - val_loss: 0.2859
Epoch 22/256
3063/3063 - 54s - loss: 0.2825 - val_loss: 0.3298
Epoch 23/256
3063/3063 - 55s - loss: 0.2805 - val_loss: 0.2859
Epoch 24/256
3063/3063 - 54s - loss: 0.2786 - val_loss: 0.2844
Epoch 25/256
3063/3063 - 54s - loss: 0.2776 - val_loss: 0.2806
Epoch 26/256
3063/3063 - 54s - loss: 0.2757 - val_loss: 0.2803
Epoch 27/256
3063/3063 - 54s - loss: 0.2741 - val_loss: 0.2945
Epoch 28/256
3063/3063 - 54s - loss: 0.2730 - val_loss: 0.2786
Epoch 29/256
3063/3063 - 54s - loss: 0.2706 - val_loss: 0.2812
Epoch 30/256
3063/3063 - 53s - loss: 0.2696 - val_loss: 0.2748
Epoch 31/256
3063/3063 - 54s - loss: 0.2692 - val_loss: 0.2801
Epoch 32/256
3063/3063 - 54s - loss: 0.2675 - val_loss: 0.2840
Epoch 33/256
3063/3063 - 54s - loss: 0.2667 - val_loss: 0.2798
Epoch 34/256
3063/3063 - 54s - loss: 0.2655 - val_loss: 0.2969
Epoch 35/256
3063/3063 - 54s - loss: 0.2640 - val_loss: 0.2708
Epoch 36/256
3063/3063 - 54s - loss: 0.2624 - val_loss: 0.2862
Epoch 37/256
3063/3063 - 54s - loss: 0.2613 - val_loss: 0.2691
Epoch 38/256
3063/3063 - 54s - loss: 0.2605 - val_loss: 0.2747
Epoch 39/256
3063/3063 - 54s - loss: 0.2583 - val_loss: 0.2899
Epoch 40/256
3063/3063 - 54s - loss: 0.2588 - val_loss: 0.2718
Epoch 41/256
3063/3063 - 53s - loss: 0.2585 - val_loss: 0.2720
Epoch 42/256
3063/3063 - 54s - loss: 0.2574 - val_loss: 0.2678
Epoch 43/256
3063/3063 - 54s - loss: 0.2552 - val_loss: 0.2639
Epoch 44/256
3063/3063 - 54s - loss: 0.2553 - val_loss: 0.2677
Epoch 45/256
3063/3063 - 59s - loss: 0.2546 - val_loss: 0.2676
Epoch 46/256
3063/3063 - 57s - loss: 0.2532 - val_loss: 0.2804
Epoch 47/256
3063/3063 - 67s - loss: 0.2520 - val_loss: 0.2849
Epoch 48/256
3063/3063 - 65s - loss: 0.2514 - val_loss: 0.2649
Epoch 49/256
3063/3063 - 66s - loss: 0.2505 - val_loss: 0.2801
Epoch 50/256
3063/3063 - 66s - loss: 0.2502 - val_loss: 0.2664
Epoch 51/256
3063/3063 - 66s - loss: 0.2505 - val_loss: 0.2784
Epoch 52/256
3063/3063 - 60s - loss: 0.2485 - val_loss: 0.2769
Epoch 53/256
3063/3063 - 66s - loss: 0.2483 - val_loss: 0.2622
Epoch 54/256
3063/3063 - 67s - loss: 0.2483 - val_loss: 0.2650
Epoch 55/256
3063/3063 - 66s - loss: 0.2481 - val_loss: 0.2669
Epoch 56/256
3063/3063 - 65s - loss: 0.2465 - val_loss: 0.2619
Epoch 57/256
3063/3063 - 56s - loss: 0.2462 - val_loss: 0.2771
Epoch 58/256
3063/3063 - 54s - loss: 0.2460 - val_loss: 0.2712
Epoch 59/256
3063/3063 - 55s - loss: 0.2462 - val_loss: 0.2816
Epoch 60/256
3063/3063 - 54s - loss: 0.2450 - val_loss: 0.2683
Epoch 61/256
3063/3063 - 54s - loss: 0.2444 - val_loss: 0.2761
Epoch 62/256
3063/3063 - 55s - loss: 0.2433 - val_loss: 0.2737
Epoch 63/256
3063/3063 - 54s - loss: 0.2425 - val_loss: 0.2842
Epoch 64/256
3063/3063 - 54s - loss: 0.2423 - val_loss: 0.2723
Epoch 65/256
3063/3063 - 54s - loss: 0.2414 - val_loss: 0.2708
Epoch 66/256
3063/3063 - 54s - loss: 0.2411 - val_loss: 0.2667
Epoch 67/256
3063/3063 - 54s - loss: 0.2401 - val_loss: 0.2644
Epoch 68/256
3063/3063 - 54s - loss: 0.2404 - val_loss: 0.2685
Epoch 69/256
3063/3063 - 54s - loss: 0.2385 - val_loss: 0.2632
Epoch 70/256
3063/3063 - 54s - loss: 0.2381 - val_loss: 0.2618
Epoch 71/256
3063/3063 - 54s - loss: 0.2384 - val_loss: 0.2638
Epoch 72/256
3063/3063 - 54s - loss: 0.2378 - val_loss: 0.2978
Epoch 73/256
3063/3063 - 54s - loss: 0.2373 - val_loss: 0.2606
Epoch 74/256
3063/3063 - 54s - loss: 0.2358 - val_loss: 0.2603
Epoch 75/256
3063/3063 - 54s - loss: 0.2363 - val_loss: 0.2615
Epoch 76/256
3063/3063 - 54s - loss: 0.2363 - val_loss: 0.2623
Epoch 77/256
3063/3063 - 54s - loss: 0.2352 - val_loss: 0.2594
Epoch 78/256
3063/3063 - 54s - loss: 0.2345 - val_loss: 0.2679
Epoch 79/256
3063/3063 - 55s - loss: 0.2346 - val_loss: 0.2775
Epoch 80/256
3063/3063 - 55s - loss: 0.2337 - val_loss: 0.2697
Epoch 81/256
3063/3063 - 54s - loss: 0.2342 - val_loss: 0.2665
Epoch 82/256
3063/3063 - 55s - loss: 0.2332 - val_loss: 0.3415
Epoch 83/256
3063/3063 - 54s - loss: 0.2317 - val_loss: 0.2641
Epoch 84/256
3063/3063 - 54s - loss: 0.2314 - val_loss: 0.2623
Epoch 85/256
3063/3063 - 54s - loss: 0.2310 - val_loss: 0.2614
Epoch 86/256
3063/3063 - 53s - loss: 0.2311 - val_loss: 0.2604
Epoch 87/256
3063/3063 - 54s - loss: 0.2300 - val_loss: 0.2645
Epoch 88/256
3063/3063 - 54s - loss: 0.2303 - val_loss: 0.2671
Epoch 89/256
3063/3063 - 54s - loss: 0.2294 - val_loss: 0.2671
Epoch 90/256
3063/3063 - 67s - loss: 0.2286 - val_loss: 0.2664
Epoch 91/256
3063/3063 - 54s - loss: 0.2286 - val_loss: 0.2802
Epoch 92/256
3063/3063 - 54s - loss: 0.2278 - val_loss: 0.2808
Epoch 93/256
3063/3063 - 54s - loss: 0.2277 - val_loss: 0.2912
Epoch 94/256
3063/3063 - 54s - loss: 0.2273 - val_loss: 0.2629
Epoch 95/256
3063/3063 - 54s - loss: 0.2275 - val_loss: 0.2705
Epoch 96/256
3063/3063 - 54s - loss: 0.2259 - val_loss: 0.2796
Epoch 97/256
3063/3063 - 54s - loss: 0.2255 - val_loss: 0.2701
Epoch 98/256
3063/3063 - 54s - loss: 0.2256 - val_loss: 0.2766
Epoch 99/256
3063/3063 - 55s - loss: 0.2256 - val_loss: 0.2663
Epoch 100/256
3063/3063 - 54s - loss: 0.2248 - val_loss: 0.2637
Epoch 101/256
3063/3063 - 55s - loss: 0.2241 - val_loss: 0.2746
Epoch 102/256
3063/3063 - 54s - loss: 0.2249 - val_loss: 0.2750
Epoch 103/256
3063/3063 - 54s - loss: 0.2242 - val_loss: 0.2652
Epoch 104/256
3063/3063 - 54s - loss: 0.2227 - val_loss: 0.2613
Epoch 105/256
3063/3063 - 54s - loss: 0.2224 - val_loss: 0.2801
Epoch 106/256
3063/3063 - 55s - loss: 0.2201 - val_loss: 0.2612
Epoch 107/256
3063/3063 - 55s - loss: 0.2205 - val_loss: 0.2614
Epoch 108/256
3063/3063 - 55s - loss: 0.2212 - val_loss: 0.2697
Epoch 109/256
3063/3063 - 71s - loss: 0.2202 - val_loss: 0.2654
2022-06-02 18:36:22.224122: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses, conv2d_2_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.
###
first saving models
currently on latent_two_5_(64, 128, 256, 128, 2)_64
latent_two_5_(64, 128, 256, 128, 2)_64 is saved in models/data100k_raw_combined_atlas_cut_latent_two_5_(64, 128, 256, 128, 2)_64latent28
now saving paramters of experimenter
saved experimenter at /data/delon/experimenter/data100k_raw_combined_atlas_cutlatent28
RIGHT NOW: latent_eight
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/256
3063/3063 - 57s - loss: 0.3904 - val_loss: 0.3562
Epoch 2/256
3063/3063 - 56s - loss: 0.3557 - val_loss: 0.3646
Epoch 3/256
3063/3063 - 55s - loss: 0.3471 - val_loss: 0.3385
Epoch 4/256
3063/3063 - 56s - loss: 0.3381 - val_loss: 0.3385
Epoch 5/256
3063/3063 - 56s - loss: 0.3319 - val_loss: 0.3211
Epoch 6/256
3063/3063 - 56s - loss: 0.3260 - val_loss: 0.3266
Epoch 7/256
3063/3063 - 56s - loss: 0.3189 - val_loss: 0.3166
Epoch 8/256
3063/3063 - 55s - loss: 0.3115 - val_loss: 0.3061
Epoch 9/256
3063/3063 - 56s - loss: 0.3070 - val_loss: 0.3041
Epoch 10/256
3063/3063 - 56s - loss: 0.3001 - val_loss: 0.3030
Epoch 11/256
3063/3063 - 56s - loss: 0.2953 - val_loss: 0.2930
Epoch 12/256
3063/3063 - 56s - loss: 0.2909 - val_loss: 0.3114
Epoch 13/256
3063/3063 - 56s - loss: 0.2870 - val_loss: 0.2939
Epoch 14/256
3063/3063 - 57s - loss: 0.2835 - val_loss: 0.3024
Epoch 15/256
3063/3063 - 56s - loss: 0.2798 - val_loss: 0.2913
Epoch 16/256
3063/3063 - 56s - loss: 0.2783 - val_loss: 0.2766
Epoch 17/256
3063/3063 - 56s - loss: 0.2764 - val_loss: 0.2782
Epoch 18/256
3063/3063 - 56s - loss: 0.2732 - val_loss: 0.2883
Epoch 19/256
3063/3063 - 56s - loss: 0.2724 - val_loss: 0.2878
Epoch 20/256
3063/3063 - 56s - loss: 0.2706 - val_loss: 0.2743
Epoch 21/256
3063/3063 - 56s - loss: 0.2687 - val_loss: 0.2842
Epoch 22/256
3063/3063 - 56s - loss: 0.2674 - val_loss: 0.3061
Epoch 23/256
3063/3063 - 56s - loss: 0.2644 - val_loss: 0.2701
Epoch 24/256
3063/3063 - 56s - loss: 0.2630 - val_loss: 0.3071
Epoch 25/256
3063/3063 - 56s - loss: 0.2622 - val_loss: 0.2675
Epoch 26/256
3063/3063 - 56s - loss: 0.2604 - val_loss: 0.2685
Epoch 27/256
3063/3063 - 56s - loss: 0.2582 - val_loss: 0.2785
Epoch 28/256
3063/3063 - 55s - loss: 0.2566 - val_loss: 0.2641
Epoch 29/256
3063/3063 - 56s - loss: 0.2549 - val_loss: 0.2658
Epoch 30/256
3063/3063 - 56s - loss: 0.2536 - val_loss: 0.2650
Epoch 31/256
3063/3063 - 56s - loss: 0.2532 - val_loss: 0.2742
Epoch 32/256
3063/3063 - 56s - loss: 0.2515 - val_loss: 0.2752
Epoch 33/256
3063/3063 - 56s - loss: 0.2507 - val_loss: 0.2671
Epoch 34/256
3063/3063 - 56s - loss: 0.2488 - val_loss: 0.2649
Epoch 35/256
3063/3063 - 56s - loss: 0.2475 - val_loss: 0.2643
Epoch 36/256
3063/3063 - 56s - loss: 0.2463 - val_loss: 0.2560
Epoch 37/256
3063/3063 - 56s - loss: 0.2448 - val_loss: 0.2541
Epoch 38/256
3063/3063 - 55s - loss: 0.2440 - val_loss: 0.2615
Epoch 39/256
3063/3063 - 55s - loss: 0.2422 - val_loss: 0.2624
Epoch 40/256
3063/3063 - 56s - loss: 0.2404 - val_loss: 0.2512
Epoch 41/256
3063/3063 - 55s - loss: 0.2407 - val_loss: 0.2526
Epoch 42/256
3063/3063 - 56s - loss: 0.2405 - val_loss: 0.2573
Epoch 43/256
3063/3063 - 55s - loss: 0.2383 - val_loss: 0.2552
Epoch 44/256
3063/3063 - 56s - loss: 0.2387 - val_loss: 0.2549
Epoch 45/256
3063/3063 - 56s - loss: 0.2369 - val_loss: 0.2509
Epoch 46/256
3063/3063 - 56s - loss: 0.2357 - val_loss: 0.2502
Epoch 47/256
3063/3063 - 55s - loss: 0.2336 - val_loss: 0.2636
Epoch 48/256
3063/3063 - 56s - loss: 0.2340 - val_loss: 0.2488
Epoch 49/256
3063/3063 - 56s - loss: 0.2327 - val_loss: 0.2584
Epoch 50/256
3063/3063 - 56s - loss: 0.2307 - val_loss: 0.2522
Epoch 51/256
3063/3063 - 55s - loss: 0.2309 - val_loss: 0.2620
Epoch 52/256
3063/3063 - 57s - loss: 0.2298 - val_loss: 0.2728
Epoch 53/256
3063/3063 - 61s - loss: 0.2287 - val_loss: 0.2517
Epoch 54/256
3063/3063 - 66s - loss: 0.2279 - val_loss: 0.2466
Epoch 55/256
3063/3063 - 65s - loss: 0.2279 - val_loss: 0.2486
Epoch 56/256
3063/3063 - 65s - loss: 0.2267 - val_loss: 0.2505
Epoch 57/256
3063/3063 - 65s - loss: 0.2254 - val_loss: 0.2491
Epoch 58/256
3063/3063 - 65s - loss: 0.2250 - val_loss: 0.2603
Epoch 59/256
3063/3063 - 67s - loss: 0.2247 - val_loss: 0.2625
Epoch 60/256
3063/3063 - 58s - loss: 0.2240 - val_loss: 0.2465
Epoch 61/256
3063/3063 - 67s - loss: 0.2241 - val_loss: 0.2556
Epoch 62/256
3063/3063 - 67s - loss: 0.2224 - val_loss: 0.2611
Epoch 63/256
3063/3063 - 67s - loss: 0.2212 - val_loss: 0.2698
Epoch 64/256
3063/3063 - 67s - loss: 0.2221 - val_loss: 0.2486
Epoch 65/256
3063/3063 - 58s - loss: 0.2205 - val_loss: 0.2479
Epoch 66/256
3063/3063 - 57s - loss: 0.2198 - val_loss: 0.2541
Epoch 67/256
3063/3063 - 55s - loss: 0.2192 - val_loss: 0.2501
Epoch 68/256
3063/3063 - 56s - loss: 0.2180 - val_loss: 0.2462
Epoch 69/256
3063/3063 - 56s - loss: 0.2177 - val_loss: 0.2504
Epoch 70/256
3063/3063 - 55s - loss: 0.2159 - val_loss: 0.2488
Epoch 71/256
3063/3063 - 55s - loss: 0.2170 - val_loss: 0.2507
Epoch 72/256
3063/3063 - 55s - loss: 0.2159 - val_loss: 0.2561
Epoch 73/256
3063/3063 - 55s - loss: 0.2146 - val_loss: 0.2495
Epoch 74/256
3063/3063 - 55s - loss: 0.2142 - val_loss: 0.2441
Epoch 75/256
3063/3063 - 55s - loss: 0.2143 - val_loss: 0.2515
Epoch 76/256
3063/3063 - 56s - loss: 0.2137 - val_loss: 0.2529
Epoch 77/256
3063/3063 - 56s - loss: 0.2120 - val_loss: 0.2453
Epoch 78/256
3063/3063 - 56s - loss: 0.2121 - val_loss: 0.2485
Epoch 79/256
3063/3063 - 56s - loss: 0.2116 - val_loss: 0.2702
Epoch 80/256
3063/3063 - 56s - loss: 0.2111 - val_loss: 0.2465
Epoch 81/256
3063/3063 - 56s - loss: 0.2109 - val_loss: 0.2472
Epoch 82/256
3063/3063 - 56s - loss: 0.2093 - val_loss: 0.3082
Epoch 83/256
3063/3063 - 55s - loss: 0.2095 - val_loss: 0.2548
Epoch 84/256
3063/3063 - 56s - loss: 0.2094 - val_loss: 0.2435
Epoch 85/256
3063/3063 - 56s - loss: 0.2076 - val_loss: 0.2551
Epoch 86/256
3063/3063 - 55s - loss: 0.2081 - val_loss: 0.2569
Epoch 87/256
3063/3063 - 55s - loss: 0.2064 - val_loss: 0.2536
Epoch 88/256
3063/3063 - 56s - loss: 0.2073 - val_loss: 0.2532
Epoch 89/256
3063/3063 - 56s - loss: 0.2057 - val_loss: 0.2614
Epoch 90/256
3063/3063 - 56s - loss: 0.2036 - val_loss: 0.2533
Epoch 91/256
3063/3063 - 56s - loss: 0.2042 - val_loss: 0.2569
Epoch 92/256
3063/3063 - 56s - loss: 0.2038 - val_loss: 0.2656
Epoch 93/256
3063/3063 - 56s - loss: 0.2027 - val_loss: 0.2624
Epoch 94/256
3063/3063 - 56s - loss: 0.2032 - val_loss: 0.2536
Epoch 95/256
3063/3063 - 56s - loss: 0.2025 - val_loss: 0.2572
Epoch 96/256
3063/3063 - 56s - loss: 0.2008 - val_loss: 0.2554
Epoch 97/256
3063/3063 - 56s - loss: 0.2002 - val_loss: 0.2718
Epoch 98/256
3063/3063 - 56s - loss: 0.2013 - val_loss: 0.2488
Epoch 99/256
3063/3063 - 56s - loss: 0.2012 - val_loss: 0.2476
Epoch 100/256
3063/3063 - 56s - loss: 0.1985 - val_loss: 0.2499
Epoch 101/256
3063/3063 - 56s - loss: 0.1983 - val_loss: 0.2513
Epoch 102/256
3063/3063 - 56s - loss: 0.1989 - val_loss: 0.2566
Epoch 103/256
3063/3063 - 56s - loss: 0.1984 - val_loss: 0.2563
Epoch 104/256
3063/3063 - 55s - loss: 0.1971 - val_loss: 0.2521
Epoch 105/256
3063/3063 - 56s - loss: 0.1979 - val_loss: 0.2531
Epoch 106/256
3063/3063 - 56s - loss: 0.1961 - val_loss: 0.2593
Epoch 107/256
3063/3063 - 56s - loss: 0.1954 - val_loss: 0.2592
Epoch 108/256
3063/3063 - 56s - loss: 0.1954 - val_loss: 0.2611
Epoch 109/256
3063/3063 - 55s - loss: 0.1941 - val_loss: 0.2532
Epoch 110/256
3063/3063 - 55s - loss: 0.1939 - val_loss: 0.2645
Epoch 111/256
3063/3063 - 56s - loss: 0.1936 - val_loss: 0.2661
Epoch 112/256
3063/3063 - 56s - loss: 0.1918 - val_loss: 0.2573
Epoch 113/256
3063/3063 - 56s - loss: 0.1910 - val_loss: 0.2597
Epoch 114/256
3063/3063 - 56s - loss: 0.1908 - val_loss: 0.2523
Epoch 115/256
3063/3063 - 56s - loss: 0.1911 - val_loss: 0.2721
Epoch 116/256
3063/3063 - 56s - loss: 0.1898 - val_loss: 0.2543
WARNING:absl:Found untraced functions such as conv2d_5_layer_call_fn, conv2d_5_layer_call_and_return_conditional_losses, conv2d_6_layer_call_fn, conv2d_6_layer_call_and_return_conditional_losses, conv2d_7_layer_call_fn while saving (showing 5 of 50). These functions will not be directly callable after loading.
###
first saving models
currently on latent_two_5_(64, 128, 256, 128, 2)_64
	this one already saved, skipped
currently on latent_eight_5_(64, 128, 256, 128, 8)_64
latent_eight_5_(64, 128, 256, 128, 8)_64 is saved in models/data100k_raw_combined_atlas_cut_latent_eight_5_(64, 128, 256, 128, 8)_64latent28
now saving paramters of experimenter
saved experimenter at /data/delon/experimenter/data100k_raw_combined_atlas_cutlatent28
