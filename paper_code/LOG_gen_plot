nohup: ignoring input
2022-05-18 23:08:52.890869: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-05-18 23:10:09.460094: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1
2022-05-18 23:10:09.475408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:84:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2022-05-18 23:10:09.476426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: 
pciBusID: 0000:85:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2022-05-18 23:10:09.476462: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-05-18 23:10:09.479939: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2022-05-18 23:10:09.479997: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
2022-05-18 23:10:09.481428: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10
2022-05-18 23:10:09.481729: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10
2022-05-18 23:10:09.485877: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11
2022-05-18 23:10:09.486737: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11
2022-05-18 23:10:09.486964: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2022-05-18 23:10:09.490720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1
2022-05-18 23:10:09.491415: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-05-18 23:10:09.607104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:84:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2022-05-18 23:10:09.608164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: 
pciBusID: 0000:85:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2022-05-18 23:10:09.612006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1
2022-05-18 23:10:09.612220: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2022-05-18 23:10:10.572127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-05-18 23:10:10.572187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1 
2022-05-18 23:10:10.572203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N Y 
2022-05-18 23:10:10.572210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   Y N 
2022-05-18 23:10:10.577216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8599 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:84:00.0, compute capability: 3.7)
2022-05-18 23:10:10.578694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10792 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id: 0000:85:00.0, compute capability: 3.7)
2022-05-18 23:10:10.957632: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
2022-05-18 23:10:10.958405: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2099935000 Hz
2022-05-18 23:10:12.843045: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2022-05-18 23:10:13.111871: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
Initializing Experimenter
	Loading Data from ../data/data100k_raw_combined_atlas_cut.pkl
	Data Loaded
	Creating Splits
	Splits Created
Done initalizing
RIGHT NOW: particlewise
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/512
3063/3063 - 14s - loss: 0.3998 - val_loss: 0.3703
Epoch 2/512
3063/3063 - 11s - loss: 0.3634 - val_loss: 0.3578
Epoch 3/512
3063/3063 - 11s - loss: 0.3569 - val_loss: 0.3543
Epoch 4/512
3063/3063 - 11s - loss: 0.3513 - val_loss: 0.3524
Epoch 5/512
3063/3063 - 11s - loss: 0.3476 - val_loss: 0.3474
Epoch 6/512
3063/3063 - 10s - loss: 0.3450 - val_loss: 0.3418
Epoch 7/512
3063/3063 - 11s - loss: 0.3416 - val_loss: 0.3559
Epoch 8/512
3063/3063 - 11s - loss: 0.3397 - val_loss: 0.3402
Epoch 9/512
3063/3063 - 10s - loss: 0.3386 - val_loss: 0.3402
Epoch 10/512
3063/3063 - 10s - loss: 0.3371 - val_loss: 0.3454
Epoch 11/512
3063/3063 - 10s - loss: 0.3359 - val_loss: 0.3468
Epoch 12/512
3063/3063 - 11s - loss: 0.3347 - val_loss: 0.3358
Epoch 13/512
3063/3063 - 10s - loss: 0.3341 - val_loss: 0.3330
Epoch 14/512
3063/3063 - 10s - loss: 0.3335 - val_loss: 0.3368
Epoch 15/512
3063/3063 - 10s - loss: 0.3314 - val_loss: 0.3423
Epoch 16/512
3063/3063 - 10s - loss: 0.3307 - val_loss: 0.3437
Epoch 17/512
3063/3063 - 10s - loss: 0.3293 - val_loss: 0.3321
Epoch 18/512
3063/3063 - 11s - loss: 0.3269 - val_loss: 0.3369
Epoch 19/512
3063/3063 - 11s - loss: 0.3249 - val_loss: 0.3354
Epoch 20/512
3063/3063 - 10s - loss: 0.3234 - val_loss: 0.3396
Epoch 21/512
3063/3063 - 11s - loss: 0.3217 - val_loss: 0.3279
Epoch 22/512
3063/3063 - 10s - loss: 0.3211 - val_loss: 0.3564
Epoch 23/512
3063/3063 - 10s - loss: 0.3184 - val_loss: 0.3259
Epoch 24/512
3063/3063 - 10s - loss: 0.3181 - val_loss: 0.3468
Epoch 25/512
3063/3063 - 10s - loss: 0.3169 - val_loss: 0.3235
Epoch 26/512
3063/3063 - 10s - loss: 0.3166 - val_loss: 0.3205
Epoch 27/512
3063/3063 - 10s - loss: 0.3151 - val_loss: 0.3245
Epoch 28/512
3063/3063 - 11s - loss: 0.3142 - val_loss: 0.3226
Epoch 29/512
3063/3063 - 11s - loss: 0.3134 - val_loss: 0.3226
Epoch 30/512
3063/3063 - 10s - loss: 0.3114 - val_loss: 0.3229
Epoch 31/512
3063/3063 - 10s - loss: 0.3108 - val_loss: 0.3235
Epoch 32/512
3063/3063 - 11s - loss: 0.3100 - val_loss: 0.3249
Epoch 33/512
3063/3063 - 10s - loss: 0.3102 - val_loss: 0.3223
Epoch 34/512
3063/3063 - 11s - loss: 0.3081 - val_loss: 0.3289
Epoch 35/512
3063/3063 - 10s - loss: 0.3077 - val_loss: 0.3211
Epoch 36/512
3063/3063 - 10s - loss: 0.3068 - val_loss: 0.3267
Epoch 37/512
3063/3063 - 10s - loss: 0.3065 - val_loss: 0.3224
Epoch 38/512
3063/3063 - 10s - loss: 0.3050 - val_loss: 0.3234
Epoch 39/512
3063/3063 - 10s - loss: 0.3042 - val_loss: 0.3237
Epoch 40/512
3063/3063 - 10s - loss: 0.3041 - val_loss: 0.3281
Epoch 41/512
3063/3063 - 10s - loss: 0.3033 - val_loss: 0.3190
Epoch 42/512
3063/3063 - 10s - loss: 0.3026 - val_loss: 0.3259
Epoch 43/512
3063/3063 - 10s - loss: 0.3016 - val_loss: 0.3169
Epoch 44/512
3063/3063 - 10s - loss: 0.3017 - val_loss: 0.3200
Epoch 45/512
3063/3063 - 11s - loss: 0.3004 - val_loss: 0.3251
Epoch 46/512
3063/3063 - 10s - loss: 0.3000 - val_loss: 0.3169
Epoch 47/512
3063/3063 - 10s - loss: 0.2991 - val_loss: 0.3270
Epoch 48/512
3063/3063 - 11s - loss: 0.2989 - val_loss: 0.3495
Epoch 49/512
3063/3063 - 11s - loss: 0.2974 - val_loss: 0.3179
Epoch 50/512
3063/3063 - 10s - loss: 0.2970 - val_loss: 0.3203
Epoch 51/512
3063/3063 - 10s - loss: 0.2967 - val_loss: 0.3295
Epoch 52/512
3063/3063 - 10s - loss: 0.2964 - val_loss: 0.3315
Epoch 53/512
3063/3063 - 11s - loss: 0.2955 - val_loss: 0.3262
Epoch 54/512
3063/3063 - 10s - loss: 0.2951 - val_loss: 0.3252
Epoch 55/512
3063/3063 - 10s - loss: 0.2948 - val_loss: 0.3263
Epoch 56/512
3063/3063 - 11s - loss: 0.2940 - val_loss: 0.3221
Epoch 57/512
3063/3063 - 10s - loss: 0.2929 - val_loss: 0.3288
Epoch 58/512
3063/3063 - 11s - loss: 0.2916 - val_loss: 0.3230
Epoch 59/512
3063/3063 - 10s - loss: 0.2917 - val_loss: 0.3180
Epoch 60/512
3063/3063 - 10s - loss: 0.2909 - val_loss: 0.3275
Epoch 61/512
3063/3063 - 10s - loss: 0.2901 - val_loss: 0.3267
Epoch 62/512
3063/3063 - 10s - loss: 0.2897 - val_loss: 0.3413
Epoch 63/512
3063/3063 - 10s - loss: 0.2885 - val_loss: 0.3382
Epoch 64/512
3063/3063 - 10s - loss: 0.2883 - val_loss: 0.3230
Epoch 65/512
3063/3063 - 11s - loss: 0.2878 - val_loss: 0.3238
Epoch 66/512
3063/3063 - 10s - loss: 0.2874 - val_loss: 0.3298
Epoch 67/512
3063/3063 - 10s - loss: 0.2856 - val_loss: 0.3233
Epoch 68/512
3063/3063 - 11s - loss: 0.2855 - val_loss: 0.3213
Epoch 69/512
3063/3063 - 11s - loss: 0.2846 - val_loss: 0.3295
Epoch 70/512
3063/3063 - 11s - loss: 0.2842 - val_loss: 0.3287
Epoch 71/512
3063/3063 - 10s - loss: 0.2840 - val_loss: 0.3558
Epoch 72/512
3063/3063 - 11s - loss: 0.2840 - val_loss: 0.3323
Epoch 73/512
3063/3063 - 10s - loss: 0.2828 - val_loss: 0.3248
Epoch 74/512
3063/3063 - 11s - loss: 0.2812 - val_loss: 0.3294
Epoch 75/512
3063/3063 - 11s - loss: 0.2814 - val_loss: 0.3344
2022-05-18 23:24:22.836796: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2022-05-18 23:24:23.184667: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8201
###
RIGHT NOW: pairwise
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/512
3063/3063 - 36s - loss: 0.3995 - val_loss: 0.3610
Epoch 2/512
3063/3063 - 33s - loss: 0.3605 - val_loss: 0.3578
Epoch 3/512
3063/3063 - 33s - loss: 0.3538 - val_loss: 0.3471
Epoch 4/512
3063/3063 - 32s - loss: 0.3490 - val_loss: 0.3551
Epoch 5/512
3063/3063 - 33s - loss: 0.3453 - val_loss: 0.3409
Epoch 6/512
3063/3063 - 33s - loss: 0.3424 - val_loss: 0.3359
Epoch 7/512
3063/3063 - 33s - loss: 0.3389 - val_loss: 0.3395
Epoch 8/512
3063/3063 - 33s - loss: 0.3354 - val_loss: 0.3326
Epoch 9/512
3063/3063 - 33s - loss: 0.3335 - val_loss: 0.3301
Epoch 10/512
3063/3063 - 34s - loss: 0.3287 - val_loss: 0.3288
Epoch 11/512
3063/3063 - 34s - loss: 0.3215 - val_loss: 0.3182
Epoch 12/512
3063/3063 - 33s - loss: 0.3146 - val_loss: 0.3126
Epoch 13/512
3063/3063 - 33s - loss: 0.3072 - val_loss: 0.2987
Epoch 14/512
3063/3063 - 34s - loss: 0.3000 - val_loss: 0.3119
Epoch 15/512
3063/3063 - 34s - loss: 0.2933 - val_loss: 0.3079
Epoch 16/512
3063/3063 - 34s - loss: 0.2890 - val_loss: 0.2863
Epoch 17/512
3063/3063 - 34s - loss: 0.2845 - val_loss: 0.2943
Epoch 18/512
3063/3063 - 35s - loss: 0.2802 - val_loss: 0.2823
Epoch 19/512
3063/3063 - 34s - loss: 0.2761 - val_loss: 0.2800
Epoch 20/512
3063/3063 - 34s - loss: 0.2730 - val_loss: 0.2733
Epoch 21/512
3063/3063 - 34s - loss: 0.2705 - val_loss: 0.2752
Epoch 22/512
3063/3063 - 34s - loss: 0.2680 - val_loss: 0.3178
Epoch 23/512
3063/3063 - 34s - loss: 0.2654 - val_loss: 0.2729
Epoch 24/512
3063/3063 - 34s - loss: 0.2626 - val_loss: 0.2797
Epoch 25/512
3063/3063 - 34s - loss: 0.2605 - val_loss: 0.2716
Epoch 26/512
3063/3063 - 34s - loss: 0.2580 - val_loss: 0.2637
Epoch 27/512
3063/3063 - 33s - loss: 0.2554 - val_loss: 0.2738
Epoch 28/512
3063/3063 - 34s - loss: 0.2547 - val_loss: 0.2554
Epoch 29/512
3063/3063 - 33s - loss: 0.2524 - val_loss: 0.2541
Epoch 30/512
3063/3063 - 34s - loss: 0.2499 - val_loss: 0.2677
Epoch 31/512
3063/3063 - 33s - loss: 0.2492 - val_loss: 0.2645
Epoch 32/512
3063/3063 - 34s - loss: 0.2478 - val_loss: 0.2641
Epoch 33/512
3063/3063 - 34s - loss: 0.2465 - val_loss: 0.2521
Epoch 34/512
3063/3063 - 34s - loss: 0.2443 - val_loss: 0.2577
Epoch 35/512
3063/3063 - 34s - loss: 0.2432 - val_loss: 0.2671
Epoch 36/512
3063/3063 - 33s - loss: 0.2418 - val_loss: 0.2541
Epoch 37/512
3063/3063 - 33s - loss: 0.2404 - val_loss: 0.2579
Epoch 38/512
3063/3063 - 34s - loss: 0.2394 - val_loss: 0.2702
Epoch 39/512
3063/3063 - 33s - loss: 0.2369 - val_loss: 0.2705
Epoch 40/512
3063/3063 - 34s - loss: 0.2366 - val_loss: 0.2571
Epoch 41/512
3063/3063 - 33s - loss: 0.2354 - val_loss: 0.2503
Epoch 42/512
3063/3063 - 34s - loss: 0.2340 - val_loss: 0.2552
Epoch 43/512
3063/3063 - 33s - loss: 0.2337 - val_loss: 0.2446
Epoch 44/512
3063/3063 - 33s - loss: 0.2315 - val_loss: 0.2471
Epoch 45/512
3063/3063 - 34s - loss: 0.2304 - val_loss: 0.2459
Epoch 46/512
3063/3063 - 34s - loss: 0.2285 - val_loss: 0.2477
Epoch 47/512
3063/3063 - 33s - loss: 0.2284 - val_loss: 0.2639
Epoch 48/512
3063/3063 - 33s - loss: 0.2273 - val_loss: 0.2437
Epoch 49/512
3063/3063 - 33s - loss: 0.2268 - val_loss: 0.2633
Epoch 50/512
3063/3063 - 33s - loss: 0.2257 - val_loss: 0.2511
Epoch 51/512
3063/3063 - 33s - loss: 0.2248 - val_loss: 0.2674
Epoch 52/512
3063/3063 - 33s - loss: 0.2242 - val_loss: 0.2560
Epoch 53/512
3063/3063 - 33s - loss: 0.2222 - val_loss: 0.2533
Epoch 54/512
3063/3063 - 34s - loss: 0.2214 - val_loss: 0.2419
Epoch 55/512
3063/3063 - 33s - loss: 0.2208 - val_loss: 0.2482
Epoch 56/512
3063/3063 - 33s - loss: 0.2192 - val_loss: 0.2454
Epoch 57/512
3063/3063 - 37s - loss: 0.2190 - val_loss: 0.2561
Epoch 58/512
3063/3063 - 33s - loss: 0.2172 - val_loss: 0.2501
Epoch 59/512
3063/3063 - 33s - loss: 0.2170 - val_loss: 0.2479
Epoch 60/512
3063/3063 - 33s - loss: 0.2160 - val_loss: 0.2440
Epoch 61/512
3063/3063 - 33s - loss: 0.2155 - val_loss: 0.2526
Epoch 62/512
3063/3063 - 33s - loss: 0.2145 - val_loss: 0.2625
Epoch 63/512
3063/3063 - 33s - loss: 0.2133 - val_loss: 0.2576
Epoch 64/512
3063/3063 - 33s - loss: 0.2131 - val_loss: 0.2504
Epoch 65/512
3063/3063 - 34s - loss: 0.2107 - val_loss: 0.2565
Epoch 66/512
3063/3063 - 34s - loss: 0.2118 - val_loss: 0.2687
Epoch 67/512
3063/3063 - 34s - loss: 0.2102 - val_loss: 0.2418
Epoch 68/512
3063/3063 - 34s - loss: 0.2088 - val_loss: 0.2424
Epoch 69/512
3063/3063 - 33s - loss: 0.2086 - val_loss: 0.2413
Epoch 70/512
3063/3063 - 33s - loss: 0.2073 - val_loss: 0.2456
Epoch 71/512
3063/3063 - 34s - loss: 0.2066 - val_loss: 0.2529
Epoch 72/512
3063/3063 - 34s - loss: 0.2075 - val_loss: 0.2630
Epoch 73/512
3063/3063 - 33s - loss: 0.2046 - val_loss: 0.2520
Epoch 74/512
3063/3063 - 34s - loss: 0.2038 - val_loss: 0.2536
Epoch 75/512
3063/3063 - 33s - loss: 0.2031 - val_loss: 0.2494
Epoch 76/512
3063/3063 - 33s - loss: 0.2019 - val_loss: 0.2516
Epoch 77/512
3063/3063 - 33s - loss: 0.2015 - val_loss: 0.2430
Epoch 78/512
3063/3063 - 34s - loss: 0.1999 - val_loss: 0.2422
Epoch 79/512
3063/3063 - 34s - loss: 0.1993 - val_loss: 0.2468
Epoch 80/512
3063/3063 - 34s - loss: 0.1991 - val_loss: 0.2489
Epoch 81/512
3063/3063 - 34s - loss: 0.1983 - val_loss: 0.2438
Epoch 82/512
3063/3063 - 33s - loss: 0.1977 - val_loss: 0.2874
Epoch 83/512
3063/3063 - 33s - loss: 0.1960 - val_loss: 0.2494
Epoch 84/512
3063/3063 - 34s - loss: 0.1954 - val_loss: 0.2397
Epoch 85/512
3063/3063 - 34s - loss: 0.1951 - val_loss: 0.2524
Epoch 86/512
3063/3063 - 33s - loss: 0.1958 - val_loss: 0.2579
Epoch 87/512
3063/3063 - 33s - loss: 0.1926 - val_loss: 0.2537
Epoch 88/512
3063/3063 - 33s - loss: 0.1924 - val_loss: 0.2516
Epoch 89/512
3063/3063 - 33s - loss: 0.1916 - val_loss: 0.2583
Epoch 90/512
3063/3063 - 33s - loss: 0.1895 - val_loss: 0.2580
Epoch 91/512
3063/3063 - 33s - loss: 0.1892 - val_loss: 0.2539
Epoch 92/512
3063/3063 - 34s - loss: 0.1891 - val_loss: 0.2675
Epoch 93/512
3063/3063 - 33s - loss: 0.1889 - val_loss: 0.2761
Epoch 94/512
3063/3063 - 34s - loss: 0.1885 - val_loss: 0.2598
Epoch 95/512
3063/3063 - 34s - loss: 0.1867 - val_loss: 0.2615
Epoch 96/512
3063/3063 - 33s - loss: 0.1844 - val_loss: 0.2662
Epoch 97/512
3063/3063 - 34s - loss: 0.1848 - val_loss: 0.2789
Epoch 98/512
3063/3063 - 34s - loss: 0.1833 - val_loss: 0.2532
Epoch 99/512
3063/3063 - 34s - loss: 0.1837 - val_loss: 0.2590
Epoch 100/512
3063/3063 - 34s - loss: 0.1829 - val_loss: 0.2598
Epoch 101/512
3063/3063 - 33s - loss: 0.1831 - val_loss: 0.2492
Epoch 102/512
3063/3063 - 34s - loss: 0.1808 - val_loss: 0.2776
Epoch 103/512
3063/3063 - 34s - loss: 0.1820 - val_loss: 0.2593
Epoch 104/512
3063/3063 - 33s - loss: 0.1807 - val_loss: 0.2607
Epoch 105/512
3063/3063 - 33s - loss: 0.1789 - val_loss: 0.2641
Epoch 106/512
3063/3063 - 34s - loss: 0.1768 - val_loss: 0.2653
Epoch 107/512
3063/3063 - 33s - loss: 0.1765 - val_loss: 0.2628
Epoch 108/512
3063/3063 - 34s - loss: 0.1765 - val_loss: 0.2619
Epoch 109/512
3063/3063 - 34s - loss: 0.1758 - val_loss: 0.2512
Epoch 110/512
3063/3063 - 34s - loss: 0.1758 - val_loss: 0.2813
Epoch 111/512
3063/3063 - 33s - loss: 0.1740 - val_loss: 0.2651
Epoch 112/512
3063/3063 - 34s - loss: 0.1714 - val_loss: 0.2718
Epoch 113/512
3063/3063 - 33s - loss: 0.1706 - val_loss: 0.2811
Epoch 114/512
3063/3063 - 33s - loss: 0.1716 - val_loss: 0.2677
Epoch 115/512
3063/3063 - 34s - loss: 0.1706 - val_loss: 0.2688
Epoch 116/512
3063/3063 - 34s - loss: 0.1695 - val_loss: 0.2584
###
RIGHT NOW: tripletwise
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/512
3063/3063 - 298s - loss: 0.4006 - val_loss: 0.3667
Epoch 2/512
3063/3063 - 295s - loss: 0.3640 - val_loss: 0.3606
Epoch 3/512
3063/3063 - 295s - loss: 0.3558 - val_loss: 0.3493
Epoch 4/512
3063/3063 - 295s - loss: 0.3500 - val_loss: 0.3510
Epoch 5/512
3063/3063 - 295s - loss: 0.3465 - val_loss: 0.3388
Epoch 6/512
3063/3063 - 296s - loss: 0.3440 - val_loss: 0.3401
Epoch 7/512
3063/3063 - 295s - loss: 0.3409 - val_loss: 0.3456
Epoch 8/512
3063/3063 - 295s - loss: 0.3383 - val_loss: 0.3356
Epoch 9/512
3063/3063 - 295s - loss: 0.3370 - val_loss: 0.3350
Epoch 10/512
3063/3063 - 295s - loss: 0.3347 - val_loss: 0.3403
Epoch 11/512
3063/3063 - 295s - loss: 0.3324 - val_loss: 0.3398
Epoch 12/512
3063/3063 - 295s - loss: 0.3290 - val_loss: 0.3317
Epoch 13/512
3063/3063 - 295s - loss: 0.3270 - val_loss: 0.3236
Epoch 14/512
3063/3063 - 295s - loss: 0.3230 - val_loss: 0.3309
Epoch 15/512
3063/3063 - 296s - loss: 0.3192 - val_loss: 0.3275
Epoch 16/512
3063/3063 - 296s - loss: 0.3166 - val_loss: 0.3153
Epoch 17/512
3063/3063 - 295s - loss: 0.3117 - val_loss: 0.3108
Epoch 18/512
3063/3063 - 296s - loss: 0.3078 - val_loss: 0.3071
Epoch 19/512
3063/3063 - 295s - loss: 0.3043 - val_loss: 0.3158
Epoch 20/512
3063/3063 - 296s - loss: 0.3000 - val_loss: 0.3086
Epoch 21/512
3063/3063 - 295s - loss: 0.2955 - val_loss: 0.3031
Epoch 22/512
3063/3063 - 295s - loss: 0.2914 - val_loss: 0.3525
Epoch 23/512
3063/3063 - 295s - loss: 0.2870 - val_loss: 0.2879
Epoch 24/512
3063/3063 - 295s - loss: 0.2829 - val_loss: 0.2868
Epoch 25/512
3063/3063 - 295s - loss: 0.2796 - val_loss: 0.2838
Epoch 26/512
3063/3063 - 295s - loss: 0.2764 - val_loss: 0.2788
Epoch 27/512
3063/3063 - 295s - loss: 0.2733 - val_loss: 0.2905
Epoch 28/512
3063/3063 - 295s - loss: 0.2718 - val_loss: 0.2725
Epoch 29/512
3063/3063 - 295s - loss: 0.2688 - val_loss: 0.2699
Epoch 30/512
3063/3063 - 295s - loss: 0.2659 - val_loss: 0.2805
Epoch 31/512
3063/3063 - 296s - loss: 0.2640 - val_loss: 0.2724
Epoch 32/512
3063/3063 - 295s - loss: 0.2632 - val_loss: 0.2792
Epoch 33/512
3063/3063 - 296s - loss: 0.2608 - val_loss: 0.2708
Epoch 34/512
3063/3063 - 295s - loss: 0.2583 - val_loss: 0.2702
Epoch 35/512
3063/3063 - 295s - loss: 0.2565 - val_loss: 0.2693
Epoch 36/512
3063/3063 - 295s - loss: 0.2557 - val_loss: 0.2693
Epoch 37/512
3063/3063 - 295s - loss: 0.2538 - val_loss: 0.2636
Epoch 38/512
3063/3063 - 295s - loss: 0.2522 - val_loss: 0.2615
Epoch 39/512
3063/3063 - 295s - loss: 0.2488 - val_loss: 0.2708
Epoch 40/512
3063/3063 - 295s - loss: 0.2486 - val_loss: 0.2590
Epoch 41/512
3063/3063 - 295s - loss: 0.2475 - val_loss: 0.2540
Epoch 42/512
3063/3063 - 295s - loss: 0.2462 - val_loss: 0.2553
Epoch 43/512
3063/3063 - 295s - loss: 0.2437 - val_loss: 0.2510
Epoch 44/512
3063/3063 - 295s - loss: 0.2432 - val_loss: 0.2524
Epoch 45/512
3063/3063 - 295s - loss: 0.2424 - val_loss: 0.2526
Epoch 46/512
3063/3063 - 295s - loss: 0.2396 - val_loss: 0.2627
Epoch 47/512
3063/3063 - 295s - loss: 0.2392 - val_loss: 0.2592
Epoch 48/512
3063/3063 - 295s - loss: 0.2373 - val_loss: 0.2547
Epoch 49/512
3063/3063 - 295s - loss: 0.2367 - val_loss: 0.2712
Epoch 50/512
3063/3063 - 295s - loss: 0.2356 - val_loss: 0.2480
Epoch 51/512
3063/3063 - 295s - loss: 0.2347 - val_loss: 0.2619
Epoch 52/512
3063/3063 - 295s - loss: 0.2333 - val_loss: 0.2558
Epoch 53/512
3063/3063 - 295s - loss: 0.2317 - val_loss: 0.2571
Epoch 54/512
3063/3063 - 295s - loss: 0.2307 - val_loss: 0.2459
Epoch 55/512
3063/3063 - 295s - loss: 0.2306 - val_loss: 0.2458
Epoch 56/512
3063/3063 - 295s - loss: 0.2294 - val_loss: 0.2435
Epoch 57/512
3063/3063 - 295s - loss: 0.2274 - val_loss: 0.2605
Epoch 58/512
3063/3063 - 295s - loss: 0.2275 - val_loss: 0.2523
Epoch 59/512
3063/3063 - 295s - loss: 0.2264 - val_loss: 0.2612
Epoch 60/512
3063/3063 - 295s - loss: 0.2263 - val_loss: 0.2466
Epoch 61/512
3063/3063 - 295s - loss: 0.2254 - val_loss: 0.2473
Epoch 62/512
3063/3063 - 295s - loss: 0.2235 - val_loss: 0.2688
Epoch 63/512
3063/3063 - 295s - loss: 0.2222 - val_loss: 0.2543
Epoch 64/512
3063/3063 - 295s - loss: 0.2229 - val_loss: 0.2474
Epoch 65/512
3063/3063 - 295s - loss: 0.2212 - val_loss: 0.2496
Epoch 66/512
3063/3063 - 295s - loss: 0.2207 - val_loss: 0.2579
Epoch 67/512
3063/3063 - 295s - loss: 0.2192 - val_loss: 0.2433
Epoch 68/512
3063/3063 - 295s - loss: 0.2180 - val_loss: 0.2447
Epoch 69/512
3063/3063 - 295s - loss: 0.2176 - val_loss: 0.2407
Epoch 70/512
3063/3063 - 295s - loss: 0.2173 - val_loss: 0.2442
Epoch 71/512
3063/3063 - 295s - loss: 0.2162 - val_loss: 0.2461
Epoch 72/512
3063/3063 - 295s - loss: 0.2161 - val_loss: 0.2582
Epoch 73/512
3063/3063 - 295s - loss: 0.2159 - val_loss: 0.2413
Epoch 74/512
3063/3063 - 295s - loss: 0.2144 - val_loss: 0.2431
Epoch 75/512
3063/3063 - 295s - loss: 0.2138 - val_loss: 0.2433
Epoch 76/512
3063/3063 - 295s - loss: 0.2124 - val_loss: 0.2513
Epoch 77/512
3063/3063 - 295s - loss: 0.2118 - val_loss: 0.2399
Epoch 78/512
3063/3063 - 295s - loss: 0.2103 - val_loss: 0.2439
Epoch 79/512
3063/3063 - 295s - loss: 0.2103 - val_loss: 0.2513
Epoch 80/512
3063/3063 - 295s - loss: 0.2094 - val_loss: 0.2439
Epoch 81/512
3063/3063 - 295s - loss: 0.2096 - val_loss: 0.2437
Epoch 82/512
3063/3063 - 295s - loss: 0.2095 - val_loss: 0.2771
Epoch 83/512
3063/3063 - 295s - loss: 0.2074 - val_loss: 0.2535
Epoch 84/512
3063/3063 - 295s - loss: 0.2070 - val_loss: 0.2382
Epoch 85/512
3063/3063 - 295s - loss: 0.2058 - val_loss: 0.2488
Epoch 86/512
3063/3063 - 295s - loss: 0.2065 - val_loss: 0.2539
Epoch 87/512
3063/3063 - 295s - loss: 0.2048 - val_loss: 0.2408
Epoch 88/512
3063/3063 - 295s - loss: 0.2048 - val_loss: 0.2429
Epoch 89/512
3063/3063 - 295s - loss: 0.2026 - val_loss: 0.2458
Epoch 90/512
3063/3063 - 295s - loss: 0.2018 - val_loss: 0.2423
Epoch 91/512
3063/3063 - 295s - loss: 0.2018 - val_loss: 0.2667
Epoch 92/512
3063/3063 - 295s - loss: 0.2022 - val_loss: 0.2581
Epoch 93/512
3063/3063 - 295s - loss: 0.2015 - val_loss: 0.2647
Epoch 94/512
3063/3063 - 295s - loss: 0.2004 - val_loss: 0.2447
Epoch 95/512
3063/3063 - 295s - loss: 0.1998 - val_loss: 0.2594
Epoch 96/512
3063/3063 - 295s - loss: 0.1985 - val_loss: 0.2456
Epoch 97/512
3063/3063 - 295s - loss: 0.1978 - val_loss: 0.2638
Epoch 98/512
3063/3063 - 295s - loss: 0.1969 - val_loss: 0.2546
Epoch 99/512
3063/3063 - 295s - loss: 0.1973 - val_loss: 0.2500
Epoch 100/512
3063/3063 - 295s - loss: 0.1961 - val_loss: 0.2430
Epoch 101/512
3063/3063 - 295s - loss: 0.1953 - val_loss: 0.2471
Epoch 102/512
3063/3063 - 295s - loss: 0.1955 - val_loss: 0.2502
Epoch 103/512
3063/3063 - 295s - loss: 0.1953 - val_loss: 0.2416
Epoch 104/512
3063/3063 - 295s - loss: 0.1936 - val_loss: 0.2555
Epoch 105/512
3063/3063 - 295s - loss: 0.1934 - val_loss: 0.2551
Epoch 106/512
3063/3063 - 295s - loss: 0.1924 - val_loss: 0.2570
Epoch 107/512
3063/3063 - 295s - loss: 0.1918 - val_loss: 0.2452
Epoch 108/512
3063/3063 - 295s - loss: 0.1907 - val_loss: 0.2637
Epoch 109/512
3063/3063 - 295s - loss: 0.1905 - val_loss: 0.2432
Epoch 110/512
3063/3063 - 295s - loss: 0.1892 - val_loss: 0.2580
Epoch 111/512
3063/3063 - 295s - loss: 0.1889 - val_loss: 0.2503
Epoch 112/512
3063/3063 - 295s - loss: 0.1881 - val_loss: 0.2512
Epoch 113/512
3063/3063 - 295s - loss: 0.1880 - val_loss: 0.2586
Epoch 114/512
3063/3063 - 295s - loss: 0.1880 - val_loss: 0.2577
Epoch 115/512
3063/3063 - 295s - loss: 0.1866 - val_loss: 0.2605
Epoch 116/512
3063/3063 - 295s - loss: 0.1869 - val_loss: 0.2492
###
RIGHT NOW: pairwise_nl
tf.data.datset created for training data
Model not yet created, creating new model
[[ 64 128 256 128  64]]
Epoch 1/512
3063/3063 - 39s - loss: 0.4024 - val_loss: 0.3681
Epoch 2/512
3063/3063 - 37s - loss: 0.3647 - val_loss: 0.3752
Epoch 3/512
3063/3063 - 37s - loss: 0.3570 - val_loss: 0.3523
Epoch 4/512
3063/3063 - 37s - loss: 0.3511 - val_loss: 0.3475
Epoch 5/512
3063/3063 - 38s - loss: 0.3477 - val_loss: 0.3437
Epoch 6/512
3063/3063 - 37s - loss: 0.3444 - val_loss: 0.3392
Epoch 7/512
3063/3063 - 38s - loss: 0.3417 - val_loss: 0.3513
Epoch 8/512
3063/3063 - 37s - loss: 0.3375 - val_loss: 0.3334
Epoch 9/512
3063/3063 - 38s - loss: 0.3348 - val_loss: 0.3349
Epoch 10/512
3063/3063 - 37s - loss: 0.3280 - val_loss: 0.3437
Epoch 11/512
3063/3063 - 37s - loss: 0.3210 - val_loss: 0.3149
Epoch 12/512
3063/3063 - 37s - loss: 0.3144 - val_loss: 0.3059
Epoch 13/512
3063/3063 - 37s - loss: 0.3088 - val_loss: 0.3017
Epoch 14/512
3063/3063 - 36s - loss: 0.3023 - val_loss: 0.3093
Epoch 15/512
3063/3063 - 37s - loss: 0.2965 - val_loss: 0.3008
Epoch 16/512
3063/3063 - 37s - loss: 0.2916 - val_loss: 0.2827
Epoch 17/512
3063/3063 - 37s - loss: 0.2865 - val_loss: 0.2846
Epoch 18/512
3063/3063 - 37s - loss: 0.2820 - val_loss: 0.3008
Epoch 19/512
3063/3063 - 37s - loss: 0.2778 - val_loss: 0.2970
Epoch 20/512
3063/3063 - 36s - loss: 0.2737 - val_loss: 0.2769
Epoch 21/512
3063/3063 - 37s - loss: 0.2711 - val_loss: 0.2742
Epoch 22/512
3063/3063 - 37s - loss: 0.2680 - val_loss: 0.3271
Epoch 23/512
3063/3063 - 37s - loss: 0.2652 - val_loss: 0.2660
Epoch 24/512
3063/3063 - 37s - loss: 0.2619 - val_loss: 0.2741
Epoch 25/512
3063/3063 - 37s - loss: 0.2602 - val_loss: 0.2645
Epoch 26/512
3063/3063 - 37s - loss: 0.2583 - val_loss: 0.2600
Epoch 27/512
3063/3063 - 37s - loss: 0.2549 - val_loss: 0.2667
Epoch 28/512
3063/3063 - 37s - loss: 0.2531 - val_loss: 0.2543
Epoch 29/512
3063/3063 - 37s - loss: 0.2499 - val_loss: 0.2507
Epoch 30/512
3063/3063 - 36s - loss: 0.2490 - val_loss: 0.2544
Epoch 31/512
3063/3063 - 37s - loss: 0.2467 - val_loss: 0.2564
Epoch 32/512
3063/3063 - 36s - loss: 0.2445 - val_loss: 0.2530
Epoch 33/512
3063/3063 - 36s - loss: 0.2430 - val_loss: 0.2475
Epoch 34/512
3063/3063 - 37s - loss: 0.2399 - val_loss: 0.2522
Epoch 35/512
3063/3063 - 37s - loss: 0.2386 - val_loss: 0.2612
Epoch 36/512
3063/3063 - 36s - loss: 0.2373 - val_loss: 0.2514
Epoch 37/512
3063/3063 - 36s - loss: 0.2346 - val_loss: 0.2453
Epoch 38/512
3063/3063 - 36s - loss: 0.2336 - val_loss: 0.2589
Epoch 39/512
3063/3063 - 36s - loss: 0.2321 - val_loss: 0.2479
Epoch 40/512
3063/3063 - 36s - loss: 0.2305 - val_loss: 0.2447
Epoch 41/512
3063/3063 - 37s - loss: 0.2301 - val_loss: 0.2482
Epoch 42/512
3063/3063 - 37s - loss: 0.2294 - val_loss: 0.2471
Epoch 43/512
3063/3063 - 37s - loss: 0.2271 - val_loss: 0.2375
Epoch 44/512
3063/3063 - 37s - loss: 0.2254 - val_loss: 0.2358
Epoch 45/512
3063/3063 - 37s - loss: 0.2249 - val_loss: 0.2393
Epoch 46/512
3063/3063 - 37s - loss: 0.2230 - val_loss: 0.2637
Epoch 47/512
3063/3063 - 37s - loss: 0.2214 - val_loss: 0.2770
Epoch 48/512
3063/3063 - 37s - loss: 0.2217 - val_loss: 0.2367
Epoch 49/512
3063/3063 - 37s - loss: 0.2198 - val_loss: 0.2692
Epoch 50/512
3063/3063 - 37s - loss: 0.2194 - val_loss: 0.2426
Epoch 51/512
3063/3063 - 37s - loss: 0.2179 - val_loss: 0.2699
Epoch 52/512
3063/3063 - 37s - loss: 0.2173 - val_loss: 0.2502
Epoch 53/512
3063/3063 - 37s - loss: 0.2166 - val_loss: 0.2488
Epoch 54/512
3063/3063 - 37s - loss: 0.2165 - val_loss: 0.2369
Epoch 55/512
3063/3063 - 37s - loss: 0.2144 - val_loss: 0.2378
Epoch 56/512
3063/3063 - 37s - loss: 0.2138 - val_loss: 0.2350
Epoch 57/512
3063/3063 - 37s - loss: 0.2126 - val_loss: 0.2377
Epoch 58/512
3063/3063 - 37s - loss: 0.2118 - val_loss: 0.2412
Epoch 59/512
3063/3063 - 37s - loss: 0.2101 - val_loss: 0.2452
Epoch 60/512
3063/3063 - 37s - loss: 0.2097 - val_loss: 0.2413
Epoch 61/512
3063/3063 - 37s - loss: 0.2103 - val_loss: 0.2602
Epoch 62/512
3063/3063 - 38s - loss: 0.2098 - val_loss: 0.2385
Epoch 63/512
3063/3063 - 37s - loss: 0.2069 - val_loss: 0.2562
Epoch 64/512
3063/3063 - 37s - loss: 0.2068 - val_loss: 0.2440
Epoch 65/512
3063/3063 - 37s - loss: 0.2064 - val_loss: 0.2631
Epoch 66/512
3063/3063 - 37s - loss: 0.2060 - val_loss: 0.2372
Epoch 67/512
3063/3063 - 38s - loss: 0.2046 - val_loss: 0.2402
Epoch 68/512
3063/3063 - 37s - loss: 0.2041 - val_loss: 0.2387
Epoch 69/512
3063/3063 - 38s - loss: 0.2028 - val_loss: 0.2349
Epoch 70/512
3063/3063 - 37s - loss: 0.2017 - val_loss: 0.2407
Epoch 71/512
3063/3063 - 37s - loss: 0.2005 - val_loss: 0.2401
Epoch 72/512
3063/3063 - 37s - loss: 0.2007 - val_loss: 0.2374
Epoch 73/512
3063/3063 - 37s - loss: 0.1985 - val_loss: 0.2398
Epoch 74/512
3063/3063 - 37s - loss: 0.1976 - val_loss: 0.2368
Epoch 75/512
3063/3063 - 37s - loss: 0.1976 - val_loss: 0.2419
Epoch 76/512
3063/3063 - 37s - loss: 0.1973 - val_loss: 0.2436
Epoch 77/512
3063/3063 - 38s - loss: 0.1963 - val_loss: 0.2379
Epoch 78/512
3063/3063 - 37s - loss: 0.1952 - val_loss: 0.2376
Epoch 79/512
3063/3063 - 37s - loss: 0.1953 - val_loss: 0.2442
Epoch 80/512
3063/3063 - 37s - loss: 0.1959 - val_loss: 0.2568
Epoch 81/512
3063/3063 - 37s - loss: 0.1948 - val_loss: 0.2362
Epoch 82/512
3063/3063 - 36s - loss: 0.1926 - val_loss: 0.2599
Epoch 83/512
3063/3063 - 36s - loss: 0.1916 - val_loss: 0.2505
Epoch 84/512
3063/3063 - 36s - loss: 0.1906 - val_loss: 0.2336
Epoch 85/512
3063/3063 - 37s - loss: 0.1896 - val_loss: 0.2351
Epoch 86/512
3063/3063 - 37s - loss: 0.1907 - val_loss: 0.2539
Epoch 87/512
3063/3063 - 37s - loss: 0.1890 - val_loss: 0.2429
Epoch 88/512
3063/3063 - 37s - loss: 0.1899 - val_loss: 0.2480
Epoch 89/512
3063/3063 - 37s - loss: 0.1859 - val_loss: 0.2396
Epoch 90/512
3063/3063 - 37s - loss: 0.1854 - val_loss: 0.2526
Epoch 91/512
3063/3063 - 37s - loss: 0.1862 - val_loss: 0.2444
Epoch 92/512
3063/3063 - 37s - loss: 0.1833 - val_loss: 0.2584
Epoch 93/512
3063/3063 - 37s - loss: 0.1841 - val_loss: 0.2633
Epoch 94/512
3063/3063 - 37s - loss: 0.1848 - val_loss: 0.2504
Epoch 95/512
3063/3063 - 38s - loss: 0.1831 - val_loss: 0.2424
Epoch 96/512
3063/3063 - 37s - loss: 0.1801 - val_loss: 0.2417
Epoch 97/512
3063/3063 - 37s - loss: 0.1820 - val_loss: 0.2626
Epoch 98/512
3063/3063 - 37s - loss: 0.1803 - val_loss: 0.2533
Epoch 99/512
3063/3063 - 37s - loss: 0.1790 - val_loss: 0.2411
Epoch 100/512
3063/3063 - 41s - loss: 0.1792 - val_loss: 0.2424
Epoch 101/512
3063/3063 - 37s - loss: 0.1789 - val_loss: 0.2579
Epoch 102/512
3063/3063 - 37s - loss: 0.1775 - val_loss: 0.2636
Epoch 103/512
3063/3063 - 38s - loss: 0.1751 - val_loss: 0.2504
Epoch 104/512
3063/3063 - 37s - loss: 0.1749 - val_loss: 0.2477
Epoch 105/512
3063/3063 - 37s - loss: 0.1752 - val_loss: 0.2560
Epoch 106/512
3063/3063 - 37s - loss: 0.1746 - val_loss: 0.2552
Epoch 107/512
3063/3063 - 37s - loss: 0.1727 - val_loss: 0.2466
Epoch 108/512
3063/3063 - 36s - loss: 0.1731 - val_loss: 0.2605
Epoch 109/512
3063/3063 - 38s - loss: 0.1706 - val_loss: 0.2497
Epoch 110/512
3063/3063 - 37s - loss: 0.1705 - val_loss: 0.2695
Epoch 111/512
3063/3063 - 37s - loss: 0.1702 - val_loss: 0.2605
Epoch 112/512
3063/3063 - 37s - loss: 0.1707 - val_loss: 0.2593
Epoch 113/512
3063/3063 - 37s - loss: 0.1698 - val_loss: 0.2621
Epoch 114/512
3063/3063 - 37s - loss: 0.1687 - val_loss: 0.2601
Epoch 115/512
3063/3063 - 37s - loss: 0.1667 - val_loss: 0.2549
Epoch 116/512
3063/3063 - 37s - loss: 0.1659 - val_loss: 0.2618
###
RIGHT NOW: pairwise_nl_iter
tf.data.datset created for training data
Model not yet created, creating new model
((64, 64, 116, 64, 64), (64, 64, 116, 64, 64), (64, 64, 116, 64, 64))
Epoch 1/512
3063/3063 - 79s - loss: 0.4146 - val_loss: 0.3824
Epoch 2/512
3063/3063 - 73s - loss: 0.3705 - val_loss: 0.3917
Epoch 3/512
3063/3063 - 73s - loss: 0.3616 - val_loss: 0.3496
Epoch 4/512
3063/3063 - 73s - loss: 0.3566 - val_loss: 0.3465
Epoch 5/512
3063/3063 - 72s - loss: 0.3528 - val_loss: 0.3412
Epoch 6/512
3063/3063 - 73s - loss: 0.3490 - val_loss: 0.3406
Epoch 7/512
3063/3063 - 73s - loss: 0.3448 - val_loss: 0.3489
Epoch 8/512
3063/3063 - 75s - loss: 0.3405 - val_loss: 0.3364
Epoch 9/512
3063/3063 - 75s - loss: 0.3367 - val_loss: 0.3415
Epoch 10/512
3063/3063 - 74s - loss: 0.3304 - val_loss: 0.3288
Epoch 11/512
3063/3063 - 75s - loss: 0.3243 - val_loss: 0.3207
Epoch 12/512
3063/3063 - 71s - loss: 0.3190 - val_loss: 0.3174
Epoch 13/512
3063/3063 - 73s - loss: 0.3149 - val_loss: 0.3096
Epoch 14/512
3063/3063 - 73s - loss: 0.3104 - val_loss: 0.3070
Epoch 15/512
3063/3063 - 73s - loss: 0.3073 - val_loss: 0.3091
Epoch 16/512
3063/3063 - 74s - loss: 0.3019 - val_loss: 0.2931
Epoch 17/512
3063/3063 - 74s - loss: 0.2988 - val_loss: 0.3109
Epoch 18/512
3063/3063 - 73s - loss: 0.2917 - val_loss: 0.3028
Epoch 19/512
3063/3063 - 73s - loss: 0.2870 - val_loss: 0.3105
Epoch 20/512
3063/3063 - 71s - loss: 0.2834 - val_loss: 0.2893
Epoch 21/512
3063/3063 - 72s - loss: 0.2819 - val_loss: 0.2871
Epoch 22/512
3063/3063 - 74s - loss: 0.2786 - val_loss: 0.3157
Epoch 23/512
3063/3063 - 75s - loss: 0.2760 - val_loss: 0.2757
Epoch 24/512
3063/3063 - 75s - loss: 0.2722 - val_loss: 0.2715
Epoch 25/512
3063/3063 - 75s - loss: 0.2724 - val_loss: 0.2730
Epoch 26/512
3063/3063 - 72s - loss: 0.2692 - val_loss: 0.2716
Epoch 27/512
3063/3063 - 71s - loss: 0.2675 - val_loss: 0.2711
Epoch 28/512
3063/3063 - 73s - loss: 0.2656 - val_loss: 0.2643
Epoch 29/512
3063/3063 - 73s - loss: 0.2633 - val_loss: 0.2615
Epoch 30/512
3063/3063 - 72s - loss: 0.2618 - val_loss: 0.2724
Epoch 31/512
3063/3063 - 70s - loss: 0.2602 - val_loss: 0.2691
Epoch 32/512
3063/3063 - 70s - loss: 0.2597 - val_loss: 0.2696
Epoch 33/512
3063/3063 - 73s - loss: 0.2572 - val_loss: 0.2677
Epoch 34/512
3063/3063 - 76s - loss: 0.2553 - val_loss: 0.2669
Epoch 35/512
3063/3063 - 75s - loss: 0.2553 - val_loss: 0.2716
Epoch 36/512
3063/3063 - 75s - loss: 0.2525 - val_loss: 0.2805
Epoch 37/512
3063/3063 - 73s - loss: 0.2515 - val_loss: 0.2539
Epoch 38/512
3063/3063 - 72s - loss: 0.2504 - val_loss: 0.2668
Epoch 39/512
3063/3063 - 72s - loss: 0.2476 - val_loss: 0.2848
Epoch 40/512
3063/3063 - 74s - loss: 0.2471 - val_loss: 0.2719
Epoch 41/512
3063/3063 - 74s - loss: 0.2468 - val_loss: 0.2530
Epoch 42/512
3063/3063 - 73s - loss: 0.2463 - val_loss: 0.2543
Epoch 43/512
3063/3063 - 74s - loss: 0.2457 - val_loss: 0.2511
Epoch 44/512
3063/3063 - 74s - loss: 0.2439 - val_loss: 0.2514
Epoch 45/512
3063/3063 - 74s - loss: 0.2424 - val_loss: 0.2635
Epoch 46/512
3063/3063 - 73s - loss: 0.2413 - val_loss: 0.2618
Epoch 47/512
3063/3063 - 75s - loss: 0.2385 - val_loss: 0.2704
Epoch 48/512
3063/3063 - 74s - loss: 0.2389 - val_loss: 0.2523
Epoch 49/512
3063/3063 - 75s - loss: 0.2374 - val_loss: 0.2651
Epoch 50/512
3063/3063 - 74s - loss: 0.2362 - val_loss: 0.2529
Epoch 51/512
3063/3063 - 73s - loss: 0.2367 - val_loss: 0.2697
Epoch 52/512
3063/3063 - 73s - loss: 0.2332 - val_loss: 0.2621
Epoch 53/512
3063/3063 - 73s - loss: 0.2333 - val_loss: 0.2534
Epoch 54/512
3063/3063 - 75s - loss: 0.2321 - val_loss: 0.2509
Epoch 55/512
3063/3063 - 75s - loss: 0.2329 - val_loss: 0.2477
Epoch 56/512
3063/3063 - 73s - loss: 0.2307 - val_loss: 0.2444
Epoch 57/512
3063/3063 - 75s - loss: 0.2299 - val_loss: 0.2595
Epoch 58/512
3063/3063 - 73s - loss: 0.2292 - val_loss: 0.2587
Epoch 59/512
3063/3063 - 74s - loss: 0.2293 - val_loss: 0.2545
Epoch 60/512
3063/3063 - 75s - loss: 0.2273 - val_loss: 0.2471
Epoch 61/512
3063/3063 - 75s - loss: 0.2269 - val_loss: 0.2602
Epoch 62/512
3063/3063 - 75s - loss: 0.2255 - val_loss: 0.2610
Epoch 63/512
3063/3063 - 75s - loss: 0.2246 - val_loss: 0.2645
Epoch 64/512
3063/3063 - 81s - loss: 0.2241 - val_loss: 0.2427
Epoch 65/512
3063/3063 - 73s - loss: 0.2237 - val_loss: 0.2911
Epoch 66/512
3063/3063 - 74s - loss: 0.2237 - val_loss: 0.2459
Epoch 67/512
3063/3063 - 74s - loss: 0.2216 - val_loss: 0.2450
Epoch 68/512
3063/3063 - 75s - loss: 0.2216 - val_loss: 0.2410
Epoch 69/512
3063/3063 - 75s - loss: 0.2216 - val_loss: 0.2387
Epoch 70/512
3063/3063 - 75s - loss: 0.2195 - val_loss: 0.2401
Epoch 71/512
3063/3063 - 76s - loss: 0.2194 - val_loss: 0.2396
Epoch 72/512
3063/3063 - 75s - loss: 0.2193 - val_loss: 0.2621
Epoch 73/512
3063/3063 - 74s - loss: 0.2171 - val_loss: 0.2457
Epoch 74/512
3063/3063 - 75s - loss: 0.2168 - val_loss: 0.2444
Epoch 75/512
3063/3063 - 72s - loss: 0.2160 - val_loss: 0.2423
Epoch 76/512
3063/3063 - 74s - loss: 0.2151 - val_loss: 0.2407
Epoch 77/512
3063/3063 - 74s - loss: 0.2150 - val_loss: 0.2388
Epoch 78/512
3063/3063 - 74s - loss: 0.2140 - val_loss: 0.2397
Epoch 79/512
3063/3063 - 75s - loss: 0.2127 - val_loss: 0.2540
Epoch 80/512
3063/3063 - 74s - loss: 0.2126 - val_loss: 0.2461
Epoch 81/512
3063/3063 - 73s - loss: 0.2134 - val_loss: 0.2374
Epoch 82/512
3063/3063 - 75s - loss: 0.2120 - val_loss: 0.2901
Epoch 83/512
3063/3063 - 74s - loss: 0.2113 - val_loss: 0.2713
Epoch 84/512
3063/3063 - 74s - loss: 0.2115 - val_loss: 0.2420
Epoch 85/512
3063/3063 - 74s - loss: 0.2103 - val_loss: 0.2421
Epoch 86/512
3063/3063 - 74s - loss: 0.2100 - val_loss: 0.2457
Epoch 87/512
3063/3063 - 74s - loss: 0.2084 - val_loss: 0.2440
Epoch 88/512
3063/3063 - 71s - loss: 0.2074 - val_loss: 0.2435
Epoch 89/512
3063/3063 - 74s - loss: 0.2070 - val_loss: 0.2467
Epoch 90/512
3063/3063 - 74s - loss: 0.2078 - val_loss: 0.2555
Epoch 91/512
3063/3063 - 75s - loss: 0.2056 - val_loss: 0.2485
Epoch 92/512
3063/3063 - 75s - loss: 0.2043 - val_loss: 0.2524
Epoch 93/512
3063/3063 - 74s - loss: 0.2048 - val_loss: 0.2660
Epoch 94/512
3063/3063 - 75s - loss: 0.2051 - val_loss: 0.2458
Epoch 95/512
3063/3063 - 74s - loss: 0.2043 - val_loss: 0.2400
Epoch 96/512
3063/3063 - 74s - loss: 0.2029 - val_loss: 0.2477
Epoch 97/512
3063/3063 - 75s - loss: 0.2016 - val_loss: 0.2517
Epoch 98/512
3063/3063 - 72s - loss: 0.2016 - val_loss: 0.2582
Epoch 99/512
3063/3063 - 75s - loss: 0.2022 - val_loss: 0.2465
Epoch 100/512
3063/3063 - 75s - loss: 0.1999 - val_loss: 0.2409
Epoch 101/512
3063/3063 - 74s - loss: 0.2000 - val_loss: 0.2439
Epoch 102/512
3063/3063 - 75s - loss: 0.2002 - val_loss: 0.2481
Epoch 103/512
3063/3063 - 75s - loss: 0.1996 - val_loss: 0.2420
Epoch 104/512
3063/3063 - 74s - loss: 0.1977 - val_loss: 0.2450
Epoch 105/512
3063/3063 - 75s - loss: 0.1983 - val_loss: 0.2585
Epoch 106/512
3063/3063 - 74s - loss: 0.1970 - val_loss: 0.2422
Epoch 107/512
3063/3063 - 75s - loss: 0.1962 - val_loss: 0.2457
Epoch 108/512
3063/3063 - 74s - loss: 0.1950 - val_loss: 0.2497
Epoch 109/512
3063/3063 - 75s - loss: 0.1958 - val_loss: 0.2415
Epoch 110/512
3063/3063 - 75s - loss: 0.1951 - val_loss: 0.2552
Epoch 111/512
3063/3063 - 76s - loss: 0.1949 - val_loss: 0.2517
Epoch 112/512
3063/3063 - 73s - loss: 0.1927 - val_loss: 0.2484
Epoch 113/512
3063/3063 - 72s - loss: 0.1922 - val_loss: 0.2436
###
RIGHT NOW: nested_concat
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/512
3063/3063 - 29s - loss: 0.4061 - val_loss: 0.3755
Epoch 2/512
3063/3063 - 25s - loss: 0.3627 - val_loss: 0.3514
Epoch 3/512
3063/3063 - 25s - loss: 0.3545 - val_loss: 0.3487
Epoch 4/512
3063/3063 - 25s - loss: 0.3484 - val_loss: 0.3520
Epoch 5/512
3063/3063 - 25s - loss: 0.3450 - val_loss: 0.3380
Epoch 6/512
3063/3063 - 26s - loss: 0.3415 - val_loss: 0.3372
Epoch 7/512
3063/3063 - 26s - loss: 0.3381 - val_loss: 0.3442
Epoch 8/512
3063/3063 - 27s - loss: 0.3361 - val_loss: 0.3358
Epoch 9/512
3063/3063 - 26s - loss: 0.3343 - val_loss: 0.3359
Epoch 10/512
3063/3063 - 26s - loss: 0.3304 - val_loss: 0.3365
Epoch 11/512
3063/3063 - 26s - loss: 0.3277 - val_loss: 0.3455
Epoch 12/512
3063/3063 - 25s - loss: 0.3246 - val_loss: 0.3314
Epoch 13/512
3063/3063 - 27s - loss: 0.3237 - val_loss: 0.3200
Epoch 14/512
3063/3063 - 26s - loss: 0.3216 - val_loss: 0.3223
Epoch 15/512
3063/3063 - 26s - loss: 0.3182 - val_loss: 0.3357
Epoch 16/512
3063/3063 - 26s - loss: 0.3180 - val_loss: 0.3291
Epoch 17/512
3063/3063 - 26s - loss: 0.3159 - val_loss: 0.3169
Epoch 18/512
3063/3063 - 26s - loss: 0.3136 - val_loss: 0.3295
Epoch 19/512
3063/3063 - 26s - loss: 0.3127 - val_loss: 0.3191
Epoch 20/512
3063/3063 - 26s - loss: 0.3117 - val_loss: 0.3218
Epoch 21/512
3063/3063 - 26s - loss: 0.3098 - val_loss: 0.3149
Epoch 22/512
3063/3063 - 25s - loss: 0.3087 - val_loss: 0.3352
Epoch 23/512
3063/3063 - 26s - loss: 0.3070 - val_loss: 0.3175
Epoch 24/512
3063/3063 - 26s - loss: 0.3053 - val_loss: 0.3237
Epoch 25/512
3063/3063 - 26s - loss: 0.3049 - val_loss: 0.3137
Epoch 26/512
3063/3063 - 25s - loss: 0.3037 - val_loss: 0.3108
Epoch 27/512
3063/3063 - 25s - loss: 0.3019 - val_loss: 0.3110
Epoch 28/512
3063/3063 - 25s - loss: 0.3006 - val_loss: 0.3072
Epoch 29/512
3063/3063 - 25s - loss: 0.2982 - val_loss: 0.3100
Epoch 30/512
3063/3063 - 25s - loss: 0.2952 - val_loss: 0.3113
Epoch 31/512
3063/3063 - 25s - loss: 0.2949 - val_loss: 0.3070
Epoch 32/512
3063/3063 - 25s - loss: 0.2928 - val_loss: 0.3120
Epoch 33/512
3063/3063 - 25s - loss: 0.2915 - val_loss: 0.2946
Epoch 34/512
3063/3063 - 24s - loss: 0.2896 - val_loss: 0.3003
Epoch 35/512
3063/3063 - 24s - loss: 0.2878 - val_loss: 0.2942
Epoch 36/512
3063/3063 - 25s - loss: 0.2862 - val_loss: 0.2936
Epoch 37/512
3063/3063 - 24s - loss: 0.2855 - val_loss: 0.2946
Epoch 38/512
3063/3063 - 23s - loss: 0.2836 - val_loss: 0.2921
Epoch 39/512
3063/3063 - 23s - loss: 0.2817 - val_loss: 0.2972
Epoch 40/512
3063/3063 - 23s - loss: 0.2805 - val_loss: 0.2947
Epoch 41/512
3063/3063 - 25s - loss: 0.2799 - val_loss: 0.2952
Epoch 42/512
3063/3063 - 23s - loss: 0.2784 - val_loss: 0.2952
Epoch 43/512
3063/3063 - 22s - loss: 0.2771 - val_loss: 0.2848
Epoch 44/512
3063/3063 - 24s - loss: 0.2763 - val_loss: 0.2929
Epoch 45/512
3063/3063 - 26s - loss: 0.2748 - val_loss: 0.2964
Epoch 46/512
3063/3063 - 26s - loss: 0.2739 - val_loss: 0.2920
Epoch 47/512
3063/3063 - 26s - loss: 0.2723 - val_loss: 0.2928
Epoch 48/512
3063/3063 - 24s - loss: 0.2721 - val_loss: 0.3014
Epoch 49/512
3063/3063 - 24s - loss: 0.2690 - val_loss: 0.2858
Epoch 50/512
3063/3063 - 25s - loss: 0.2678 - val_loss: 0.2876
Epoch 51/512
3063/3063 - 25s - loss: 0.2675 - val_loss: 0.2978
Epoch 52/512
3063/3063 - 25s - loss: 0.2672 - val_loss: 0.2895
Epoch 53/512
3063/3063 - 25s - loss: 0.2646 - val_loss: 0.2826
Epoch 54/512
3063/3063 - 26s - loss: 0.2644 - val_loss: 0.2843
Epoch 55/512
3063/3063 - 26s - loss: 0.2621 - val_loss: 0.2828
Epoch 56/512
3063/3063 - 26s - loss: 0.2606 - val_loss: 0.2814
Epoch 57/512
3063/3063 - 27s - loss: 0.2591 - val_loss: 0.2954
Epoch 58/512
3063/3063 - 26s - loss: 0.2593 - val_loss: 0.2791
Epoch 59/512
3063/3063 - 25s - loss: 0.2570 - val_loss: 0.2794
Epoch 60/512
3063/3063 - 26s - loss: 0.2568 - val_loss: 0.2823
Epoch 61/512
3063/3063 - 25s - loss: 0.2555 - val_loss: 0.2822
Epoch 62/512
3063/3063 - 26s - loss: 0.2532 - val_loss: 0.2933
Epoch 63/512
3063/3063 - 26s - loss: 0.2532 - val_loss: 0.2959
Epoch 64/512
3063/3063 - 26s - loss: 0.2510 - val_loss: 0.2772
Epoch 65/512
3063/3063 - 26s - loss: 0.2497 - val_loss: 0.2837
Epoch 66/512
3063/3063 - 25s - loss: 0.2499 - val_loss: 0.2904
Epoch 67/512
3063/3063 - 25s - loss: 0.2488 - val_loss: 0.2800
Epoch 68/512
3063/3063 - 26s - loss: 0.2490 - val_loss: 0.2778
Epoch 69/512
3063/3063 - 26s - loss: 0.2467 - val_loss: 0.2794
Epoch 70/512
3063/3063 - 27s - loss: 0.2443 - val_loss: 0.2793
Epoch 71/512
3063/3063 - 26s - loss: 0.2443 - val_loss: 0.2775
Epoch 72/512
3063/3063 - 25s - loss: 0.2448 - val_loss: 0.2879
Epoch 73/512
3063/3063 - 25s - loss: 0.2421 - val_loss: 0.2826
Epoch 74/512
3063/3063 - 25s - loss: 0.2417 - val_loss: 0.2684
Epoch 75/512
3063/3063 - 25s - loss: 0.2429 - val_loss: 0.2799
Epoch 76/512
3063/3063 - 25s - loss: 0.2414 - val_loss: 0.2803
Epoch 77/512
3063/3063 - 25s - loss: 0.2399 - val_loss: 0.2705
Epoch 78/512
3063/3063 - 25s - loss: 0.2388 - val_loss: 0.2824
Epoch 79/512
3063/3063 - 27s - loss: 0.2376 - val_loss: 0.2711
Epoch 80/512
3063/3063 - 26s - loss: 0.2375 - val_loss: 0.2767
Epoch 81/512
3063/3063 - 26s - loss: 0.2362 - val_loss: 0.2728
Epoch 82/512
3063/3063 - 25s - loss: 0.2378 - val_loss: 0.2729
Epoch 83/512
3063/3063 - 25s - loss: 0.2349 - val_loss: 0.2866
Epoch 84/512
3063/3063 - 24s - loss: 0.2352 - val_loss: 0.2760
Epoch 85/512
3063/3063 - 25s - loss: 0.2334 - val_loss: 0.2770
Epoch 86/512
3063/3063 - 21s - loss: 0.2353 - val_loss: 0.2719
Epoch 87/512
3063/3063 - 21s - loss: 0.2323 - val_loss: 0.2786
Epoch 88/512
3063/3063 - 23s - loss: 0.2338 - val_loss: 0.2738
Epoch 89/512
3063/3063 - 23s - loss: 0.2304 - val_loss: 0.2695
Epoch 90/512
3063/3063 - 22s - loss: 0.2325 - val_loss: 0.2782
Epoch 91/512
3063/3063 - 21s - loss: 0.2301 - val_loss: 0.2744
Epoch 92/512
3063/3063 - 21s - loss: 0.2287 - val_loss: 0.2754
Epoch 93/512
3063/3063 - 22s - loss: 0.2281 - val_loss: 0.2834
Epoch 94/512
3063/3063 - 21s - loss: 0.2273 - val_loss: 0.2748
Epoch 95/512
3063/3063 - 21s - loss: 0.2273 - val_loss: 0.2821
Epoch 96/512
3063/3063 - 21s - loss: 0.2272 - val_loss: 0.2781
Epoch 97/512
3063/3063 - 21s - loss: 0.2273 - val_loss: 0.2802
Epoch 98/512
3063/3063 - 21s - loss: 0.2264 - val_loss: 0.2752
Epoch 99/512
3063/3063 - 21s - loss: 0.2256 - val_loss: 0.2715
Epoch 100/512
3063/3063 - 21s - loss: 0.2242 - val_loss: 0.2735
Epoch 101/512
3063/3063 - 21s - loss: 0.2260 - val_loss: 0.2751
Epoch 102/512
3063/3063 - 21s - loss: 0.2243 - val_loss: 0.2868
Epoch 103/512
3063/3063 - 20s - loss: 0.2241 - val_loss: 0.2754
Epoch 104/512
3063/3063 - 20s - loss: 0.2218 - val_loss: 0.2741
Epoch 105/512
3063/3063 - 21s - loss: 0.2223 - val_loss: 0.2922
Epoch 106/512
3063/3063 - 20s - loss: 0.2201 - val_loss: 0.2745
###
RIGHT NOW: naivednn
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/512
3063/3063 - 11s - loss: 0.4171 - val_loss: 0.3897
Epoch 2/512
3063/3063 - 10s - loss: 0.3865 - val_loss: 0.3765
Epoch 3/512
3063/3063 - 10s - loss: 0.3769 - val_loss: 0.3696
Epoch 4/512
3063/3063 - 10s - loss: 0.3697 - val_loss: 0.3710
Epoch 5/512
3063/3063 - 9s - loss: 0.3658 - val_loss: 0.3710
Epoch 6/512
3063/3063 - 10s - loss: 0.3631 - val_loss: 0.3778
Epoch 7/512
3063/3063 - 10s - loss: 0.3577 - val_loss: 0.3703
Epoch 8/512
3063/3063 - 9s - loss: 0.3552 - val_loss: 0.3677
Epoch 9/512
3063/3063 - 9s - loss: 0.3517 - val_loss: 0.3660
Epoch 10/512
3063/3063 - 9s - loss: 0.3489 - val_loss: 0.3811
Epoch 11/512
3063/3063 - 10s - loss: 0.3469 - val_loss: 0.3852
Epoch 12/512
3063/3063 - 9s - loss: 0.3442 - val_loss: 0.3717
Epoch 13/512
3063/3063 - 9s - loss: 0.3413 - val_loss: 0.3736
Epoch 14/512
3063/3063 - 9s - loss: 0.3397 - val_loss: 0.3747
Epoch 15/512
3063/3063 - 9s - loss: 0.3354 - val_loss: 0.3718
Epoch 16/512
3063/3063 - 9s - loss: 0.3349 - val_loss: 0.3733
Epoch 17/512
3063/3063 - 9s - loss: 0.3318 - val_loss: 0.3693
Epoch 18/512
3063/3063 - 9s - loss: 0.3285 - val_loss: 0.3858
Epoch 19/512
3063/3063 - 9s - loss: 0.3268 - val_loss: 0.3820
Epoch 20/512
3063/3063 - 9s - loss: 0.3242 - val_loss: 0.3755
Epoch 21/512
3063/3063 - 9s - loss: 0.3228 - val_loss: 0.3862
Epoch 22/512
3063/3063 - 9s - loss: 0.3188 - val_loss: 0.3860
Epoch 23/512
3063/3063 - 9s - loss: 0.3160 - val_loss: 0.3803
Epoch 24/512
3063/3063 - 9s - loss: 0.3161 - val_loss: 0.3901
Epoch 25/512
3063/3063 - 9s - loss: 0.3132 - val_loss: 0.3852
Epoch 26/512
3063/3063 - 9s - loss: 0.3102 - val_loss: 0.3923
Epoch 27/512
3063/3063 - 10s - loss: 0.3080 - val_loss: 0.3869
Epoch 28/512
3063/3063 - 9s - loss: 0.3057 - val_loss: 0.4013
Epoch 29/512
3063/3063 - 10s - loss: 0.3047 - val_loss: 0.4008
Epoch 30/512
3063/3063 - 9s - loss: 0.3006 - val_loss: 0.4106
Epoch 31/512
3063/3063 - 10s - loss: 0.3000 - val_loss: 0.4110
Epoch 32/512
3063/3063 - 9s - loss: 0.2971 - val_loss: 0.4075
Epoch 33/512
3063/3063 - 9s - loss: 0.2952 - val_loss: 0.3941
Epoch 34/512
3063/3063 - 10s - loss: 0.2929 - val_loss: 0.4327
Epoch 35/512
3063/3063 - 10s - loss: 0.2908 - val_loss: 0.4069
Epoch 36/512
3063/3063 - 9s - loss: 0.2882 - val_loss: 0.4024
Epoch 37/512
3063/3063 - 9s - loss: 0.2870 - val_loss: 0.4211
Epoch 38/512
3063/3063 - 9s - loss: 0.2835 - val_loss: 0.4094
Epoch 39/512
3063/3063 - 9s - loss: 0.2812 - val_loss: 0.4154
Epoch 40/512
3063/3063 - 9s - loss: 0.2790 - val_loss: 0.4271
Epoch 41/512
3063/3063 - 9s - loss: 0.2751 - val_loss: 0.4225
###
DNN Classifier
tf.data.datset created for training data
Model not yet created, creating new model
Epoch 1/512
3063/3063 - 11s - loss: 0.3947 - val_loss: 0.3558
Epoch 2/512
3063/3063 - 10s - loss: 0.3828 - val_loss: 0.3593
Epoch 3/512
3063/3063 - 10s - loss: 0.3818 - val_loss: 0.3670
Epoch 4/512
3063/3063 - 10s - loss: 0.3805 - val_loss: 0.3539
Epoch 5/512
3063/3063 - 9s - loss: 0.3782 - val_loss: 0.3496
Epoch 6/512
3063/3063 - 10s - loss: 0.3792 - val_loss: 0.3495
Epoch 7/512
3063/3063 - 9s - loss: 0.3762 - val_loss: 0.3537
Epoch 8/512
3063/3063 - 9s - loss: 0.3776 - val_loss: 0.3532
Epoch 9/512
3063/3063 - 9s - loss: 0.3765 - val_loss: 0.3519
Epoch 10/512
3063/3063 - 9s - loss: 0.3761 - val_loss: 0.3484
Epoch 11/512
3063/3063 - 9s - loss: 0.3755 - val_loss: 0.3494
Epoch 12/512
3063/3063 - 9s - loss: 0.3751 - val_loss: 0.3473
Epoch 13/512
3063/3063 - 9s - loss: 0.3757 - val_loss: 0.3518
Epoch 14/512
3063/3063 - 9s - loss: 0.3762 - val_loss: 0.3497
Epoch 15/512
3063/3063 - 9s - loss: 0.3748 - val_loss: 0.3512
Epoch 16/512
3063/3063 - 9s - loss: 0.3753 - val_loss: 0.3487
Epoch 17/512
3063/3063 - 9s - loss: 0.3752 - val_loss: 0.3464
Epoch 18/512
3063/3063 - 9s - loss: 0.3739 - val_loss: 0.3495
Epoch 19/512
3063/3063 - 9s - loss: 0.3744 - val_loss: 0.3500
Epoch 20/512
3063/3063 - 9s - loss: 0.3743 - val_loss: 0.3473
Epoch 21/512
3063/3063 - 9s - loss: 0.3734 - val_loss: 0.3502
Epoch 22/512
3063/3063 - 9s - loss: 0.3743 - val_loss: 0.3499
Epoch 23/512
3063/3063 - 9s - loss: 0.3727 - val_loss: 0.3475
Epoch 24/512
3063/3063 - 9s - loss: 0.3732 - val_loss: 0.3471
Epoch 25/512
3063/3063 - 10s - loss: 0.3728 - val_loss: 0.3496
Epoch 26/512
3063/3063 - 9s - loss: 0.3738 - val_loss: 0.3504
Epoch 27/512
3063/3063 - 9s - loss: 0.3745 - val_loss: 0.3476
Epoch 28/512
3063/3063 - 10s - loss: 0.3738 - val_loss: 0.3465
Epoch 29/512
3063/3063 - 9s - loss: 0.3735 - val_loss: 0.3492
Epoch 30/512
3063/3063 - 9s - loss: 0.3729 - val_loss: 0.3452
Epoch 31/512
3063/3063 - 10s - loss: 0.3736 - val_loss: 0.3466
Epoch 32/512
3063/3063 - 9s - loss: 0.3723 - val_loss: 0.3478
Epoch 33/512
3063/3063 - 10s - loss: 0.3737 - val_loss: 0.3475
Epoch 34/512
3063/3063 - 10s - loss: 0.3732 - val_loss: 0.3528
Epoch 35/512
3063/3063 - 10s - loss: 0.3723 - val_loss: 0.3465
Epoch 36/512
3063/3063 - 9s - loss: 0.3734 - val_loss: 0.3475
Epoch 37/512
3063/3063 - 10s - loss: 0.3715 - val_loss: 0.3494
Epoch 38/512
3063/3063 - 9s - loss: 0.3724 - val_loss: 0.3479
Epoch 39/512
3063/3063 - 9s - loss: 0.3736 - val_loss: 0.3527
Epoch 40/512
3063/3063 - 9s - loss: 0.3727 - val_loss: 0.3507
Epoch 41/512
3063/3063 - 9s - loss: 0.3712 - val_loss: 0.3465
Epoch 42/512
3063/3063 - 9s - loss: 0.3726 - val_loss: 0.3495
Epoch 43/512
3063/3063 - 10s - loss: 0.3726 - val_loss: 0.3476
Epoch 44/512
3063/3063 - 10s - loss: 0.3739 - val_loss: 0.3459
Epoch 45/512
3063/3063 - 10s - loss: 0.3724 - val_loss: 0.3503
Epoch 46/512
3063/3063 - 9s - loss: 0.3727 - val_loss: 0.3478
Epoch 47/512
3063/3063 - 10s - loss: 0.3722 - val_loss: 0.3480
Epoch 48/512
3063/3063 - 10s - loss: 0.3725 - val_loss: 0.3495
Epoch 49/512
3063/3063 - 10s - loss: 0.3726 - val_loss: 0.3505
Epoch 50/512
3063/3063 - 10s - loss: 0.3717 - val_loss: 0.3458
Epoch 51/512
3063/3063 - 10s - loss: 0.3730 - val_loss: 0.3488
Epoch 52/512
3063/3063 - 10s - loss: 0.3728 - val_loss: 0.3458
Epoch 53/512
3063/3063 - 10s - loss: 0.3712 - val_loss: 0.3488
Epoch 54/512
3063/3063 - 10s - loss: 0.3731 - val_loss: 0.3463
Epoch 55/512
3063/3063 - 10s - loss: 0.3739 - val_loss: 0.3473
Epoch 56/512
3063/3063 - 10s - loss: 0.3717 - val_loss: 0.3507
Epoch 57/512
3063/3063 - 10s - loss: 0.3707 - val_loss: 0.3465
Epoch 58/512
3063/3063 - 10s - loss: 0.3718 - val_loss: 0.3466
Epoch 59/512
3063/3063 - 9s - loss: 0.3725 - val_loss: 0.3462
Epoch 60/512
3063/3063 - 10s - loss: 0.3719 - val_loss: 0.3492
Epoch 61/512
3063/3063 - 10s - loss: 0.3709 - val_loss: 0.3467
Epoch 62/512
3063/3063 - 10s - loss: 0.3718 - val_loss: 0.3517
###
[14:36:44] WARNING: ../src/learner.cc:627: 
Parameters: { "n_estimators" } might not be used.

  This could be a false alarm, with some parameters getting used by language bindings but
  then being mistakenly passed down to XGBoost core, or some parameter actually being used
  but getting flagged wrongly here. Please open an issue if you find any such cases.


0.9252823678777353
At 0.5 threshold we have BDT signal efficiency 0.860
alright we're gonna start look at ['particlewise', 'pairwise', 'tripletwise', 'pairwise_nl', 'pairwise_nl_iter', 'nested_concat', 'naivednn', 'dnn']
getting ROC for particlewise
currently on particlewise_128_4_64
getting ROC for pairwise
currently on pairwise_5_(64, 128, 256, 128, 64)_64
getting ROC for tripletwise
currently on tripletwise_5_(64, 128, 256, 128, 64)_64
getting ROC for pairwise_nl
currently on pairwise_nl_5_(64, 128, 256, 128, 64)_32_64
getting ROC for pairwise_nl_iter
currently on pairwise_nl_iter_5_((64, 64, 116, 64, 64), (64, 64, 116, 64, 64), (64, 64, 116, 64, 64))_32_64
getting ROC for nested_concat
currently on nested_concat_70_4_64_3
getting ROC for naivednn
currently on naivednn_256_3_2
getting ROC for dnn
currently on dnn_256_3_2
getting ROC for naivednn
pog
getting ROC for dnn
pog
getting ROC for particlewise
pog
getting ROC for nested_concat
pog
getting ROC for pairwise
pog
getting ROC for pairwise_nl_iter
pog
getting ROC for tripletwise
pog
getting ROC for pairwise_nl
pog
getting ROC for particlewise
pog
getting ROC for pairwise
pog
getting ROC for tripletwise
pog
getting ROC for pairwise_nl
pog
getting ROC for pairwise_nl_iter
pog
getting ROC for nested_concat
pog
getting ROC for naivednn
pog
getting ROC for dnn
pog
[t-SNE] Computing 151 nearest neighbors...
[t-SNE] Indexed 30000 samples in 11.693s...
[t-SNE] Computed neighbors for 30000 samples in 22.370s...
[t-SNE] Computed conditional probabilities for sample 1000 / 30000
[t-SNE] Computed conditional probabilities for sample 2000 / 30000
[t-SNE] Computed conditional probabilities for sample 3000 / 30000
[t-SNE] Computed conditional probabilities for sample 4000 / 30000
[t-SNE] Computed conditional probabilities for sample 5000 / 30000
[t-SNE] Computed conditional probabilities for sample 6000 / 30000
[t-SNE] Computed conditional probabilities for sample 7000 / 30000
[t-SNE] Computed conditional probabilities for sample 8000 / 30000
[t-SNE] Computed conditional probabilities for sample 9000 / 30000
[t-SNE] Computed conditional probabilities for sample 10000 / 30000
[t-SNE] Computed conditional probabilities for sample 11000 / 30000
[t-SNE] Computed conditional probabilities for sample 12000 / 30000
[t-SNE] Computed conditional probabilities for sample 13000 / 30000
[t-SNE] Computed conditional probabilities for sample 14000 / 30000
[t-SNE] Computed conditional probabilities for sample 15000 / 30000
[t-SNE] Computed conditional probabilities for sample 16000 / 30000
[t-SNE] Computed conditional probabilities for sample 17000 / 30000
[t-SNE] Computed conditional probabilities for sample 18000 / 30000
[t-SNE] Computed conditional probabilities for sample 19000 / 30000
[t-SNE] Computed conditional probabilities for sample 20000 / 30000
[t-SNE] Computed conditional probabilities for sample 21000 / 30000
[t-SNE] Computed conditional probabilities for sample 22000 / 30000
[t-SNE] Computed conditional probabilities for sample 23000 / 30000
[t-SNE] Computed conditional probabilities for sample 24000 / 30000
[t-SNE] Computed conditional probabilities for sample 25000 / 30000
[t-SNE] Computed conditional probabilities for sample 26000 / 30000
[t-SNE] Computed conditional probabilities for sample 27000 / 30000
[t-SNE] Computed conditional probabilities for sample 28000 / 30000
[t-SNE] Computed conditional probabilities for sample 29000 / 30000
[t-SNE] Computed conditional probabilities for sample 30000 / 30000
[t-SNE] Mean sigma: 0.034264
[t-SNE] Computed conditional probabilities in 3.851s
[t-SNE] Iteration 50: error = 103.3188477, gradient norm = 0.0001223 (50 iterations in 7.657s)
[t-SNE] Iteration 100: error = 98.7269897, gradient norm = 0.0001217 (50 iterations in 10.173s)
[t-SNE] Iteration 150: error = 98.6671829, gradient norm = 0.0002199 (50 iterations in 9.129s)
[t-SNE] Iteration 200: error = 98.5317917, gradient norm = 0.0002127 (50 iterations in 8.552s)
[t-SNE] Iteration 250: error = 98.4650879, gradient norm = 0.0000454 (50 iterations in 8.262s)
[t-SNE] KL divergence after 250 iterations with early exaggeration: 98.465088
[t-SNE] Iteration 300: error = 4.1989841, gradient norm = 0.0005367 (50 iterations in 6.183s)
[t-SNE] Iteration 350: error = 3.9380784, gradient norm = 0.0002497 (50 iterations in 7.447s)
[t-SNE] Iteration 400: error = 3.8130271, gradient norm = 0.0001552 (50 iterations in 7.106s)
[t-SNE] Iteration 450: error = 3.7363062, gradient norm = 0.0001055 (50 iterations in 6.151s)
[t-SNE] Iteration 500: error = 3.6835649, gradient norm = 0.0000788 (50 iterations in 6.720s)
[t-SNE] Iteration 550: error = 3.6448932, gradient norm = 0.0000606 (50 iterations in 6.521s)
[t-SNE] Iteration 600: error = 3.6148777, gradient norm = 0.0000498 (50 iterations in 7.176s)
[t-SNE] Iteration 650: error = 3.5908830, gradient norm = 0.0000421 (50 iterations in 6.978s)
[t-SNE] Iteration 700: error = 3.5713973, gradient norm = 0.0000359 (50 iterations in 6.989s)
[t-SNE] Iteration 750: error = 3.5552149, gradient norm = 0.0000313 (50 iterations in 6.047s)
[t-SNE] Iteration 800: error = 3.5416615, gradient norm = 0.0000282 (50 iterations in 6.489s)
[t-SNE] Iteration 850: error = 3.5302236, gradient norm = 0.0000256 (50 iterations in 7.532s)
[t-SNE] Iteration 900: error = 3.5205460, gradient norm = 0.0000242 (50 iterations in 7.518s)
[t-SNE] Iteration 950: error = 3.5123463, gradient norm = 0.0000219 (50 iterations in 7.184s)
[t-SNE] Iteration 1000: error = 3.5052233, gradient norm = 0.0000198 (50 iterations in 6.743s)
[t-SNE] Iteration 1050: error = 3.4989469, gradient norm = 0.0000182 (50 iterations in 7.108s)
[t-SNE] Iteration 1100: error = 3.4934273, gradient norm = 0.0000168 (50 iterations in 6.668s)
[t-SNE] Iteration 1150: error = 3.4885716, gradient norm = 0.0000155 (50 iterations in 6.198s)
[t-SNE] Iteration 1200: error = 3.4842625, gradient norm = 0.0000150 (50 iterations in 6.620s)
[t-SNE] Iteration 1250: error = 3.4804547, gradient norm = 0.0000145 (50 iterations in 7.384s)
[t-SNE] Iteration 1300: error = 3.4770195, gradient norm = 0.0000137 (50 iterations in 7.120s)
[t-SNE] Iteration 1350: error = 3.4739845, gradient norm = 0.0000127 (50 iterations in 6.902s)
[t-SNE] Iteration 1400: error = 3.4712019, gradient norm = 0.0000123 (50 iterations in 7.193s)
[t-SNE] Iteration 1450: error = 3.4687207, gradient norm = 0.0000119 (50 iterations in 6.120s)
[t-SNE] Iteration 1500: error = 3.4665504, gradient norm = 0.0000113 (50 iterations in 6.583s)
[t-SNE] Iteration 1550: error = 3.4645040, gradient norm = 0.0000108 (50 iterations in 6.611s)
[t-SNE] Iteration 1600: error = 3.4626012, gradient norm = 0.0000106 (50 iterations in 6.455s)
[t-SNE] Iteration 1650: error = 3.4608836, gradient norm = 0.0000106 (50 iterations in 6.395s)
[t-SNE] Iteration 1700: error = 3.4594331, gradient norm = 0.0000104 (50 iterations in 6.658s)
[t-SNE] Iteration 1750: error = 3.4581082, gradient norm = 0.0000101 (50 iterations in 6.192s)
[t-SNE] Iteration 1800: error = 3.4569092, gradient norm = 0.0000099 (50 iterations in 6.189s)
[t-SNE] Iteration 1850: error = 3.4558089, gradient norm = 0.0000100 (50 iterations in 6.991s)
[t-SNE] Iteration 1900: error = 3.4548526, gradient norm = 0.0000096 (50 iterations in 6.665s)
[t-SNE] Iteration 1950: error = 3.4539573, gradient norm = 0.0000093 (50 iterations in 6.330s)
[t-SNE] Iteration 2000: error = 3.4530909, gradient norm = 0.0000092 (50 iterations in 6.165s)
[t-SNE] KL divergence after 2000 iterations: 3.453091
